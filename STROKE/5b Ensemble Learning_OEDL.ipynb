{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, GroupKFold\n",
    "from sklearn.metrics import accuracy_score,auc,roc_curve\n",
    "from sklearn.preprocessing import LabelBinarizer,StandardScaler\n",
    "\n",
    "from sklearn import model_selection as cv\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Administrator\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "from dbn.tensorflow import SupervisedDBNClassification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input,Dense,LSTM,GRU,BatchNormalization\n",
    "from tensorflow.keras.layers import PReLU\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras as K\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import mean_absolute_error as MAE\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./dataset/joint/2.storke_combine_code_multi_balance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.array(df.pop('NIHSS'))\n",
    "X = np.array(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, stratify=Y,random_state = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Class = [0,1,2]\n",
    "Class_dict = dict(zip(Class, range(len(Class))))\n",
    "Class_dict\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(list(Class_dict.values()))\n",
    "y_train_labels = lb.transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_lstm = x_train.reshape(x_train.shape[0],1,x_train.shape[1])\n",
    "x_test_lstm = x_test.reshape(x_test.shape[0],1,x_test.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PUSE(object):\n",
    "    def __init__(self, n, narvs, k, x_lb, x_ub):\n",
    "        self.n = n #恒星数量\n",
    "        self.narvs = narvs #宇宙的维度\n",
    "        self.x_lb = x_lb  # 宇宙的边界\n",
    "        self.x_ub = x_ub # 宇宙的边界\n",
    "        self.k = k # 膨胀的次数（时间）\n",
    "        \n",
    "        self.explore1 = 2 # 个体恒星的膨胀因子\n",
    "        self.explore2 = 2 # 所有恒星的膨胀因子\n",
    "        self.w_start = 5 # 恒星的冲击波\n",
    "        self.w_end = 1 # 恒星的冲击波\n",
    "        self.volumemax = 6 #最大爆炸能量\n",
    "\n",
    "        \n",
    "        self.x1 = [] #经度\n",
    "        self.x2 = [] #维度\n",
    "        self.e1 = []\n",
    "        self.e2 = []\n",
    "        self.fit = np.zeros((n,1))\n",
    "                \n",
    "        self.init_Star()\n",
    "        \n",
    "        self.x1 = np.array(self.x1).astype('int64')\n",
    "        self.x2 = np.array(self.x2).astype('int64')\n",
    "        \n",
    "        self.init_Energy()\n",
    "        \n",
    "        \n",
    "        self.xbest = []\n",
    "        self.prexbest = []\n",
    "\n",
    "        self.init_MAXenergy()\n",
    "        \n",
    "        \n",
    " \n",
    "    def init_Star(self):\n",
    "        m1 = np.zeros((self.n,1))\n",
    "        m2 = np.zeros((self.n,1))\n",
    "        n1 = np.zeros((self.n,1))\n",
    "        n2 = np.zeros((self.n,1))\n",
    "        \n",
    "        m1[0]=0.7      \n",
    "        for i in range(1,self.n):\n",
    "            k = i-1\n",
    "            m1[i] = 2.3*((m1[k])**2)*math.sin(math.pi*m1[k])\n",
    "        m2[0]=m1[self.n-1]\n",
    "        for i in range(1,self.n):\n",
    "            k = i-1\n",
    "            m2[i] = 2.3*((m2[k])**2)*math.sin(math.pi*m2[k])\n",
    "            \n",
    "        for i in range(self.n):        \n",
    "            n1[i] = int(np.ceil(m1[i]*(self.x_lb-self.x_lb))+self.x_lb) \n",
    "            n2[i] = int(np.ceil(m2[i]*(self.x_lb-self.x_lb))+self.x_lb)\n",
    "            self.x1.append(n1[i])\n",
    "            self.x2.append(n2[i])\n",
    "        self.e1 = [int(np.ceil(np.random.rand()*(2*self.volumemax)))-self.volumemax for i in range(self.n)]\n",
    "        self.e2 = [int(np.ceil(np.random.rand()*(2*self.volumemax)))-self.volumemax for i in range(self.n)]\n",
    "        \n",
    "    def init_Energy(self):\n",
    "        for i in range(self.n):\n",
    "            self.fit[i] = func(self.x1[i],self.x2[i])\n",
    "    \n",
    "    def init_MAXenergy(self):\n",
    "        fitbest = min(self.fit)\n",
    "        indbest = self.fit.tolist().index(fitbest)\n",
    "        self.xbest.append(self.x1[indbest])\n",
    "        self.xbest.append(self.x2[indbest])\n",
    "        self.prexbest = self.xbest\n",
    "\n",
    "    def iterator(self):       \n",
    "        for d in range(self.k):\n",
    "            for i in range(self.n):\n",
    "                w = int(np.ceil(self.w_start - (self.w_start-self.w_end)*d/self.k))\n",
    "                self.e1[i] = int(np.ceil(w*self.e1[i] + self.explore1*np.random.rand()*(self.prexbest[0]-self.x1[i]) + self.explore2*np.random.rand()*(self.xbest[0]-self.x1[i])))\n",
    "                self.e2[i] = int(np.ceil(w*self.e2[i] + self.explore1*np.random.rand()*(self.prexbest[1]-self.x2[i]) + self.explore2*np.random.rand()*(self.xbest[1]-self.x2[i])))\n",
    "                for j in range(self.n):\n",
    "                    if self.e1[j] < -self.volumemax:\n",
    "                        self.e1[j] = -self.volumemax\n",
    "                    elif self.e1[j] > self.volumemax:\n",
    "                        self.e1[j] = self.volumemax\n",
    "                    if self.e2[j] < -self.volumemax:\n",
    "                        self.e2[j] = -self.volumemax\n",
    "                    elif self.e2[j] > self.volumemax:\n",
    "                        self.e2[j] = self.volumemax\n",
    "                self.x1[i] = self.x1[i] + self.e1[i]\n",
    "                self.x2[i] = self.x2[i] + self.e2[i]\n",
    "                for j in range(self.n):\n",
    "                    if self.x1[j] < self.x_lb:\n",
    "                        self.x1[j] = self.x_lb\n",
    "                    elif self.x1[j] > self.x_ub-1:\n",
    "                        self.x1[j] = self.x_ub-1\n",
    "                    if self.x2[j] < self.x_lb:\n",
    "                        self.x2[j] = self.x_lb\n",
    "                    elif self.x2[j] > self.x_ub-1:\n",
    "                        self.x2[j] = self.x_ub-1 \n",
    "                self.fit[i] = func(self.x1[i],self.x2[i]) \n",
    "                if self.fit[i] < func(self.prexbest[0],self.prexbest[1]):\n",
    "                    self.prexbest[0] = self.x1[i]\n",
    "                    self.prexbest[1] = self.x2[i]\n",
    "                if self.fit[i] < func(self.xbest[0],self.xbest[1]):\n",
    "                    self.xbest[0] = self.prexbest[0]\n",
    "                    self.xbest[1] = self.prexbest[1]\n",
    "                self.fitbest = func(self.xbest[0],self.xbest[1])\n",
    "        return self.fitbest,self.xbest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Predict(X):\n",
    "    RNN_test_label = []\n",
    "    Class = [0,1,2]\n",
    "    Class_dict = dict(zip(Class, range(len(Class))))\n",
    "    Class_dict\n",
    "    for i in range(0,X.shape[0]):\n",
    "        RNN_test_label.append(Class_dict[np.argmax(X[i])])\n",
    "    RNN_test_label = np.array(RNN_test_label,dtype = 'int64')\n",
    "    return RNN_test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 80)                7280      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 80)                6480      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 3)                 243       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,003\n",
      "Trainable params: 14,003\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#DNN\n",
    "def buildDNN(layer1,layer2,n_class):\n",
    "    init = K.initializers.glorot_uniform(seed=1)\n",
    "    simple_adam = tf.keras.optimizers.Adam()\n",
    "    model = K.models.Sequential()\n",
    "    model.add(K.layers.Dense(units=layer1, input_dim=90, kernel_initializer=init, activation='relu'))\n",
    "    model.add(K.layers.Dense(units=layer2, kernel_initializer=init, activation='relu'))\n",
    "    model.add(K.layers.Dense(units=n_class, kernel_initializer=init, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=simple_adam, metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "dnn = buildDNN(layer1=80,layer2=80,n_class=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DBN\n",
    "def buildDBN(layer1,layer2,epoca,K=500):\n",
    "    \n",
    "    # cantidad de neuronas ocultas \n",
    "    hidden_layers = []\n",
    "    hidden_layers.append( int(layer1))\n",
    "    hidden_layers.append( int(layer2))\n",
    "\n",
    "    DBN_classifier = SupervisedDBNClassification(hidden_layers_structure = hidden_layers,\n",
    "                                                    learning_rate_rbm=0.05,\n",
    "                                                    learning_rate=0.1,\n",
    "                                                    n_epochs_rbm=epoca,\n",
    "                                                    n_iter_backprop=K,\n",
    "                                                    batch_size=32,\n",
    "                                                    activation_function='relu',\n",
    "                                                    dropout_p=0.2)\n",
    "    return DBN_classifier\n",
    "\n",
    "dbn = buildDBN(layer1=64,layer2=64,epoca=20,K=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1, 90)]           0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100)               76400     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 3)                 303       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 86,803\n",
      "Trainable params: 86,803\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#LSTM_RNN\n",
    "def buildLSTM(timeStep,inputColNum,outStep,learnRate=1e-4):\n",
    "    '''\n",
    "    搭建LSTM网络，激活函数为tanh\n",
    "    timeStep：输入时间步\n",
    "    inputColNum：输入列数\n",
    "    outStep：输出时间步\n",
    "    learnRate：学习率    \n",
    "    '''\n",
    "    #输入层\n",
    "    inputLayer = Input(shape=(timeStep,inputColNum))\n",
    "\n",
    "    #中间层\n",
    "    middle = LSTM(100,activation='tanh')(inputLayer)\n",
    "    middle = Dense(100,activation='tanh')(middle)\n",
    "\n",
    "    #输出层 全连接\n",
    "    outputLayer = Dense(outStep)(middle)\n",
    "    \n",
    "    #建模\n",
    "    model = Model(inputs=inputLayer,outputs=outputLayer)\n",
    "    optimizer = Adam(learning_rate=learnRate)\n",
    "    model.compile(optimizer=optimizer,loss='mse') \n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "#搭建LSTM\n",
    "lstm = buildLSTM(timeStep=1,inputColNum=90,outStep=3,learnRate=1e-4)\n",
    "# epochs = 1000#迭代次数\n",
    "# batchSize = 20#批处理量\n",
    "# lstm.fit(x3_train_lstm,y3_train_labels,epochs=epochs,verbose=0,batch_size=batchSize) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "_N_FOLDS = 5  # 采用5折交叉验证\n",
    "kf = StratifiedKFold(n_splits=5, random_state=2020, shuffle=True)  # sklearn的交叉验证模块，用于划分数据\n",
    "_N_CLASS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 350 samples\n",
      "Epoch 1/100\n",
      "350/350 [==============================] - 0s 219us/sample - loss: 0.9490 - acc: 0.6686\n",
      "Epoch 2/100\n",
      "350/350 [==============================] - 0s 29us/sample - loss: 0.7463 - acc: 0.7457\n",
      "Epoch 3/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.5720 - acc: 0.8114\n",
      "Epoch 4/100\n",
      "350/350 [==============================] - 0s 34us/sample - loss: 0.4466 - acc: 0.8200\n",
      "Epoch 5/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.3329 - acc: 0.9057\n",
      "Epoch 6/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.2641 - acc: 0.9286\n",
      "Epoch 7/100\n",
      "350/350 [==============================] - 0s 40us/sample - loss: 0.2234 - acc: 0.9286\n",
      "Epoch 8/100\n",
      "350/350 [==============================] - 0s 31us/sample - loss: 0.1790 - acc: 0.9543\n",
      "Epoch 9/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.1505 - acc: 0.9514\n",
      "Epoch 10/100\n",
      "350/350 [==============================] - 0s 34us/sample - loss: 0.1314 - acc: 0.9629\n",
      "Epoch 11/100\n",
      "350/350 [==============================] - 0s 31us/sample - loss: 0.1132 - acc: 0.9686\n",
      "Epoch 12/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0998 - acc: 0.9800\n",
      "Epoch 13/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0845 - acc: 0.9800\n",
      "Epoch 14/100\n",
      "350/350 [==============================] - 0s 31us/sample - loss: 0.0703 - acc: 0.9886\n",
      "Epoch 15/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0608 - acc: 0.9943\n",
      "Epoch 16/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0606 - acc: 0.9800\n",
      "Epoch 17/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0472 - acc: 1.0000\n",
      "Epoch 18/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0384 - acc: 1.0000\n",
      "Epoch 19/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0342 - acc: 0.9943\n",
      "Epoch 20/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0322 - acc: 1.0000\n",
      "Epoch 21/100\n",
      "350/350 [==============================] - 0s 31us/sample - loss: 0.0273 - acc: 1.0000\n",
      "Epoch 22/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0254 - acc: 1.0000\n",
      "Epoch 23/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0209 - acc: 1.0000\n",
      "Epoch 24/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0169 - acc: 1.0000\n",
      "Epoch 25/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0182 - acc: 1.0000\n",
      "Epoch 26/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0160 - acc: 1.0000\n",
      "Epoch 27/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0138 - acc: 1.0000\n",
      "Epoch 28/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0108 - acc: 1.0000\n",
      "Epoch 29/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0099 - acc: 1.0000\n",
      "Epoch 30/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0087 - acc: 1.0000\n",
      "Epoch 31/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0078 - acc: 1.0000\n",
      "Epoch 32/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0074 - acc: 1.0000\n",
      "Epoch 33/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0068 - acc: 1.0000\n",
      "Epoch 34/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0062 - acc: 1.0000\n",
      "Epoch 35/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0059 - acc: 1.0000\n",
      "Epoch 36/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0053 - acc: 1.0000\n",
      "Epoch 37/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0050 - acc: 1.0000\n",
      "Epoch 38/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0047 - acc: 1.0000\n",
      "Epoch 39/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0044 - acc: 1.0000\n",
      "Epoch 40/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0043 - acc: 1.0000\n",
      "Epoch 41/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0038 - acc: 1.0000\n",
      "Epoch 42/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0036 - acc: 1.0000\n",
      "Epoch 43/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0034 - acc: 1.0000\n",
      "Epoch 44/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0031 - acc: 1.0000\n",
      "Epoch 45/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0029 - acc: 1.0000\n",
      "Epoch 46/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0028 - acc: 1.0000\n",
      "Epoch 47/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0026 - acc: 1.0000\n",
      "Epoch 48/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0025 - acc: 1.0000\n",
      "Epoch 49/100\n",
      "350/350 [==============================] - 0s 34us/sample - loss: 0.0024 - acc: 1.0000\n",
      "Epoch 50/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0022 - acc: 1.0000\n",
      "Epoch 51/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0021 - acc: 1.0000\n",
      "Epoch 52/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 53/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0019 - acc: 1.0000\n",
      "Epoch 54/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0019 - acc: 1.0000\n",
      "Epoch 55/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0017 - acc: 1.0000\n",
      "Epoch 56/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0017 - acc: 1.0000\n",
      "Epoch 57/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0016 - acc: 1.0000\n",
      "Epoch 58/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0016 - acc: 1.0000\n",
      "Epoch 59/100\n",
      "350/350 [==============================] - 0s 31us/sample - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 60/100\n",
      "350/350 [==============================] - 0s 31us/sample - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 61/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 62/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 63/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 64/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 65/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 66/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 67/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 68/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0010 - acc: 1.0000\n",
      "Epoch 69/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 9.7182e-04 - acc: 1.0000\n",
      "Epoch 70/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 9.2757e-04 - acc: 1.0000\n",
      "Epoch 71/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 9.2056e-04 - acc: 1.0000\n",
      "Epoch 72/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 8.5665e-04 - acc: 1.0000\n",
      "Epoch 73/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 8.4607e-04 - acc: 1.0000\n",
      "Epoch 74/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 8.2364e-04 - acc: 1.0000\n",
      "Epoch 75/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 7.7142e-04 - acc: 1.0000\n",
      "Epoch 76/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 7.5485e-04 - acc: 1.0000\n",
      "Epoch 77/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 7.2458e-04 - acc: 1.0000\n",
      "Epoch 78/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 7.0173e-04 - acc: 1.0000\n",
      "Epoch 79/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 6.8570e-04 - acc: 1.0000\n",
      "Epoch 80/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 6.7242e-04 - acc: 1.0000\n",
      "Epoch 81/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 6.4463e-04 - acc: 1.0000\n",
      "Epoch 82/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 6.2414e-04 - acc: 1.0000\n",
      "Epoch 83/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 6.1009e-04 - acc: 1.0000\n",
      "Epoch 84/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 5.8112e-04 - acc: 1.0000\n",
      "Epoch 85/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 5.7246e-04 - acc: 1.0000\n",
      "Epoch 86/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 5.6256e-04 - acc: 1.0000\n",
      "Epoch 87/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 5.2565e-04 - acc: 1.0000\n",
      "Epoch 88/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 5.1942e-04 - acc: 1.0000\n",
      "Epoch 89/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 5.0335e-04 - acc: 1.0000\n",
      "Epoch 90/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 4.8103e-04 - acc: 1.0000\n",
      "Epoch 91/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 4.6566e-04 - acc: 1.0000\n",
      "Epoch 92/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 4.6240e-04 - acc: 1.0000\n",
      "Epoch 93/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 4.5531e-04 - acc: 1.0000\n",
      "Epoch 94/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 4.3315e-04 - acc: 1.0000\n",
      "Epoch 95/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 4.1972e-04 - acc: 1.0000\n",
      "Epoch 96/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 4.1731e-04 - acc: 1.0000\n",
      "Epoch 97/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 3.9825e-04 - acc: 1.0000\n",
      "Epoch 98/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 3.9119e-04 - acc: 1.0000\n",
      "Epoch 99/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 3.7773e-04 - acc: 1.0000\n",
      "Epoch 100/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 3.7074e-04 - acc: 1.0000\n",
      "Train on 350 samples\n",
      "Epoch 1/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.1157 - acc: 0.9771\n",
      "Epoch 2/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.1615 - acc: 0.9314\n",
      "Epoch 3/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.1391 - acc: 0.9486\n",
      "Epoch 4/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0357 - acc: 0.9829\n",
      "Epoch 5/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0077 - acc: 1.0000\n",
      "Epoch 6/100\n",
      "350/350 [==============================] - 0s 29us/sample - loss: 0.0072 - acc: 1.0000\n",
      "Epoch 7/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0040 - acc: 1.0000\n",
      "Epoch 8/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0039 - acc: 1.0000\n",
      "Epoch 9/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0026 - acc: 1.0000\n",
      "Epoch 10/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0023 - acc: 1.0000\n",
      "Epoch 11/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 12/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0019 - acc: 1.0000\n",
      "Epoch 13/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0017 - acc: 1.0000\n",
      "Epoch 14/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0016 - acc: 1.0000\n",
      "Epoch 15/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0015 - acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 17/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 18/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 19/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 20/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 9.8033e-04 - acc: 1.0000\n",
      "Epoch 21/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 9.4168e-04 - acc: 1.0000\n",
      "Epoch 22/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 9.0821e-04 - acc: 1.0000\n",
      "Epoch 23/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 8.3581e-04 - acc: 1.0000\n",
      "Epoch 24/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 8.0033e-04 - acc: 1.0000\n",
      "Epoch 25/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 7.5933e-04 - acc: 1.0000\n",
      "Epoch 26/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 7.2356e-04 - acc: 1.0000\n",
      "Epoch 27/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 6.8719e-04 - acc: 1.0000\n",
      "Epoch 28/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 6.6441e-04 - acc: 1.0000\n",
      "Epoch 29/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 6.4955e-04 - acc: 1.0000\n",
      "Epoch 30/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 5.9612e-04 - acc: 1.0000\n",
      "Epoch 31/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 5.7640e-04 - acc: 1.0000\n",
      "Epoch 32/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 5.5183e-04 - acc: 1.0000\n",
      "Epoch 33/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 5.2939e-04 - acc: 1.0000\n",
      "Epoch 34/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 5.1216e-04 - acc: 1.0000\n",
      "Epoch 35/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 4.8635e-04 - acc: 1.0000\n",
      "Epoch 36/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 4.6677e-04 - acc: 1.0000\n",
      "Epoch 37/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 4.5145e-04 - acc: 1.0000\n",
      "Epoch 38/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 4.3607e-04 - acc: 1.0000\n",
      "Epoch 39/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 4.1959e-04 - acc: 1.0000\n",
      "Epoch 40/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 4.0526e-04 - acc: 1.0000\n",
      "Epoch 41/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 3.9421e-04 - acc: 1.0000\n",
      "Epoch 42/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 3.7827e-04 - acc: 1.0000\n",
      "Epoch 43/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 3.6519e-04 - acc: 1.0000\n",
      "Epoch 44/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 3.5293e-04 - acc: 1.0000\n",
      "Epoch 45/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 3.4564e-04 - acc: 1.0000\n",
      "Epoch 46/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 3.3201e-04 - acc: 1.0000\n",
      "Epoch 47/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 3.2287e-04 - acc: 1.0000\n",
      "Epoch 48/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 3.1685e-04 - acc: 1.0000\n",
      "Epoch 49/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 3.0445e-04 - acc: 1.0000\n",
      "Epoch 50/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 2.9916e-04 - acc: 1.0000\n",
      "Epoch 51/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 2.8670e-04 - acc: 1.0000\n",
      "Epoch 52/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 2.7581e-04 - acc: 1.0000\n",
      "Epoch 53/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 2.7047e-04 - acc: 1.0000\n",
      "Epoch 54/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 2.6452e-04 - acc: 1.0000\n",
      "Epoch 55/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 2.5447e-04 - acc: 1.0000\n",
      "Epoch 56/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 2.4869e-04 - acc: 1.0000\n",
      "Epoch 57/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 2.4020e-04 - acc: 1.0000\n",
      "Epoch 58/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 2.3693e-04 - acc: 1.0000\n",
      "Epoch 59/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 2.3456e-04 - acc: 1.0000\n",
      "Epoch 60/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 2.2124e-04 - acc: 1.0000\n",
      "Epoch 61/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 2.1933e-04 - acc: 1.0000\n",
      "Epoch 62/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 2.0976e-04 - acc: 1.0000\n",
      "Epoch 63/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 2.0552e-04 - acc: 1.0000\n",
      "Epoch 64/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 2.0013e-04 - acc: 1.0000\n",
      "Epoch 65/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 1.9387e-04 - acc: 1.0000\n",
      "Epoch 66/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 1.8789e-04 - acc: 1.0000\n",
      "Epoch 67/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 1.8399e-04 - acc: 1.0000\n",
      "Epoch 68/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 1.7793e-04 - acc: 1.0000\n",
      "Epoch 69/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 1.7318e-04 - acc: 1.0000\n",
      "Epoch 70/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 1.6862e-04 - acc: 1.0000\n",
      "Epoch 71/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 1.6505e-04 - acc: 1.0000\n",
      "Epoch 72/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 1.6050e-04 - acc: 1.0000\n",
      "Epoch 73/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 1.5645e-04 - acc: 1.0000\n",
      "Epoch 74/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 1.5207e-04 - acc: 1.0000\n",
      "Epoch 75/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 1.4957e-04 - acc: 1.0000\n",
      "Epoch 76/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 1.4545e-04 - acc: 1.0000\n",
      "Epoch 77/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 1.4107e-04 - acc: 1.0000\n",
      "Epoch 78/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 1.3909e-04 - acc: 1.0000\n",
      "Epoch 79/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 1.3362e-04 - acc: 1.0000\n",
      "Epoch 80/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 1.3050e-04 - acc: 1.0000\n",
      "Epoch 81/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 1.2449e-04 - acc: 1.0000\n",
      "Epoch 82/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 1.1953e-04 - acc: 1.0000\n",
      "Epoch 83/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 1.1663e-04 - acc: 1.0000\n",
      "Epoch 84/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 1.1281e-04 - acc: 1.0000\n",
      "Epoch 85/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 1.0964e-04 - acc: 1.0000\n",
      "Epoch 86/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 1.0672e-04 - acc: 1.0000\n",
      "Epoch 87/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 1.0336e-04 - acc: 1.0000\n",
      "Epoch 88/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 1.0052e-04 - acc: 1.0000\n",
      "Epoch 89/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 9.7981e-05 - acc: 1.0000\n",
      "Epoch 90/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 9.4852e-05 - acc: 1.0000\n",
      "Epoch 91/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 9.2790e-05 - acc: 1.0000\n",
      "Epoch 92/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 8.9531e-05 - acc: 1.0000\n",
      "Epoch 93/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 8.8045e-05 - acc: 1.0000\n",
      "Epoch 94/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 8.4193e-05 - acc: 1.0000\n",
      "Epoch 95/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 8.2786e-05 - acc: 1.0000\n",
      "Epoch 96/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 8.0684e-05 - acc: 1.0000\n",
      "Epoch 97/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 7.8012e-05 - acc: 1.0000\n",
      "Epoch 98/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 7.5907e-05 - acc: 1.0000\n",
      "Epoch 99/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 7.3771e-05 - acc: 1.0000\n",
      "Epoch 100/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 7.2258e-05 - acc: 1.0000\n",
      "Train on 350 samples\n",
      "Epoch 1/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 8.1669e-04 - acc: 1.0000\n",
      "Epoch 2/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 6.2940e-04 - acc: 1.0000\n",
      "Epoch 3/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 3.4303e-04 - acc: 1.0000\n",
      "Epoch 4/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 1.3435e-04 - acc: 1.0000\n",
      "Epoch 5/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 1.2301e-04 - acc: 1.0000\n",
      "Epoch 6/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 1.1430e-04 - acc: 1.0000\n",
      "Epoch 7/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 1.0851e-04 - acc: 1.0000\n",
      "Epoch 8/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 1.0392e-04 - acc: 1.0000\n",
      "Epoch 9/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 9.8718e-05 - acc: 1.0000\n",
      "Epoch 10/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 9.4674e-05 - acc: 1.0000\n",
      "Epoch 11/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 9.1231e-05 - acc: 1.0000\n",
      "Epoch 12/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 8.8977e-05 - acc: 1.0000\n",
      "Epoch 13/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 8.5067e-05 - acc: 1.0000\n",
      "Epoch 14/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 8.3075e-05 - acc: 1.0000\n",
      "Epoch 15/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 8.0224e-05 - acc: 1.0000\n",
      "Epoch 16/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 7.8274e-05 - acc: 1.0000\n",
      "Epoch 17/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 7.6148e-05 - acc: 1.0000\n",
      "Epoch 18/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 7.4919e-05 - acc: 1.0000\n",
      "Epoch 19/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 7.2124e-05 - acc: 1.0000\n",
      "Epoch 20/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 7.0692e-05 - acc: 1.0000\n",
      "Epoch 21/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 6.9312e-05 - acc: 1.0000\n",
      "Epoch 22/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 6.7442e-05 - acc: 1.0000\n",
      "Epoch 23/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 6.6396e-05 - acc: 1.0000\n",
      "Epoch 24/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 6.4419e-05 - acc: 1.0000\n",
      "Epoch 25/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 6.3121e-05 - acc: 1.0000\n",
      "Epoch 26/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 6.2021e-05 - acc: 1.0000\n",
      "Epoch 27/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 6.0384e-05 - acc: 1.0000\n",
      "Epoch 28/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 5.9123e-05 - acc: 1.0000\n",
      "Epoch 29/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 5.7679e-05 - acc: 1.0000\n",
      "Epoch 30/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 5.6822e-05 - acc: 1.0000\n",
      "Epoch 31/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 5.5672e-05 - acc: 1.0000\n",
      "Epoch 32/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 5.4330e-05 - acc: 1.0000\n",
      "Epoch 33/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 5.3491e-05 - acc: 1.0000\n",
      "Epoch 34/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 5.2624e-05 - acc: 1.0000\n",
      "Epoch 35/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 5.1641e-05 - acc: 1.0000\n",
      "Epoch 36/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 5.0626e-05 - acc: 1.0000\n",
      "Epoch 37/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 4.9440e-05 - acc: 1.0000\n",
      "Epoch 38/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 4.8653e-05 - acc: 1.0000\n",
      "Epoch 39/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 4.7914e-05 - acc: 1.0000\n",
      "Epoch 40/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 4.7205e-05 - acc: 1.0000\n",
      "Epoch 41/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 4.5961e-05 - acc: 1.0000\n",
      "Epoch 42/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 4.5160e-05 - acc: 1.0000\n",
      "Epoch 43/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 4.4960e-05 - acc: 1.0000\n",
      "Epoch 44/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 4.3429e-05 - acc: 1.0000\n",
      "Epoch 45/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 4.2773e-05 - acc: 1.0000\n",
      "Epoch 46/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 4.2053e-05 - acc: 1.0000\n",
      "Epoch 47/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 4.1186e-05 - acc: 1.0000\n",
      "Epoch 48/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 4.0533e-05 - acc: 1.0000\n",
      "Epoch 49/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 3.9905e-05 - acc: 1.0000\n",
      "Epoch 50/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 3.9130e-05 - acc: 1.0000\n",
      "Epoch 51/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 3.8499e-05 - acc: 1.0000\n",
      "Epoch 52/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 3.8006e-05 - acc: 1.0000\n",
      "Epoch 53/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 3.7470e-05 - acc: 1.0000\n",
      "Epoch 54/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 3.6585e-05 - acc: 1.0000\n",
      "Epoch 55/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 3.6093e-05 - acc: 1.0000\n",
      "Epoch 56/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 3.5456e-05 - acc: 1.0000\n",
      "Epoch 57/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 3.4908e-05 - acc: 1.0000\n",
      "Epoch 58/100\n",
      "350/350 [==============================] - 0s 57us/sample - loss: 3.4190e-05 - acc: 1.0000\n",
      "Epoch 59/100\n",
      "350/350 [==============================] - 0s 37us/sample - loss: 3.3536e-05 - acc: 1.0000\n",
      "Epoch 60/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 3.3225e-05 - acc: 1.0000\n",
      "Epoch 61/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 3.2479e-05 - acc: 1.0000\n",
      "Epoch 62/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 3.2120e-05 - acc: 1.0000\n",
      "Epoch 63/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 3.1465e-05 - acc: 1.0000\n",
      "Epoch 64/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 3.0970e-05 - acc: 1.0000\n",
      "Epoch 65/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 3.0529e-05 - acc: 1.0000\n",
      "Epoch 66/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 3.0022e-05 - acc: 1.0000\n",
      "Epoch 67/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 2.9513e-05 - acc: 1.0000\n",
      "Epoch 68/100\n",
      "350/350 [==============================] - 0s 31us/sample - loss: 2.9190e-05 - acc: 1.0000\n",
      "Epoch 69/100\n",
      "350/350 [==============================] - 0s 31us/sample - loss: 2.8732e-05 - acc: 1.0000\n",
      "Epoch 70/100\n",
      "350/350 [==============================] - 0s 31us/sample - loss: 2.8423e-05 - acc: 1.0000\n",
      "Epoch 71/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 2.7886e-05 - acc: 1.0000\n",
      "Epoch 72/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 2.7425e-05 - acc: 1.0000\n",
      "Epoch 73/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 2.7003e-05 - acc: 1.0000\n",
      "Epoch 74/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 2.6494e-05 - acc: 1.0000\n",
      "Epoch 75/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 2.6228e-05 - acc: 1.0000\n",
      "Epoch 76/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 2.5825e-05 - acc: 1.0000\n",
      "Epoch 77/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 2.5424e-05 - acc: 1.0000\n",
      "Epoch 78/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 2.5136e-05 - acc: 1.0000\n",
      "Epoch 79/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 2.4551e-05 - acc: 1.0000\n",
      "Epoch 80/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 2.4360e-05 - acc: 1.0000\n",
      "Epoch 81/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 2.3923e-05 - acc: 1.0000\n",
      "Epoch 82/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 2.3539e-05 - acc: 1.0000\n",
      "Epoch 83/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 2.3227e-05 - acc: 1.0000\n",
      "Epoch 84/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 2.2949e-05 - acc: 1.0000\n",
      "Epoch 85/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 2.2517e-05 - acc: 1.0000\n",
      "Epoch 86/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 2.2232e-05 - acc: 1.0000\n",
      "Epoch 87/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 2.1956e-05 - acc: 1.0000\n",
      "Epoch 88/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 2.1629e-05 - acc: 1.0000\n",
      "Epoch 89/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 2.1338e-05 - acc: 1.0000\n",
      "Epoch 90/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 2.1088e-05 - acc: 1.0000\n",
      "Epoch 91/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 2.0764e-05 - acc: 1.0000\n",
      "Epoch 92/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 2.0423e-05 - acc: 1.0000\n",
      "Epoch 93/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 2.0380e-05 - acc: 1.0000\n",
      "Epoch 94/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 1.9937e-05 - acc: 1.0000\n",
      "Epoch 95/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 1.9693e-05 - acc: 1.0000\n",
      "Epoch 96/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 1.9471e-05 - acc: 1.0000\n",
      "Epoch 97/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 1.9108e-05 - acc: 1.0000\n",
      "Epoch 98/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 1.8903e-05 - acc: 1.0000\n",
      "Epoch 99/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 1.8646e-05 - acc: 1.0000\n",
      "Epoch 100/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 1.8351e-05 - acc: 1.0000\n",
      "Train on 351 samples\n",
      "Epoch 1/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 2.4097e-05 - acc: 1.0000\n",
      "Epoch 2/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 2.2382e-05 - acc: 1.0000\n",
      "Epoch 3/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 2.1479e-05 - acc: 1.0000\n",
      "Epoch 4/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 2.0901e-05 - acc: 1.0000\n",
      "Epoch 5/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 2.0315e-05 - acc: 1.0000\n",
      "Epoch 6/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 1.9702e-05 - acc: 1.0000\n",
      "Epoch 7/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 1.9228e-05 - acc: 1.0000\n",
      "Epoch 8/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 1.8860e-05 - acc: 1.0000\n",
      "Epoch 9/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 1.8510e-05 - acc: 1.0000\n",
      "Epoch 10/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 1.8229e-05 - acc: 1.0000\n",
      "Epoch 11/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 1.7804e-05 - acc: 1.0000\n",
      "Epoch 12/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 1.7489e-05 - acc: 1.0000\n",
      "Epoch 13/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 1.7179e-05 - acc: 1.0000\n",
      "Epoch 14/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 1.6907e-05 - acc: 1.0000\n",
      "Epoch 15/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 1.6653e-05 - acc: 1.0000\n",
      "Epoch 16/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 1.6319e-05 - acc: 1.0000\n",
      "Epoch 17/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 1.6062e-05 - acc: 1.0000\n",
      "Epoch 18/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 1.5993e-05 - acc: 1.0000\n",
      "Epoch 19/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 1.5572e-05 - acc: 1.0000\n",
      "Epoch 20/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 1.5384e-05 - acc: 1.0000\n",
      "Epoch 21/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 1.5102e-05 - acc: 1.0000\n",
      "Epoch 22/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 1.4905e-05 - acc: 1.0000\n",
      "Epoch 23/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 1.4716e-05 - acc: 1.0000\n",
      "Epoch 24/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 1.4474e-05 - acc: 1.0000\n",
      "Epoch 25/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 1.4252e-05 - acc: 1.0000\n",
      "Epoch 26/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 1.4084e-05 - acc: 1.0000\n",
      "Epoch 27/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 1.3852e-05 - acc: 1.0000\n",
      "Epoch 28/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 1.3716e-05 - acc: 1.0000\n",
      "Epoch 29/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 1.3453e-05 - acc: 1.0000\n",
      "Epoch 30/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 1.3341e-05 - acc: 1.0000\n",
      "Epoch 31/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 1.3106e-05 - acc: 1.0000\n",
      "Epoch 32/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 1.2899e-05 - acc: 1.0000\n",
      "Epoch 33/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 1.2776e-05 - acc: 1.0000\n",
      "Epoch 34/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 1.2558e-05 - acc: 1.0000\n",
      "Epoch 35/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 1.2418e-05 - acc: 1.0000\n",
      "Epoch 36/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 1.2271e-05 - acc: 1.0000\n",
      "Epoch 37/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 1.2150e-05 - acc: 1.0000\n",
      "Epoch 38/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 1.1885e-05 - acc: 1.0000\n",
      "Epoch 39/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 1.1746e-05 - acc: 1.0000\n",
      "Epoch 40/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 1.1588e-05 - acc: 1.0000\n",
      "Epoch 41/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 1.1466e-05 - acc: 1.0000\n",
      "Epoch 42/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 1.1281e-05 - acc: 1.0000\n",
      "Epoch 43/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 1.1144e-05 - acc: 1.0000\n",
      "Epoch 44/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 1.0991e-05 - acc: 1.0000\n",
      "Epoch 45/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 1.0858e-05 - acc: 1.0000\n",
      "Epoch 46/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 1.0714e-05 - acc: 1.0000\n",
      "Epoch 47/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 1.0552e-05 - acc: 1.0000\n",
      "Epoch 48/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 1.0442e-05 - acc: 1.0000\n",
      "Epoch 49/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 1.0293e-05 - acc: 1.0000\n",
      "Epoch 50/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 1.0153e-05 - acc: 1.0000\n",
      "Epoch 51/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 1.0046e-05 - acc: 1.0000\n",
      "Epoch 52/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 9.9038e-06 - acc: 1.0000\n",
      "Epoch 53/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 9.7513e-06 - acc: 1.0000\n",
      "Epoch 54/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 9.6800e-06 - acc: 1.0000\n",
      "Epoch 55/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 9.5014e-06 - acc: 1.0000\n",
      "Epoch 56/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 9.3859e-06 - acc: 1.0000\n",
      "Epoch 57/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 9.2742e-06 - acc: 1.0000\n",
      "Epoch 58/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 9.1411e-06 - acc: 1.0000\n",
      "Epoch 59/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 9.0426e-06 - acc: 1.0000\n",
      "Epoch 60/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 8.9387e-06 - acc: 1.0000\n",
      "Epoch 61/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 8.8174e-06 - acc: 1.0000\n",
      "Epoch 62/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 8.6996e-06 - acc: 1.0000\n",
      "Epoch 63/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 8.6167e-06 - acc: 1.0000\n",
      "Epoch 64/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 8.5203e-06 - acc: 1.0000\n",
      "Epoch 65/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 8.3692e-06 - acc: 1.0000\n",
      "Epoch 66/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 8.3023e-06 - acc: 1.0000\n",
      "Epoch 67/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 8.1898e-06 - acc: 1.0000\n",
      "Epoch 68/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 8.1002e-06 - acc: 1.0000\n",
      "Epoch 69/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 7.9949e-06 - acc: 1.0000\n",
      "Epoch 70/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 7.8737e-06 - acc: 1.0000\n",
      "Epoch 71/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 7.7810e-06 - acc: 1.0000\n",
      "Epoch 72/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 7.6740e-06 - acc: 1.0000\n",
      "Epoch 73/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 7.5986e-06 - acc: 1.0000\n",
      "Epoch 74/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 7.5066e-06 - acc: 1.0000\n",
      "Epoch 75/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 7.3935e-06 - acc: 1.0000\n",
      "Epoch 76/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 7.3143e-06 - acc: 1.0000\n",
      "Epoch 77/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 7.2213e-06 - acc: 1.0000\n",
      "Epoch 78/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 7.1707e-06 - acc: 1.0000\n",
      "Epoch 79/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 7.0695e-06 - acc: 1.0000\n",
      "Epoch 80/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 6.9710e-06 - acc: 1.0000\n",
      "Epoch 81/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 6.9139e-06 - acc: 1.0000\n",
      "Epoch 82/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 6.7825e-06 - acc: 1.0000\n",
      "Epoch 83/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 6.7129e-06 - acc: 1.0000\n",
      "Epoch 84/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 6.6314e-06 - acc: 1.0000\n",
      "Epoch 85/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 6.6012e-06 - acc: 1.0000\n",
      "Epoch 86/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 6.4684e-06 - acc: 1.0000\n",
      "Epoch 87/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 6.3954e-06 - acc: 1.0000\n",
      "Epoch 88/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 6.3322e-06 - acc: 1.0000\n",
      "Epoch 89/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 6.2334e-06 - acc: 1.0000\n",
      "Epoch 90/100\n",
      "351/351 [==============================] - 0s 43us/sample - loss: 6.2065e-06 - acc: 1.0000\n",
      "Epoch 91/100\n",
      "351/351 [==============================] - 0s 54us/sample - loss: 6.1108e-06 - acc: 1.0000\n",
      "Epoch 92/100\n",
      "351/351 [==============================] - 0s 34us/sample - loss: 6.0221e-06 - acc: 1.0000\n",
      "Epoch 93/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 5.9488e-06 - acc: 1.0000\n",
      "Epoch 94/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 5.8897e-06 - acc: 1.0000\n",
      "Epoch 95/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 5.8285e-06 - acc: 1.0000\n",
      "Epoch 96/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 5.7610e-06 - acc: 1.0000\n",
      "Epoch 97/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 5.6808e-06 - acc: 1.0000\n",
      "Epoch 98/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 5.5997e-06 - acc: 1.0000\n",
      "Epoch 99/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 5.5232e-06 - acc: 1.0000\n",
      "Epoch 100/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 5.4903e-06 - acc: 1.0000\n",
      "Train on 351 samples\n",
      "Epoch 1/100\n",
      "351/351 [==============================] - 0s 34us/sample - loss: 6.2808e-06 - acc: 1.0000\n",
      "Epoch 2/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 6.0798e-06 - acc: 1.0000\n",
      "Epoch 3/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 5.9511e-06 - acc: 1.0000\n",
      "Epoch 4/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 5.8265e-06 - acc: 1.0000\n",
      "Epoch 5/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 5.7015e-06 - acc: 1.0000\n",
      "Epoch 6/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 5.6068e-06 - acc: 1.0000\n",
      "Epoch 7/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 5.5168e-06 - acc: 1.0000\n",
      "Epoch 8/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 5.3935e-06 - acc: 1.0000\n",
      "Epoch 9/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 5.2998e-06 - acc: 1.0000\n",
      "Epoch 10/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 5.2152e-06 - acc: 1.0000\n",
      "Epoch 11/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 5.1357e-06 - acc: 1.0000\n",
      "Epoch 12/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 5.0508e-06 - acc: 1.0000\n",
      "Epoch 13/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 4.9900e-06 - acc: 1.0000\n",
      "Epoch 14/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 4.9106e-06 - acc: 1.0000\n",
      "Epoch 15/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 4.8331e-06 - acc: 1.0000\n",
      "Epoch 16/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 4.7639e-06 - acc: 1.0000\n",
      "Epoch 17/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 4.6905e-06 - acc: 1.0000\n",
      "Epoch 18/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 4.6280e-06 - acc: 1.0000\n",
      "Epoch 19/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 4.6154e-06 - acc: 1.0000\n",
      "Epoch 20/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 4.4806e-06 - acc: 1.0000\n",
      "Epoch 21/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 4.4551e-06 - acc: 1.0000\n",
      "Epoch 22/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 4.3808e-06 - acc: 1.0000\n",
      "Epoch 23/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 4.3132e-06 - acc: 1.0000\n",
      "Epoch 24/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 4.2375e-06 - acc: 1.0000\n",
      "Epoch 25/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 4.1869e-06 - acc: 1.0000\n",
      "Epoch 26/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 4.1312e-06 - acc: 1.0000\n",
      "Epoch 27/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 4.0918e-06 - acc: 1.0000\n",
      "Epoch 28/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 4.0333e-06 - acc: 1.0000\n",
      "Epoch 29/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 3.9658e-06 - acc: 1.0000\n",
      "Epoch 30/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 3.9369e-06 - acc: 1.0000\n",
      "Epoch 31/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 3.8717e-06 - acc: 1.0000\n",
      "Epoch 32/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 3.8286e-06 - acc: 1.0000\n",
      "Epoch 33/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 3.7623e-06 - acc: 1.0000\n",
      "Epoch 34/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 3.7270e-06 - acc: 1.0000\n",
      "Epoch 35/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 3.6717e-06 - acc: 1.0000\n",
      "Epoch 36/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 3.6384e-06 - acc: 1.0000\n",
      "Epoch 37/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 3.5830e-06 - acc: 1.0000\n",
      "Epoch 38/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 3.5324e-06 - acc: 1.0000\n",
      "Epoch 39/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 3.4920e-06 - acc: 1.0000\n",
      "Epoch 40/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 3.4519e-06 - acc: 1.0000\n",
      "Epoch 41/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 3.4078e-06 - acc: 1.0000\n",
      "Epoch 42/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 3.3623e-06 - acc: 1.0000\n",
      "Epoch 43/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 3.3269e-06 - acc: 1.0000\n",
      "Epoch 44/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 3.2926e-06 - acc: 1.0000\n",
      "Epoch 45/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 3.2549e-06 - acc: 1.0000\n",
      "Epoch 46/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 3.2159e-06 - acc: 1.0000\n",
      "Epoch 47/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 3.1619e-06 - acc: 1.0000\n",
      "Epoch 48/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 3.1357e-06 - acc: 1.0000\n",
      "Epoch 49/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 3.0865e-06 - acc: 1.0000\n",
      "Epoch 50/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 3.0519e-06 - acc: 1.0000\n",
      "Epoch 51/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 3.0145e-06 - acc: 1.0000\n",
      "Epoch 52/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 2.9761e-06 - acc: 1.0000\n",
      "Epoch 53/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 2.9343e-06 - acc: 1.0000\n",
      "Epoch 54/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 2.9065e-06 - acc: 1.0000\n",
      "Epoch 55/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 2.8698e-06 - acc: 1.0000\n",
      "Epoch 56/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 2.8386e-06 - acc: 1.0000\n",
      "Epoch 57/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 2.7992e-06 - acc: 1.0000\n",
      "Epoch 58/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 2.7720e-06 - acc: 1.0000\n",
      "Epoch 59/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 2.7438e-06 - acc: 1.0000\n",
      "Epoch 60/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 2.7044e-06 - acc: 1.0000\n",
      "Epoch 61/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 2.6813e-06 - acc: 1.0000\n",
      "Epoch 62/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 2.6514e-06 - acc: 1.0000\n",
      "Epoch 63/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 2.6158e-06 - acc: 1.0000\n",
      "Epoch 64/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 2.5873e-06 - acc: 1.0000\n",
      "Epoch 65/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 2.5530e-06 - acc: 1.0000\n",
      "Epoch 66/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 2.5275e-06 - acc: 1.0000\n",
      "Epoch 67/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 2.4908e-06 - acc: 1.0000\n",
      "Epoch 68/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 2.4664e-06 - acc: 1.0000\n",
      "Epoch 69/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 2.4368e-06 - acc: 1.0000\n",
      "Epoch 70/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 2.4140e-06 - acc: 1.0000\n",
      "Epoch 71/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 2.3818e-06 - acc: 1.0000\n",
      "Epoch 72/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 2.3563e-06 - acc: 1.0000\n",
      "Epoch 73/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 2.3288e-06 - acc: 1.0000\n",
      "Epoch 74/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 2.3006e-06 - acc: 1.0000\n",
      "Epoch 75/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 2.2772e-06 - acc: 1.0000\n",
      "Epoch 76/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 2.2500e-06 - acc: 1.0000\n",
      "Epoch 77/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 2.2391e-06 - acc: 1.0000\n",
      "Epoch 78/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 2.1994e-06 - acc: 1.0000\n",
      "Epoch 79/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 2.1722e-06 - acc: 1.0000\n",
      "Epoch 80/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 2.1519e-06 - acc: 1.0000\n",
      "Epoch 81/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 2.1237e-06 - acc: 1.0000\n",
      "Epoch 82/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 2.1036e-06 - acc: 1.0000\n",
      "Epoch 83/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 2.0758e-06 - acc: 1.0000\n",
      "Epoch 84/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 2.0530e-06 - acc: 1.0000\n",
      "Epoch 85/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 2.0299e-06 - acc: 1.0000\n",
      "Epoch 86/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 2.0079e-06 - acc: 1.0000\n",
      "Epoch 87/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 1.9943e-06 - acc: 1.0000\n",
      "Epoch 88/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 1.9569e-06 - acc: 1.0000\n",
      "Epoch 89/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 1.9403e-06 - acc: 1.0000\n",
      "Epoch 90/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 1.9179e-06 - acc: 1.0000\n",
      "Epoch 91/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 1.9046e-06 - acc: 1.0000\n",
      "Epoch 92/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 1.8717e-06 - acc: 1.0000\n",
      "Epoch 93/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 1.8557e-06 - acc: 1.0000\n",
      "Epoch 94/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 1.8364e-06 - acc: 1.0000\n",
      "Epoch 95/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 1.8085e-06 - acc: 1.0000\n",
      "Epoch 96/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 1.7966e-06 - acc: 1.0000\n",
      "Epoch 97/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 1.7739e-06 - acc: 1.0000\n",
      "Epoch 98/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 1.7474e-06 - acc: 1.0000\n",
      "Epoch 99/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 1.7345e-06 - acc: 1.0000\n",
      "Epoch 100/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 1.7148e-06 - acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "oof_train1 = np.zeros((x_train.shape[0], _N_CLASS))  #  _N_CLASS\n",
    "oof_test1 = np.empty((x_test.shape[0], _N_CLASS))  #  _N_CLASS\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kf.split(x_train,y_train)):\n",
    "    kf_X_train = x_train[train_index]  #  交叉验证划分此时的训练集和验证集\n",
    "    kf_y_train = y_train[train_index]  \n",
    "    kf_y_train = lb.transform(kf_y_train)\n",
    "    kf_X_test = x_train[test_index]  # 验证集\n",
    "\n",
    "    dnn.fit(kf_X_train, kf_y_train,batch_size=20, epochs=100, shuffle=True, verbose=1)  # 当前模型进行训练\n",
    "    \n",
    "    oof_train1[test_index] = dnn.predict(kf_X_test)  # 当前验证集进行概率预测， 200 * _N_CLASS\n",
    "    oof_test1 += dnn.predict(x_test)  # 对测试集概率预测 oof_test_skf[i, :] ，  500 * _N_CLASS\n",
    "\n",
    "oof_test1 /= _N_FOLDS  # 对每一则交叉验证的结果取平均"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oof(clf, X_train, y_train, X_test):\n",
    "    oof_train = np.zeros((X_train.shape[0], _N_CLASS))  # _N_CLASS\n",
    "    oof_test = np.empty((X_test.shape[0], _N_CLASS))  # _N_CLASS\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(X_train,y_train)):\n",
    "        kf_X_train = X_train[train_index]  # 交叉验证划分此时的训练集和验证集\n",
    "        kf_y_train = y_train[train_index]  # \n",
    "        kf_X_test = X_train[test_index]  #  验证集\n",
    "\n",
    "        clf.fit(kf_X_train, kf_y_train)  # 当前模型进行训练\n",
    "\n",
    "        oof_train[test_index] = clf.predict_proba(kf_X_test)  # 当前验证集进行概率预测， _N_CLASS\n",
    "        oof_test += clf.predict_proba(X_test)  # 对测试集概率预测 oof_test_skf[i, :] ， _N_CLASS\n",
    "\n",
    "    oof_test /= _N_FOLDS  # 对每一则交叉验证的结果取平均\n",
    "    return oof_train, oof_test  # 返回当前分类器对训练集和测试集的预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      "WARNING:tensorflow:From C:\\Users\\Administrator\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 8.124232\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 7.954976\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 7.649524\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 7.238707\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 6.688264\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 5.880399\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 5.137438\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 4.565395\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 4.211407\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 4.031536\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 3.705408\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 3.487309\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 3.295171\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 3.151223\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 3.011009\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 2.899978\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 2.766960\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 2.639979\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 2.578924\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 2.479945\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 5.631411\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 5.306138\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 4.620358\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 3.691277\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 2.872805\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 2.307014\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 2.043837\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 1.723918\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 1.523417\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 1.427969\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 1.316459\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 1.273913\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 1.061549\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 1.046544\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 1.024543\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 0.951206\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 0.897571\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 0.821675\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 0.823220\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 0.780068\n",
      "[END] Pre-training step\n",
      "WARNING:tensorflow:From C:\\Users\\Administrator\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.854637\n",
      ">> Epoch 1 finished \tANN training loss 0.729266\n",
      ">> Epoch 2 finished \tANN training loss 0.626754\n",
      ">> Epoch 3 finished \tANN training loss 0.595662\n",
      ">> Epoch 4 finished \tANN training loss 0.541378\n",
      ">> Epoch 5 finished \tANN training loss 0.491149\n",
      ">> Epoch 6 finished \tANN training loss 0.461068\n",
      ">> Epoch 7 finished \tANN training loss 0.443031\n",
      ">> Epoch 8 finished \tANN training loss 0.421166\n",
      ">> Epoch 9 finished \tANN training loss 0.380983\n",
      ">> Epoch 10 finished \tANN training loss 0.374429\n",
      ">> Epoch 11 finished \tANN training loss 0.358540\n",
      ">> Epoch 12 finished \tANN training loss 0.331561\n",
      ">> Epoch 13 finished \tANN training loss 0.317324\n",
      ">> Epoch 14 finished \tANN training loss 0.312586\n",
      ">> Epoch 15 finished \tANN training loss 0.305933\n",
      ">> Epoch 16 finished \tANN training loss 0.277811\n",
      ">> Epoch 17 finished \tANN training loss 0.283885\n",
      ">> Epoch 18 finished \tANN training loss 0.274957\n",
      ">> Epoch 19 finished \tANN training loss 0.257363\n",
      ">> Epoch 20 finished \tANN training loss 0.252296\n",
      ">> Epoch 21 finished \tANN training loss 0.260075\n",
      ">> Epoch 22 finished \tANN training loss 0.242476\n",
      ">> Epoch 23 finished \tANN training loss 0.235472\n",
      ">> Epoch 24 finished \tANN training loss 0.227612\n",
      ">> Epoch 25 finished \tANN training loss 0.223325\n",
      ">> Epoch 26 finished \tANN training loss 0.202666\n",
      ">> Epoch 27 finished \tANN training loss 0.191679\n",
      ">> Epoch 28 finished \tANN training loss 0.189957\n",
      ">> Epoch 29 finished \tANN training loss 0.203860\n",
      ">> Epoch 30 finished \tANN training loss 0.212894\n",
      ">> Epoch 31 finished \tANN training loss 0.169939\n",
      ">> Epoch 32 finished \tANN training loss 0.177746\n",
      ">> Epoch 33 finished \tANN training loss 0.182532\n",
      ">> Epoch 34 finished \tANN training loss 0.169327\n",
      ">> Epoch 35 finished \tANN training loss 0.157137\n",
      ">> Epoch 36 finished \tANN training loss 0.166059\n",
      ">> Epoch 37 finished \tANN training loss 0.156027\n",
      ">> Epoch 38 finished \tANN training loss 0.162187\n",
      ">> Epoch 39 finished \tANN training loss 0.153429\n",
      ">> Epoch 40 finished \tANN training loss 0.162510\n",
      ">> Epoch 41 finished \tANN training loss 0.156547\n",
      ">> Epoch 42 finished \tANN training loss 0.142774\n",
      ">> Epoch 43 finished \tANN training loss 0.149214\n",
      ">> Epoch 44 finished \tANN training loss 0.143468\n",
      ">> Epoch 45 finished \tANN training loss 0.135161\n",
      ">> Epoch 46 finished \tANN training loss 0.131281\n",
      ">> Epoch 47 finished \tANN training loss 0.152412\n",
      ">> Epoch 48 finished \tANN training loss 0.123988\n",
      ">> Epoch 49 finished \tANN training loss 0.122765\n",
      ">> Epoch 50 finished \tANN training loss 0.121136\n",
      ">> Epoch 51 finished \tANN training loss 0.118963\n",
      ">> Epoch 52 finished \tANN training loss 0.141216\n",
      ">> Epoch 53 finished \tANN training loss 0.116687\n",
      ">> Epoch 54 finished \tANN training loss 0.114112\n",
      ">> Epoch 55 finished \tANN training loss 0.137850\n",
      ">> Epoch 56 finished \tANN training loss 0.117522\n",
      ">> Epoch 57 finished \tANN training loss 0.111730\n",
      ">> Epoch 58 finished \tANN training loss 0.110864\n",
      ">> Epoch 59 finished \tANN training loss 0.113776\n",
      ">> Epoch 60 finished \tANN training loss 0.109459\n",
      ">> Epoch 61 finished \tANN training loss 0.105158\n",
      ">> Epoch 62 finished \tANN training loss 0.166911\n",
      ">> Epoch 63 finished \tANN training loss 0.096502\n",
      ">> Epoch 64 finished \tANN training loss 0.101606\n",
      ">> Epoch 65 finished \tANN training loss 0.095614\n",
      ">> Epoch 66 finished \tANN training loss 0.088530\n",
      ">> Epoch 67 finished \tANN training loss 0.090854\n",
      ">> Epoch 68 finished \tANN training loss 0.092762\n",
      ">> Epoch 69 finished \tANN training loss 0.088489\n",
      ">> Epoch 70 finished \tANN training loss 0.086460\n",
      ">> Epoch 71 finished \tANN training loss 0.091261\n",
      ">> Epoch 72 finished \tANN training loss 0.086263\n",
      ">> Epoch 73 finished \tANN training loss 0.085766\n",
      ">> Epoch 74 finished \tANN training loss 0.089520\n",
      ">> Epoch 75 finished \tANN training loss 0.099935\n",
      ">> Epoch 76 finished \tANN training loss 0.081569\n",
      ">> Epoch 77 finished \tANN training loss 0.081226\n",
      ">> Epoch 78 finished \tANN training loss 0.077801\n",
      ">> Epoch 79 finished \tANN training loss 0.098923\n",
      ">> Epoch 80 finished \tANN training loss 0.083023\n",
      ">> Epoch 81 finished \tANN training loss 0.075742\n",
      ">> Epoch 82 finished \tANN training loss 0.073865\n",
      ">> Epoch 83 finished \tANN training loss 0.090715\n",
      ">> Epoch 84 finished \tANN training loss 0.060116\n",
      ">> Epoch 85 finished \tANN training loss 0.081618\n",
      ">> Epoch 86 finished \tANN training loss 0.093805\n",
      ">> Epoch 87 finished \tANN training loss 0.068701\n",
      ">> Epoch 88 finished \tANN training loss 0.087263\n",
      ">> Epoch 89 finished \tANN training loss 0.066522\n",
      ">> Epoch 90 finished \tANN training loss 0.065989\n",
      ">> Epoch 91 finished \tANN training loss 0.068425\n",
      ">> Epoch 92 finished \tANN training loss 0.063176\n",
      ">> Epoch 93 finished \tANN training loss 0.064170\n",
      ">> Epoch 94 finished \tANN training loss 0.088310\n",
      ">> Epoch 95 finished \tANN training loss 0.076551\n",
      ">> Epoch 96 finished \tANN training loss 0.057743\n",
      ">> Epoch 97 finished \tANN training loss 0.061435\n",
      ">> Epoch 98 finished \tANN training loss 0.067024\n",
      ">> Epoch 99 finished \tANN training loss 0.058799\n",
      ">> Epoch 100 finished \tANN training loss 0.058928\n",
      ">> Epoch 101 finished \tANN training loss 0.073255\n",
      ">> Epoch 102 finished \tANN training loss 0.062719\n",
      ">> Epoch 103 finished \tANN training loss 0.061949\n",
      ">> Epoch 104 finished \tANN training loss 0.054981\n",
      ">> Epoch 105 finished \tANN training loss 0.052508\n",
      ">> Epoch 106 finished \tANN training loss 0.071235\n",
      ">> Epoch 107 finished \tANN training loss 0.047499\n",
      ">> Epoch 108 finished \tANN training loss 0.056342\n",
      ">> Epoch 109 finished \tANN training loss 0.051806\n",
      ">> Epoch 110 finished \tANN training loss 0.058752\n",
      ">> Epoch 111 finished \tANN training loss 0.050585\n",
      ">> Epoch 112 finished \tANN training loss 0.050175\n",
      ">> Epoch 113 finished \tANN training loss 0.050402\n",
      ">> Epoch 114 finished \tANN training loss 0.053829\n",
      ">> Epoch 115 finished \tANN training loss 0.040788\n",
      ">> Epoch 116 finished \tANN training loss 0.052410\n",
      ">> Epoch 117 finished \tANN training loss 0.047159\n",
      ">> Epoch 118 finished \tANN training loss 0.040515\n",
      ">> Epoch 119 finished \tANN training loss 0.048894\n",
      ">> Epoch 120 finished \tANN training loss 0.049485\n",
      ">> Epoch 121 finished \tANN training loss 0.042242\n",
      ">> Epoch 122 finished \tANN training loss 0.041483\n",
      ">> Epoch 123 finished \tANN training loss 0.047648\n",
      ">> Epoch 124 finished \tANN training loss 0.042696\n",
      ">> Epoch 125 finished \tANN training loss 0.047445\n",
      ">> Epoch 126 finished \tANN training loss 0.042528\n",
      ">> Epoch 127 finished \tANN training loss 0.034148\n",
      ">> Epoch 128 finished \tANN training loss 0.033964\n",
      ">> Epoch 129 finished \tANN training loss 0.035629\n",
      ">> Epoch 130 finished \tANN training loss 0.030652\n",
      ">> Epoch 131 finished \tANN training loss 0.034596\n",
      ">> Epoch 132 finished \tANN training loss 0.054677\n",
      ">> Epoch 133 finished \tANN training loss 0.038701\n",
      ">> Epoch 134 finished \tANN training loss 0.029782\n",
      ">> Epoch 135 finished \tANN training loss 0.031168\n",
      ">> Epoch 136 finished \tANN training loss 0.032659\n",
      ">> Epoch 137 finished \tANN training loss 0.033820\n",
      ">> Epoch 138 finished \tANN training loss 0.041011\n",
      ">> Epoch 139 finished \tANN training loss 0.031920\n",
      ">> Epoch 140 finished \tANN training loss 0.056569\n",
      ">> Epoch 141 finished \tANN training loss 0.035274\n",
      ">> Epoch 142 finished \tANN training loss 0.034510\n",
      ">> Epoch 143 finished \tANN training loss 0.046943\n",
      ">> Epoch 144 finished \tANN training loss 0.033799\n",
      ">> Epoch 145 finished \tANN training loss 0.041728\n",
      ">> Epoch 146 finished \tANN training loss 0.032524\n",
      ">> Epoch 147 finished \tANN training loss 0.028902\n",
      ">> Epoch 148 finished \tANN training loss 0.029610\n",
      ">> Epoch 149 finished \tANN training loss 0.030706\n",
      ">> Epoch 150 finished \tANN training loss 0.034502\n",
      ">> Epoch 151 finished \tANN training loss 0.033125\n",
      ">> Epoch 152 finished \tANN training loss 0.025539\n",
      ">> Epoch 153 finished \tANN training loss 0.034453\n",
      ">> Epoch 154 finished \tANN training loss 0.034852\n",
      ">> Epoch 155 finished \tANN training loss 0.031760\n",
      ">> Epoch 156 finished \tANN training loss 0.035036\n",
      ">> Epoch 157 finished \tANN training loss 0.035277\n",
      ">> Epoch 158 finished \tANN training loss 0.030685\n",
      ">> Epoch 159 finished \tANN training loss 0.032316\n",
      ">> Epoch 160 finished \tANN training loss 0.028953\n",
      ">> Epoch 161 finished \tANN training loss 0.029021\n",
      ">> Epoch 162 finished \tANN training loss 0.038015\n",
      ">> Epoch 163 finished \tANN training loss 0.024614\n",
      ">> Epoch 164 finished \tANN training loss 0.027659\n",
      ">> Epoch 165 finished \tANN training loss 0.022001\n",
      ">> Epoch 166 finished \tANN training loss 0.023688\n",
      ">> Epoch 167 finished \tANN training loss 0.022423\n",
      ">> Epoch 168 finished \tANN training loss 0.024763\n",
      ">> Epoch 169 finished \tANN training loss 0.025039\n",
      ">> Epoch 170 finished \tANN training loss 0.039329\n",
      ">> Epoch 171 finished \tANN training loss 0.028514\n",
      ">> Epoch 172 finished \tANN training loss 0.032147\n",
      ">> Epoch 173 finished \tANN training loss 0.022033\n",
      ">> Epoch 174 finished \tANN training loss 0.032615\n",
      ">> Epoch 175 finished \tANN training loss 0.021058\n",
      ">> Epoch 176 finished \tANN training loss 0.021774\n",
      ">> Epoch 177 finished \tANN training loss 0.022747\n",
      ">> Epoch 178 finished \tANN training loss 0.019958\n",
      ">> Epoch 179 finished \tANN training loss 0.024932\n",
      ">> Epoch 180 finished \tANN training loss 0.023272\n",
      ">> Epoch 181 finished \tANN training loss 0.027647\n",
      ">> Epoch 182 finished \tANN training loss 0.034235\n",
      ">> Epoch 183 finished \tANN training loss 0.023922\n",
      ">> Epoch 184 finished \tANN training loss 0.020297\n",
      ">> Epoch 185 finished \tANN training loss 0.026846\n",
      ">> Epoch 186 finished \tANN training loss 0.024260\n",
      ">> Epoch 187 finished \tANN training loss 0.022532\n",
      ">> Epoch 188 finished \tANN training loss 0.020216\n",
      ">> Epoch 189 finished \tANN training loss 0.031997\n",
      ">> Epoch 190 finished \tANN training loss 0.025612\n",
      ">> Epoch 191 finished \tANN training loss 0.027869\n",
      ">> Epoch 192 finished \tANN training loss 0.020376\n",
      ">> Epoch 193 finished \tANN training loss 0.026046\n",
      ">> Epoch 194 finished \tANN training loss 0.025848\n",
      ">> Epoch 195 finished \tANN training loss 0.018921\n",
      ">> Epoch 196 finished \tANN training loss 0.017045\n",
      ">> Epoch 197 finished \tANN training loss 0.023844\n",
      ">> Epoch 198 finished \tANN training loss 0.018108\n",
      ">> Epoch 199 finished \tANN training loss 0.021363\n",
      ">> Epoch 200 finished \tANN training loss 0.023771\n",
      ">> Epoch 201 finished \tANN training loss 0.021135\n",
      ">> Epoch 202 finished \tANN training loss 0.021395\n",
      ">> Epoch 203 finished \tANN training loss 0.022547\n",
      ">> Epoch 204 finished \tANN training loss 0.024673\n",
      ">> Epoch 205 finished \tANN training loss 0.023433\n",
      ">> Epoch 206 finished \tANN training loss 0.018673\n",
      ">> Epoch 207 finished \tANN training loss 0.031909\n",
      ">> Epoch 208 finished \tANN training loss 0.023243\n",
      ">> Epoch 209 finished \tANN training loss 0.019615\n",
      ">> Epoch 210 finished \tANN training loss 0.029896\n",
      ">> Epoch 211 finished \tANN training loss 0.035660\n",
      ">> Epoch 212 finished \tANN training loss 0.024827\n",
      ">> Epoch 213 finished \tANN training loss 0.018312\n",
      ">> Epoch 214 finished \tANN training loss 0.016691\n",
      ">> Epoch 215 finished \tANN training loss 0.020013\n",
      ">> Epoch 216 finished \tANN training loss 0.030730\n",
      ">> Epoch 217 finished \tANN training loss 0.016131\n",
      ">> Epoch 218 finished \tANN training loss 0.019141\n",
      ">> Epoch 219 finished \tANN training loss 0.017401\n",
      ">> Epoch 220 finished \tANN training loss 0.016771\n",
      ">> Epoch 221 finished \tANN training loss 0.014861\n",
      ">> Epoch 222 finished \tANN training loss 0.015865\n",
      ">> Epoch 223 finished \tANN training loss 0.016792\n",
      ">> Epoch 224 finished \tANN training loss 0.029753\n",
      ">> Epoch 225 finished \tANN training loss 0.023402\n",
      ">> Epoch 226 finished \tANN training loss 0.019368\n",
      ">> Epoch 227 finished \tANN training loss 0.018469\n",
      ">> Epoch 228 finished \tANN training loss 0.015726\n",
      ">> Epoch 229 finished \tANN training loss 0.019383\n",
      ">> Epoch 230 finished \tANN training loss 0.018019\n",
      ">> Epoch 231 finished \tANN training loss 0.015466\n",
      ">> Epoch 232 finished \tANN training loss 0.015918\n",
      ">> Epoch 233 finished \tANN training loss 0.015505\n",
      ">> Epoch 234 finished \tANN training loss 0.014087\n",
      ">> Epoch 235 finished \tANN training loss 0.017382\n",
      ">> Epoch 236 finished \tANN training loss 0.017398\n",
      ">> Epoch 237 finished \tANN training loss 0.017252\n",
      ">> Epoch 238 finished \tANN training loss 0.016342\n",
      ">> Epoch 239 finished \tANN training loss 0.018838\n",
      ">> Epoch 240 finished \tANN training loss 0.022816\n",
      ">> Epoch 241 finished \tANN training loss 0.024050\n",
      ">> Epoch 242 finished \tANN training loss 0.017832\n",
      ">> Epoch 243 finished \tANN training loss 0.015215\n",
      ">> Epoch 244 finished \tANN training loss 0.015657\n",
      ">> Epoch 245 finished \tANN training loss 0.017691\n",
      ">> Epoch 246 finished \tANN training loss 0.018503\n",
      ">> Epoch 247 finished \tANN training loss 0.016572\n",
      ">> Epoch 248 finished \tANN training loss 0.018603\n",
      ">> Epoch 249 finished \tANN training loss 0.011990\n",
      ">> Epoch 250 finished \tANN training loss 0.014365\n",
      ">> Epoch 251 finished \tANN training loss 0.015181\n",
      ">> Epoch 252 finished \tANN training loss 0.013907\n",
      ">> Epoch 253 finished \tANN training loss 0.014814\n",
      ">> Epoch 254 finished \tANN training loss 0.016862\n",
      ">> Epoch 255 finished \tANN training loss 0.013728\n",
      ">> Epoch 256 finished \tANN training loss 0.018136\n",
      ">> Epoch 257 finished \tANN training loss 0.016732\n",
      ">> Epoch 258 finished \tANN training loss 0.016505\n",
      ">> Epoch 259 finished \tANN training loss 0.015407\n",
      ">> Epoch 260 finished \tANN training loss 0.017465\n",
      ">> Epoch 261 finished \tANN training loss 0.023394\n",
      ">> Epoch 262 finished \tANN training loss 0.012830\n",
      ">> Epoch 263 finished \tANN training loss 0.019014\n",
      ">> Epoch 264 finished \tANN training loss 0.021028\n",
      ">> Epoch 265 finished \tANN training loss 0.016216\n",
      ">> Epoch 266 finished \tANN training loss 0.014974\n",
      ">> Epoch 267 finished \tANN training loss 0.023587\n",
      ">> Epoch 268 finished \tANN training loss 0.017110\n",
      ">> Epoch 269 finished \tANN training loss 0.016811\n",
      ">> Epoch 270 finished \tANN training loss 0.016328\n",
      ">> Epoch 271 finished \tANN training loss 0.015704\n",
      ">> Epoch 272 finished \tANN training loss 0.012988\n",
      ">> Epoch 273 finished \tANN training loss 0.017015\n",
      ">> Epoch 274 finished \tANN training loss 0.014338\n",
      ">> Epoch 275 finished \tANN training loss 0.015176\n",
      ">> Epoch 276 finished \tANN training loss 0.015140\n",
      ">> Epoch 277 finished \tANN training loss 0.012567\n",
      ">> Epoch 278 finished \tANN training loss 0.025822\n",
      ">> Epoch 279 finished \tANN training loss 0.023109\n",
      ">> Epoch 280 finished \tANN training loss 0.019617\n",
      ">> Epoch 281 finished \tANN training loss 0.015040\n",
      ">> Epoch 282 finished \tANN training loss 0.015613\n",
      ">> Epoch 283 finished \tANN training loss 0.018190\n",
      ">> Epoch 284 finished \tANN training loss 0.021770\n",
      ">> Epoch 285 finished \tANN training loss 0.014529\n",
      ">> Epoch 286 finished \tANN training loss 0.015495\n",
      ">> Epoch 287 finished \tANN training loss 0.013284\n",
      ">> Epoch 288 finished \tANN training loss 0.016172\n",
      ">> Epoch 289 finished \tANN training loss 0.013212\n",
      ">> Epoch 290 finished \tANN training loss 0.008915\n",
      ">> Epoch 291 finished \tANN training loss 0.008319\n",
      ">> Epoch 292 finished \tANN training loss 0.009923\n",
      ">> Epoch 293 finished \tANN training loss 0.009146\n",
      ">> Epoch 294 finished \tANN training loss 0.011783\n",
      ">> Epoch 295 finished \tANN training loss 0.011812\n",
      ">> Epoch 296 finished \tANN training loss 0.012578\n",
      ">> Epoch 297 finished \tANN training loss 0.011926\n",
      ">> Epoch 298 finished \tANN training loss 0.014408\n",
      ">> Epoch 299 finished \tANN training loss 0.012058\n",
      ">> Epoch 300 finished \tANN training loss 0.016128\n",
      ">> Epoch 301 finished \tANN training loss 0.029618\n",
      ">> Epoch 302 finished \tANN training loss 0.015941\n",
      ">> Epoch 303 finished \tANN training loss 0.012695\n",
      ">> Epoch 304 finished \tANN training loss 0.011877\n",
      ">> Epoch 305 finished \tANN training loss 0.013038\n",
      ">> Epoch 306 finished \tANN training loss 0.011456\n",
      ">> Epoch 307 finished \tANN training loss 0.011716\n",
      ">> Epoch 308 finished \tANN training loss 0.008741\n",
      ">> Epoch 309 finished \tANN training loss 0.009884\n",
      ">> Epoch 310 finished \tANN training loss 0.012810\n",
      ">> Epoch 311 finished \tANN training loss 0.013615\n",
      ">> Epoch 312 finished \tANN training loss 0.011596\n",
      ">> Epoch 313 finished \tANN training loss 0.009855\n",
      ">> Epoch 314 finished \tANN training loss 0.009041\n",
      ">> Epoch 315 finished \tANN training loss 0.009100\n",
      ">> Epoch 316 finished \tANN training loss 0.010335\n",
      ">> Epoch 317 finished \tANN training loss 0.010635\n",
      ">> Epoch 318 finished \tANN training loss 0.011545\n",
      ">> Epoch 319 finished \tANN training loss 0.013404\n",
      ">> Epoch 320 finished \tANN training loss 0.015244\n",
      ">> Epoch 321 finished \tANN training loss 0.014132\n",
      ">> Epoch 322 finished \tANN training loss 0.010107\n",
      ">> Epoch 323 finished \tANN training loss 0.012637\n",
      ">> Epoch 324 finished \tANN training loss 0.010641\n",
      ">> Epoch 325 finished \tANN training loss 0.010754\n",
      ">> Epoch 326 finished \tANN training loss 0.008713\n",
      ">> Epoch 327 finished \tANN training loss 0.010405\n",
      ">> Epoch 328 finished \tANN training loss 0.010666\n",
      ">> Epoch 329 finished \tANN training loss 0.008790\n",
      ">> Epoch 330 finished \tANN training loss 0.011364\n",
      ">> Epoch 331 finished \tANN training loss 0.019161\n",
      ">> Epoch 332 finished \tANN training loss 0.012820\n",
      ">> Epoch 333 finished \tANN training loss 0.011984\n",
      ">> Epoch 334 finished \tANN training loss 0.011734\n",
      ">> Epoch 335 finished \tANN training loss 0.011335\n",
      ">> Epoch 336 finished \tANN training loss 0.011090\n",
      ">> Epoch 337 finished \tANN training loss 0.009254\n",
      ">> Epoch 338 finished \tANN training loss 0.010043\n",
      ">> Epoch 339 finished \tANN training loss 0.010687\n",
      ">> Epoch 340 finished \tANN training loss 0.011321\n",
      ">> Epoch 341 finished \tANN training loss 0.009058\n",
      ">> Epoch 342 finished \tANN training loss 0.014859\n",
      ">> Epoch 343 finished \tANN training loss 0.013153\n",
      ">> Epoch 344 finished \tANN training loss 0.009182\n",
      ">> Epoch 345 finished \tANN training loss 0.007660\n",
      ">> Epoch 346 finished \tANN training loss 0.011478\n",
      ">> Epoch 347 finished \tANN training loss 0.010674\n",
      ">> Epoch 348 finished \tANN training loss 0.013052\n",
      ">> Epoch 349 finished \tANN training loss 0.008670\n",
      ">> Epoch 350 finished \tANN training loss 0.010771\n",
      ">> Epoch 351 finished \tANN training loss 0.008756\n",
      ">> Epoch 352 finished \tANN training loss 0.009338\n",
      ">> Epoch 353 finished \tANN training loss 0.011404\n",
      ">> Epoch 354 finished \tANN training loss 0.010963\n",
      ">> Epoch 355 finished \tANN training loss 0.009662\n",
      ">> Epoch 356 finished \tANN training loss 0.018142\n",
      ">> Epoch 357 finished \tANN training loss 0.015871\n",
      ">> Epoch 358 finished \tANN training loss 0.011565\n",
      ">> Epoch 359 finished \tANN training loss 0.011510\n",
      ">> Epoch 360 finished \tANN training loss 0.008567\n",
      ">> Epoch 361 finished \tANN training loss 0.008026\n",
      ">> Epoch 362 finished \tANN training loss 0.008782\n",
      ">> Epoch 363 finished \tANN training loss 0.009630\n",
      ">> Epoch 364 finished \tANN training loss 0.009609\n",
      ">> Epoch 365 finished \tANN training loss 0.009946\n",
      ">> Epoch 366 finished \tANN training loss 0.012341\n",
      ">> Epoch 367 finished \tANN training loss 0.009156\n",
      ">> Epoch 368 finished \tANN training loss 0.007571\n",
      ">> Epoch 369 finished \tANN training loss 0.012393\n",
      ">> Epoch 370 finished \tANN training loss 0.007126\n",
      ">> Epoch 371 finished \tANN training loss 0.010185\n",
      ">> Epoch 372 finished \tANN training loss 0.008714\n",
      ">> Epoch 373 finished \tANN training loss 0.010033\n",
      ">> Epoch 374 finished \tANN training loss 0.009817\n",
      ">> Epoch 375 finished \tANN training loss 0.009807\n",
      ">> Epoch 376 finished \tANN training loss 0.007529\n",
      ">> Epoch 377 finished \tANN training loss 0.008170\n",
      ">> Epoch 378 finished \tANN training loss 0.007249\n",
      ">> Epoch 379 finished \tANN training loss 0.011679\n",
      ">> Epoch 380 finished \tANN training loss 0.009513\n",
      ">> Epoch 381 finished \tANN training loss 0.011934\n",
      ">> Epoch 382 finished \tANN training loss 0.008055\n",
      ">> Epoch 383 finished \tANN training loss 0.009569\n",
      ">> Epoch 384 finished \tANN training loss 0.008221\n",
      ">> Epoch 385 finished \tANN training loss 0.006877\n",
      ">> Epoch 386 finished \tANN training loss 0.011598\n",
      ">> Epoch 387 finished \tANN training loss 0.007331\n",
      ">> Epoch 388 finished \tANN training loss 0.009472\n",
      ">> Epoch 389 finished \tANN training loss 0.007270\n",
      ">> Epoch 390 finished \tANN training loss 0.009221\n",
      ">> Epoch 391 finished \tANN training loss 0.007126\n",
      ">> Epoch 392 finished \tANN training loss 0.006356\n",
      ">> Epoch 393 finished \tANN training loss 0.007691\n",
      ">> Epoch 394 finished \tANN training loss 0.005545\n",
      ">> Epoch 395 finished \tANN training loss 0.006268\n",
      ">> Epoch 396 finished \tANN training loss 0.014394\n",
      ">> Epoch 397 finished \tANN training loss 0.005508\n",
      ">> Epoch 398 finished \tANN training loss 0.005462\n",
      ">> Epoch 399 finished \tANN training loss 0.011751\n",
      ">> Epoch 400 finished \tANN training loss 0.009165\n",
      ">> Epoch 401 finished \tANN training loss 0.008836\n",
      ">> Epoch 402 finished \tANN training loss 0.006628\n",
      ">> Epoch 403 finished \tANN training loss 0.005393\n",
      ">> Epoch 404 finished \tANN training loss 0.008005\n",
      ">> Epoch 405 finished \tANN training loss 0.009842\n",
      ">> Epoch 406 finished \tANN training loss 0.009683\n",
      ">> Epoch 407 finished \tANN training loss 0.007227\n",
      ">> Epoch 408 finished \tANN training loss 0.009409\n",
      ">> Epoch 409 finished \tANN training loss 0.007085\n",
      ">> Epoch 410 finished \tANN training loss 0.006907\n",
      ">> Epoch 411 finished \tANN training loss 0.007163\n",
      ">> Epoch 412 finished \tANN training loss 0.006957\n",
      ">> Epoch 413 finished \tANN training loss 0.009413\n",
      ">> Epoch 414 finished \tANN training loss 0.009839\n",
      ">> Epoch 415 finished \tANN training loss 0.010475\n",
      ">> Epoch 416 finished \tANN training loss 0.006719\n",
      ">> Epoch 417 finished \tANN training loss 0.009143\n",
      ">> Epoch 418 finished \tANN training loss 0.008562\n",
      ">> Epoch 419 finished \tANN training loss 0.008859\n",
      ">> Epoch 420 finished \tANN training loss 0.008930\n",
      ">> Epoch 421 finished \tANN training loss 0.009431\n",
      ">> Epoch 422 finished \tANN training loss 0.007878\n",
      ">> Epoch 423 finished \tANN training loss 0.007015\n",
      ">> Epoch 424 finished \tANN training loss 0.006356\n",
      ">> Epoch 425 finished \tANN training loss 0.009287\n",
      ">> Epoch 426 finished \tANN training loss 0.008947\n",
      ">> Epoch 427 finished \tANN training loss 0.013358\n",
      ">> Epoch 428 finished \tANN training loss 0.008253\n",
      ">> Epoch 429 finished \tANN training loss 0.008720\n",
      ">> Epoch 430 finished \tANN training loss 0.008989\n",
      ">> Epoch 431 finished \tANN training loss 0.007042\n",
      ">> Epoch 432 finished \tANN training loss 0.009310\n",
      ">> Epoch 433 finished \tANN training loss 0.007361\n",
      ">> Epoch 434 finished \tANN training loss 0.005725\n",
      ">> Epoch 435 finished \tANN training loss 0.005303\n",
      ">> Epoch 436 finished \tANN training loss 0.005574\n",
      ">> Epoch 437 finished \tANN training loss 0.007512\n",
      ">> Epoch 438 finished \tANN training loss 0.005969\n",
      ">> Epoch 439 finished \tANN training loss 0.007988\n",
      ">> Epoch 440 finished \tANN training loss 0.008379\n",
      ">> Epoch 441 finished \tANN training loss 0.009089\n",
      ">> Epoch 442 finished \tANN training loss 0.009982\n",
      ">> Epoch 443 finished \tANN training loss 0.007556\n",
      ">> Epoch 444 finished \tANN training loss 0.008735\n",
      ">> Epoch 445 finished \tANN training loss 0.008195\n",
      ">> Epoch 446 finished \tANN training loss 0.007606\n",
      ">> Epoch 447 finished \tANN training loss 0.008106\n",
      ">> Epoch 448 finished \tANN training loss 0.011016\n",
      ">> Epoch 449 finished \tANN training loss 0.010108\n",
      ">> Epoch 450 finished \tANN training loss 0.008828\n",
      ">> Epoch 451 finished \tANN training loss 0.008854\n",
      ">> Epoch 452 finished \tANN training loss 0.005717\n",
      ">> Epoch 453 finished \tANN training loss 0.006168\n",
      ">> Epoch 454 finished \tANN training loss 0.006637\n",
      ">> Epoch 455 finished \tANN training loss 0.007386\n",
      ">> Epoch 456 finished \tANN training loss 0.006378\n",
      ">> Epoch 457 finished \tANN training loss 0.006195\n",
      ">> Epoch 458 finished \tANN training loss 0.005829\n",
      ">> Epoch 459 finished \tANN training loss 0.006479\n",
      ">> Epoch 460 finished \tANN training loss 0.007324\n",
      ">> Epoch 461 finished \tANN training loss 0.005822\n",
      ">> Epoch 462 finished \tANN training loss 0.010535\n",
      ">> Epoch 463 finished \tANN training loss 0.005808\n",
      ">> Epoch 464 finished \tANN training loss 0.008028\n",
      ">> Epoch 465 finished \tANN training loss 0.006687\n",
      ">> Epoch 466 finished \tANN training loss 0.004516\n",
      ">> Epoch 467 finished \tANN training loss 0.004781\n",
      ">> Epoch 468 finished \tANN training loss 0.006160\n",
      ">> Epoch 469 finished \tANN training loss 0.008759\n",
      ">> Epoch 470 finished \tANN training loss 0.006653\n",
      ">> Epoch 471 finished \tANN training loss 0.005033\n",
      ">> Epoch 472 finished \tANN training loss 0.007133\n",
      ">> Epoch 473 finished \tANN training loss 0.007345\n",
      ">> Epoch 474 finished \tANN training loss 0.004432\n",
      ">> Epoch 475 finished \tANN training loss 0.004947\n",
      ">> Epoch 476 finished \tANN training loss 0.004802\n",
      ">> Epoch 477 finished \tANN training loss 0.004183\n",
      ">> Epoch 478 finished \tANN training loss 0.004748\n",
      ">> Epoch 479 finished \tANN training loss 0.004463\n",
      ">> Epoch 480 finished \tANN training loss 0.005184\n",
      ">> Epoch 481 finished \tANN training loss 0.005436\n",
      ">> Epoch 482 finished \tANN training loss 0.006121\n",
      ">> Epoch 483 finished \tANN training loss 0.009853\n",
      ">> Epoch 484 finished \tANN training loss 0.006130\n",
      ">> Epoch 485 finished \tANN training loss 0.007930\n",
      ">> Epoch 486 finished \tANN training loss 0.008131\n",
      ">> Epoch 487 finished \tANN training loss 0.006353\n",
      ">> Epoch 488 finished \tANN training loss 0.008017\n",
      ">> Epoch 489 finished \tANN training loss 0.006822\n",
      ">> Epoch 490 finished \tANN training loss 0.004811\n",
      ">> Epoch 491 finished \tANN training loss 0.007220\n",
      ">> Epoch 492 finished \tANN training loss 0.003905\n",
      ">> Epoch 493 finished \tANN training loss 0.005580\n",
      ">> Epoch 494 finished \tANN training loss 0.004723\n",
      ">> Epoch 495 finished \tANN training loss 0.006062\n",
      ">> Epoch 496 finished \tANN training loss 0.006525\n",
      ">> Epoch 497 finished \tANN training loss 0.009774\n",
      ">> Epoch 498 finished \tANN training loss 0.006345\n",
      ">> Epoch 499 finished \tANN training loss 0.006246\n",
      ">> Epoch 500 finished \tANN training loss 0.006693\n",
      ">> Epoch 501 finished \tANN training loss 0.007411\n",
      ">> Epoch 502 finished \tANN training loss 0.006633\n",
      ">> Epoch 503 finished \tANN training loss 0.006463\n",
      ">> Epoch 504 finished \tANN training loss 0.006855\n",
      ">> Epoch 505 finished \tANN training loss 0.008443\n",
      ">> Epoch 506 finished \tANN training loss 0.006743\n",
      ">> Epoch 507 finished \tANN training loss 0.005946\n",
      ">> Epoch 508 finished \tANN training loss 0.006277\n",
      ">> Epoch 509 finished \tANN training loss 0.005705\n",
      ">> Epoch 510 finished \tANN training loss 0.011232\n",
      ">> Epoch 511 finished \tANN training loss 0.006601\n",
      ">> Epoch 512 finished \tANN training loss 0.005958\n",
      ">> Epoch 513 finished \tANN training loss 0.006098\n",
      ">> Epoch 514 finished \tANN training loss 0.005897\n",
      ">> Epoch 515 finished \tANN training loss 0.009099\n",
      ">> Epoch 516 finished \tANN training loss 0.007606\n",
      ">> Epoch 517 finished \tANN training loss 0.007865\n",
      ">> Epoch 518 finished \tANN training loss 0.008685\n",
      ">> Epoch 519 finished \tANN training loss 0.008801\n",
      ">> Epoch 520 finished \tANN training loss 0.006688\n",
      ">> Epoch 521 finished \tANN training loss 0.006753\n",
      ">> Epoch 522 finished \tANN training loss 0.005982\n",
      ">> Epoch 523 finished \tANN training loss 0.005770\n",
      ">> Epoch 524 finished \tANN training loss 0.007802\n",
      ">> Epoch 525 finished \tANN training loss 0.005216\n",
      ">> Epoch 526 finished \tANN training loss 0.005175\n",
      ">> Epoch 527 finished \tANN training loss 0.004342\n",
      ">> Epoch 528 finished \tANN training loss 0.006701\n",
      ">> Epoch 529 finished \tANN training loss 0.006676\n",
      ">> Epoch 530 finished \tANN training loss 0.007048\n",
      ">> Epoch 531 finished \tANN training loss 0.006307\n",
      ">> Epoch 532 finished \tANN training loss 0.004799\n",
      ">> Epoch 533 finished \tANN training loss 0.005664\n",
      ">> Epoch 534 finished \tANN training loss 0.005537\n",
      ">> Epoch 535 finished \tANN training loss 0.006091\n",
      ">> Epoch 536 finished \tANN training loss 0.006796\n",
      ">> Epoch 537 finished \tANN training loss 0.006530\n",
      ">> Epoch 538 finished \tANN training loss 0.007480\n",
      ">> Epoch 539 finished \tANN training loss 0.004584\n",
      ">> Epoch 540 finished \tANN training loss 0.004347\n",
      ">> Epoch 541 finished \tANN training loss 0.005065\n",
      ">> Epoch 542 finished \tANN training loss 0.005808\n",
      ">> Epoch 543 finished \tANN training loss 0.006199\n",
      ">> Epoch 544 finished \tANN training loss 0.005851\n",
      ">> Epoch 545 finished \tANN training loss 0.007998\n",
      ">> Epoch 546 finished \tANN training loss 0.006972\n",
      ">> Epoch 547 finished \tANN training loss 0.010670\n",
      ">> Epoch 548 finished \tANN training loss 0.009756\n",
      ">> Epoch 549 finished \tANN training loss 0.005374\n",
      ">> Epoch 550 finished \tANN training loss 0.005009\n",
      ">> Epoch 551 finished \tANN training loss 0.004782\n",
      ">> Epoch 552 finished \tANN training loss 0.004446\n",
      ">> Epoch 553 finished \tANN training loss 0.004391\n",
      ">> Epoch 554 finished \tANN training loss 0.006300\n",
      ">> Epoch 555 finished \tANN training loss 0.005924\n",
      ">> Epoch 556 finished \tANN training loss 0.005067\n",
      ">> Epoch 557 finished \tANN training loss 0.009064\n",
      ">> Epoch 558 finished \tANN training loss 0.006877\n",
      ">> Epoch 559 finished \tANN training loss 0.005622\n",
      ">> Epoch 560 finished \tANN training loss 0.007664\n",
      ">> Epoch 561 finished \tANN training loss 0.005899\n",
      ">> Epoch 562 finished \tANN training loss 0.005778\n",
      ">> Epoch 563 finished \tANN training loss 0.007549\n",
      ">> Epoch 564 finished \tANN training loss 0.008857\n",
      ">> Epoch 565 finished \tANN training loss 0.005107\n",
      ">> Epoch 566 finished \tANN training loss 0.005695\n",
      ">> Epoch 567 finished \tANN training loss 0.004597\n",
      ">> Epoch 568 finished \tANN training loss 0.004698\n",
      ">> Epoch 569 finished \tANN training loss 0.005782\n",
      ">> Epoch 570 finished \tANN training loss 0.005114\n",
      ">> Epoch 571 finished \tANN training loss 0.005290\n",
      ">> Epoch 572 finished \tANN training loss 0.004458\n",
      ">> Epoch 573 finished \tANN training loss 0.003747\n",
      ">> Epoch 574 finished \tANN training loss 0.003676\n",
      ">> Epoch 575 finished \tANN training loss 0.005985\n",
      ">> Epoch 576 finished \tANN training loss 0.005662\n",
      ">> Epoch 577 finished \tANN training loss 0.005567\n",
      ">> Epoch 578 finished \tANN training loss 0.004632\n",
      ">> Epoch 579 finished \tANN training loss 0.005451\n",
      ">> Epoch 580 finished \tANN training loss 0.004122\n",
      ">> Epoch 581 finished \tANN training loss 0.003467\n",
      ">> Epoch 582 finished \tANN training loss 0.004241\n",
      ">> Epoch 583 finished \tANN training loss 0.005789\n",
      ">> Epoch 584 finished \tANN training loss 0.006068\n",
      ">> Epoch 585 finished \tANN training loss 0.005090\n",
      ">> Epoch 586 finished \tANN training loss 0.005735\n",
      ">> Epoch 587 finished \tANN training loss 0.005673\n",
      ">> Epoch 588 finished \tANN training loss 0.005297\n",
      ">> Epoch 589 finished \tANN training loss 0.004940\n",
      ">> Epoch 590 finished \tANN training loss 0.004563\n",
      ">> Epoch 591 finished \tANN training loss 0.003352\n",
      ">> Epoch 592 finished \tANN training loss 0.004863\n",
      ">> Epoch 593 finished \tANN training loss 0.004524\n",
      ">> Epoch 594 finished \tANN training loss 0.004203\n",
      ">> Epoch 595 finished \tANN training loss 0.005331\n",
      ">> Epoch 596 finished \tANN training loss 0.007894\n",
      ">> Epoch 597 finished \tANN training loss 0.004953\n",
      ">> Epoch 598 finished \tANN training loss 0.005327\n",
      ">> Epoch 599 finished \tANN training loss 0.005237\n",
      ">> Epoch 600 finished \tANN training loss 0.005551\n",
      ">> Epoch 601 finished \tANN training loss 0.004904\n",
      ">> Epoch 602 finished \tANN training loss 0.005164\n",
      ">> Epoch 603 finished \tANN training loss 0.004575\n",
      ">> Epoch 604 finished \tANN training loss 0.004181\n",
      ">> Epoch 605 finished \tANN training loss 0.003341\n",
      ">> Epoch 606 finished \tANN training loss 0.003029\n",
      ">> Epoch 607 finished \tANN training loss 0.004920\n",
      ">> Epoch 608 finished \tANN training loss 0.004375\n",
      ">> Epoch 609 finished \tANN training loss 0.005692\n",
      ">> Epoch 610 finished \tANN training loss 0.004694\n",
      ">> Epoch 611 finished \tANN training loss 0.004540\n",
      ">> Epoch 612 finished \tANN training loss 0.008318\n",
      ">> Epoch 613 finished \tANN training loss 0.006151\n",
      ">> Epoch 614 finished \tANN training loss 0.006025\n",
      ">> Epoch 615 finished \tANN training loss 0.005952\n",
      ">> Epoch 616 finished \tANN training loss 0.005880\n",
      ">> Epoch 617 finished \tANN training loss 0.005383\n",
      ">> Epoch 618 finished \tANN training loss 0.004742\n",
      ">> Epoch 619 finished \tANN training loss 0.005986\n",
      ">> Epoch 620 finished \tANN training loss 0.005358\n",
      ">> Epoch 621 finished \tANN training loss 0.003812\n",
      ">> Epoch 622 finished \tANN training loss 0.003524\n",
      ">> Epoch 623 finished \tANN training loss 0.002915\n",
      ">> Epoch 624 finished \tANN training loss 0.003003\n",
      ">> Epoch 625 finished \tANN training loss 0.003722\n",
      ">> Epoch 626 finished \tANN training loss 0.003820\n",
      ">> Epoch 627 finished \tANN training loss 0.003488\n",
      ">> Epoch 628 finished \tANN training loss 0.003713\n",
      ">> Epoch 629 finished \tANN training loss 0.004537\n",
      ">> Epoch 630 finished \tANN training loss 0.003834\n",
      ">> Epoch 631 finished \tANN training loss 0.003592\n",
      ">> Epoch 632 finished \tANN training loss 0.003788\n",
      ">> Epoch 633 finished \tANN training loss 0.004530\n",
      ">> Epoch 634 finished \tANN training loss 0.003258\n",
      ">> Epoch 635 finished \tANN training loss 0.003942\n",
      ">> Epoch 636 finished \tANN training loss 0.002983\n",
      ">> Epoch 637 finished \tANN training loss 0.003457\n",
      ">> Epoch 638 finished \tANN training loss 0.003316\n",
      ">> Epoch 639 finished \tANN training loss 0.004034\n",
      ">> Epoch 640 finished \tANN training loss 0.004943\n",
      ">> Epoch 641 finished \tANN training loss 0.003608\n",
      ">> Epoch 642 finished \tANN training loss 0.003482\n",
      ">> Epoch 643 finished \tANN training loss 0.005061\n",
      ">> Epoch 644 finished \tANN training loss 0.005243\n",
      ">> Epoch 645 finished \tANN training loss 0.004605\n",
      ">> Epoch 646 finished \tANN training loss 0.007236\n",
      ">> Epoch 647 finished \tANN training loss 0.007459\n",
      ">> Epoch 648 finished \tANN training loss 0.007655\n",
      ">> Epoch 649 finished \tANN training loss 0.004332\n",
      ">> Epoch 650 finished \tANN training loss 0.004136\n",
      ">> Epoch 651 finished \tANN training loss 0.004334\n",
      ">> Epoch 652 finished \tANN training loss 0.006299\n",
      ">> Epoch 653 finished \tANN training loss 0.005153\n",
      ">> Epoch 654 finished \tANN training loss 0.003983\n",
      ">> Epoch 655 finished \tANN training loss 0.005097\n",
      ">> Epoch 656 finished \tANN training loss 0.004432\n",
      ">> Epoch 657 finished \tANN training loss 0.006349\n",
      ">> Epoch 658 finished \tANN training loss 0.008243\n",
      ">> Epoch 659 finished \tANN training loss 0.005755\n",
      ">> Epoch 660 finished \tANN training loss 0.005705\n",
      ">> Epoch 661 finished \tANN training loss 0.005132\n",
      ">> Epoch 662 finished \tANN training loss 0.005238\n",
      ">> Epoch 663 finished \tANN training loss 0.004256\n",
      ">> Epoch 664 finished \tANN training loss 0.004193\n",
      ">> Epoch 665 finished \tANN training loss 0.005119\n",
      ">> Epoch 666 finished \tANN training loss 0.004428\n",
      ">> Epoch 667 finished \tANN training loss 0.003838\n",
      ">> Epoch 668 finished \tANN training loss 0.003646\n",
      ">> Epoch 669 finished \tANN training loss 0.004410\n",
      ">> Epoch 670 finished \tANN training loss 0.004423\n",
      ">> Epoch 671 finished \tANN training loss 0.005677\n",
      ">> Epoch 672 finished \tANN training loss 0.006904\n",
      ">> Epoch 673 finished \tANN training loss 0.004314\n",
      ">> Epoch 674 finished \tANN training loss 0.004985\n",
      ">> Epoch 675 finished \tANN training loss 0.004498\n",
      ">> Epoch 676 finished \tANN training loss 0.005671\n",
      ">> Epoch 677 finished \tANN training loss 0.005026\n",
      ">> Epoch 678 finished \tANN training loss 0.004678\n",
      ">> Epoch 679 finished \tANN training loss 0.005128\n",
      ">> Epoch 680 finished \tANN training loss 0.004554\n",
      ">> Epoch 681 finished \tANN training loss 0.004635\n",
      ">> Epoch 682 finished \tANN training loss 0.004192\n",
      ">> Epoch 683 finished \tANN training loss 0.005311\n",
      ">> Epoch 684 finished \tANN training loss 0.005581\n",
      ">> Epoch 685 finished \tANN training loss 0.004790\n",
      ">> Epoch 686 finished \tANN training loss 0.005106\n",
      ">> Epoch 687 finished \tANN training loss 0.004818\n",
      ">> Epoch 688 finished \tANN training loss 0.003710\n",
      ">> Epoch 689 finished \tANN training loss 0.004235\n",
      ">> Epoch 690 finished \tANN training loss 0.003117\n",
      ">> Epoch 691 finished \tANN training loss 0.003882\n",
      ">> Epoch 692 finished \tANN training loss 0.003759\n",
      ">> Epoch 693 finished \tANN training loss 0.004326\n",
      ">> Epoch 694 finished \tANN training loss 0.003383\n",
      ">> Epoch 695 finished \tANN training loss 0.003709\n",
      ">> Epoch 696 finished \tANN training loss 0.003805\n",
      ">> Epoch 697 finished \tANN training loss 0.004488\n",
      ">> Epoch 698 finished \tANN training loss 0.004036\n",
      ">> Epoch 699 finished \tANN training loss 0.005823\n",
      ">> Epoch 700 finished \tANN training loss 0.003230\n",
      ">> Epoch 701 finished \tANN training loss 0.003166\n",
      ">> Epoch 702 finished \tANN training loss 0.004489\n",
      ">> Epoch 703 finished \tANN training loss 0.003502\n",
      ">> Epoch 704 finished \tANN training loss 0.003786\n",
      ">> Epoch 705 finished \tANN training loss 0.003239\n",
      ">> Epoch 706 finished \tANN training loss 0.002697\n",
      ">> Epoch 707 finished \tANN training loss 0.002951\n",
      ">> Epoch 708 finished \tANN training loss 0.003919\n",
      ">> Epoch 709 finished \tANN training loss 0.004203\n",
      ">> Epoch 710 finished \tANN training loss 0.003501\n",
      ">> Epoch 711 finished \tANN training loss 0.004646\n",
      ">> Epoch 712 finished \tANN training loss 0.004252\n",
      ">> Epoch 713 finished \tANN training loss 0.004562\n",
      ">> Epoch 714 finished \tANN training loss 0.004138\n",
      ">> Epoch 715 finished \tANN training loss 0.004137\n",
      ">> Epoch 716 finished \tANN training loss 0.003728\n",
      ">> Epoch 717 finished \tANN training loss 0.003291\n",
      ">> Epoch 718 finished \tANN training loss 0.003786\n",
      ">> Epoch 719 finished \tANN training loss 0.002427\n",
      ">> Epoch 720 finished \tANN training loss 0.002935\n",
      ">> Epoch 721 finished \tANN training loss 0.003992\n",
      ">> Epoch 722 finished \tANN training loss 0.003379\n",
      ">> Epoch 723 finished \tANN training loss 0.004471\n",
      ">> Epoch 724 finished \tANN training loss 0.007968\n",
      ">> Epoch 725 finished \tANN training loss 0.006918\n",
      ">> Epoch 726 finished \tANN training loss 0.004805\n",
      ">> Epoch 727 finished \tANN training loss 0.005841\n",
      ">> Epoch 728 finished \tANN training loss 0.005298\n",
      ">> Epoch 729 finished \tANN training loss 0.005091\n",
      ">> Epoch 730 finished \tANN training loss 0.006581\n",
      ">> Epoch 731 finished \tANN training loss 0.004939\n",
      ">> Epoch 732 finished \tANN training loss 0.007879\n",
      ">> Epoch 733 finished \tANN training loss 0.005337\n",
      ">> Epoch 734 finished \tANN training loss 0.008091\n",
      ">> Epoch 735 finished \tANN training loss 0.007288\n",
      ">> Epoch 736 finished \tANN training loss 0.008472\n",
      ">> Epoch 737 finished \tANN training loss 0.005639\n",
      ">> Epoch 738 finished \tANN training loss 0.004567\n",
      ">> Epoch 739 finished \tANN training loss 0.003813\n",
      ">> Epoch 740 finished \tANN training loss 0.005280\n",
      ">> Epoch 741 finished \tANN training loss 0.005400\n",
      ">> Epoch 742 finished \tANN training loss 0.003506\n",
      ">> Epoch 743 finished \tANN training loss 0.003912\n",
      ">> Epoch 744 finished \tANN training loss 0.004910\n",
      ">> Epoch 745 finished \tANN training loss 0.006092\n",
      ">> Epoch 746 finished \tANN training loss 0.004474\n",
      ">> Epoch 747 finished \tANN training loss 0.004016\n",
      ">> Epoch 748 finished \tANN training loss 0.004041\n",
      ">> Epoch 749 finished \tANN training loss 0.004863\n",
      ">> Epoch 750 finished \tANN training loss 0.005135\n",
      ">> Epoch 751 finished \tANN training loss 0.004135\n",
      ">> Epoch 752 finished \tANN training loss 0.004685\n",
      ">> Epoch 753 finished \tANN training loss 0.005508\n",
      ">> Epoch 754 finished \tANN training loss 0.003837\n",
      ">> Epoch 755 finished \tANN training loss 0.006736\n",
      ">> Epoch 756 finished \tANN training loss 0.003955\n",
      ">> Epoch 757 finished \tANN training loss 0.003361\n",
      ">> Epoch 758 finished \tANN training loss 0.003150\n",
      ">> Epoch 759 finished \tANN training loss 0.004375\n",
      ">> Epoch 760 finished \tANN training loss 0.003604\n",
      ">> Epoch 761 finished \tANN training loss 0.004213\n",
      ">> Epoch 762 finished \tANN training loss 0.003671\n",
      ">> Epoch 763 finished \tANN training loss 0.003247\n",
      ">> Epoch 764 finished \tANN training loss 0.003297\n",
      ">> Epoch 765 finished \tANN training loss 0.003584\n",
      ">> Epoch 766 finished \tANN training loss 0.002435\n",
      ">> Epoch 767 finished \tANN training loss 0.002554\n",
      ">> Epoch 768 finished \tANN training loss 0.003768\n",
      ">> Epoch 769 finished \tANN training loss 0.002835\n",
      ">> Epoch 770 finished \tANN training loss 0.003633\n",
      ">> Epoch 771 finished \tANN training loss 0.003080\n",
      ">> Epoch 772 finished \tANN training loss 0.002422\n",
      ">> Epoch 773 finished \tANN training loss 0.002357\n",
      ">> Epoch 774 finished \tANN training loss 0.002088\n",
      ">> Epoch 775 finished \tANN training loss 0.002297\n",
      ">> Epoch 776 finished \tANN training loss 0.002276\n",
      ">> Epoch 777 finished \tANN training loss 0.002787\n",
      ">> Epoch 778 finished \tANN training loss 0.002910\n",
      ">> Epoch 779 finished \tANN training loss 0.002682\n",
      ">> Epoch 780 finished \tANN training loss 0.002475\n",
      ">> Epoch 781 finished \tANN training loss 0.002898\n",
      ">> Epoch 782 finished \tANN training loss 0.002716\n",
      ">> Epoch 783 finished \tANN training loss 0.003001\n",
      ">> Epoch 784 finished \tANN training loss 0.003697\n",
      ">> Epoch 785 finished \tANN training loss 0.003406\n",
      ">> Epoch 786 finished \tANN training loss 0.003091\n",
      ">> Epoch 787 finished \tANN training loss 0.003561\n",
      ">> Epoch 788 finished \tANN training loss 0.003586\n",
      ">> Epoch 789 finished \tANN training loss 0.003264\n",
      ">> Epoch 790 finished \tANN training loss 0.005865\n",
      ">> Epoch 791 finished \tANN training loss 0.005301\n",
      ">> Epoch 792 finished \tANN training loss 0.004240\n",
      ">> Epoch 793 finished \tANN training loss 0.005530\n",
      ">> Epoch 794 finished \tANN training loss 0.004064\n",
      ">> Epoch 795 finished \tANN training loss 0.004448\n",
      ">> Epoch 796 finished \tANN training loss 0.003578\n",
      ">> Epoch 797 finished \tANN training loss 0.004141\n",
      ">> Epoch 798 finished \tANN training loss 0.003442\n",
      ">> Epoch 799 finished \tANN training loss 0.003792\n",
      ">> Epoch 800 finished \tANN training loss 0.003987\n",
      ">> Epoch 801 finished \tANN training loss 0.004486\n",
      ">> Epoch 802 finished \tANN training loss 0.003323\n",
      ">> Epoch 803 finished \tANN training loss 0.003198\n",
      ">> Epoch 804 finished \tANN training loss 0.002965\n",
      ">> Epoch 805 finished \tANN training loss 0.003052\n",
      ">> Epoch 806 finished \tANN training loss 0.004822\n",
      ">> Epoch 807 finished \tANN training loss 0.003852\n",
      ">> Epoch 808 finished \tANN training loss 0.003894\n",
      ">> Epoch 809 finished \tANN training loss 0.003912\n",
      ">> Epoch 810 finished \tANN training loss 0.003309\n",
      ">> Epoch 811 finished \tANN training loss 0.003320\n",
      ">> Epoch 812 finished \tANN training loss 0.006799\n",
      ">> Epoch 813 finished \tANN training loss 0.003328\n",
      ">> Epoch 814 finished \tANN training loss 0.002734\n",
      ">> Epoch 815 finished \tANN training loss 0.003659\n",
      ">> Epoch 816 finished \tANN training loss 0.003301\n",
      ">> Epoch 817 finished \tANN training loss 0.003008\n",
      ">> Epoch 818 finished \tANN training loss 0.002811\n",
      ">> Epoch 819 finished \tANN training loss 0.003388\n",
      ">> Epoch 820 finished \tANN training loss 0.002634\n",
      ">> Epoch 821 finished \tANN training loss 0.003374\n",
      ">> Epoch 822 finished \tANN training loss 0.004588\n",
      ">> Epoch 823 finished \tANN training loss 0.003604\n",
      ">> Epoch 824 finished \tANN training loss 0.004456\n",
      ">> Epoch 825 finished \tANN training loss 0.004584\n",
      ">> Epoch 826 finished \tANN training loss 0.004500\n",
      ">> Epoch 827 finished \tANN training loss 0.003224\n",
      ">> Epoch 828 finished \tANN training loss 0.003617\n",
      ">> Epoch 829 finished \tANN training loss 0.003696\n",
      ">> Epoch 830 finished \tANN training loss 0.002798\n",
      ">> Epoch 831 finished \tANN training loss 0.002990\n",
      ">> Epoch 832 finished \tANN training loss 0.002772\n",
      ">> Epoch 833 finished \tANN training loss 0.003303\n",
      ">> Epoch 834 finished \tANN training loss 0.003104\n",
      ">> Epoch 835 finished \tANN training loss 0.002917\n",
      ">> Epoch 836 finished \tANN training loss 0.002939\n",
      ">> Epoch 837 finished \tANN training loss 0.004839\n",
      ">> Epoch 838 finished \tANN training loss 0.003316\n",
      ">> Epoch 839 finished \tANN training loss 0.003857\n",
      ">> Epoch 840 finished \tANN training loss 0.004178\n",
      ">> Epoch 841 finished \tANN training loss 0.002973\n",
      ">> Epoch 842 finished \tANN training loss 0.002567\n",
      ">> Epoch 843 finished \tANN training loss 0.002619\n",
      ">> Epoch 844 finished \tANN training loss 0.002851\n",
      ">> Epoch 845 finished \tANN training loss 0.002358\n",
      ">> Epoch 846 finished \tANN training loss 0.003098\n",
      ">> Epoch 847 finished \tANN training loss 0.003558\n",
      ">> Epoch 848 finished \tANN training loss 0.003636\n",
      ">> Epoch 849 finished \tANN training loss 0.003315\n",
      ">> Epoch 850 finished \tANN training loss 0.002880\n",
      ">> Epoch 851 finished \tANN training loss 0.002618\n",
      ">> Epoch 852 finished \tANN training loss 0.004211\n",
      ">> Epoch 853 finished \tANN training loss 0.003332\n",
      ">> Epoch 854 finished \tANN training loss 0.006371\n",
      ">> Epoch 855 finished \tANN training loss 0.003051\n",
      ">> Epoch 856 finished \tANN training loss 0.002939\n",
      ">> Epoch 857 finished \tANN training loss 0.004138\n",
      ">> Epoch 858 finished \tANN training loss 0.002558\n",
      ">> Epoch 859 finished \tANN training loss 0.003439\n",
      ">> Epoch 860 finished \tANN training loss 0.002496\n",
      ">> Epoch 861 finished \tANN training loss 0.002512\n",
      ">> Epoch 862 finished \tANN training loss 0.002405\n",
      ">> Epoch 863 finished \tANN training loss 0.003554\n",
      ">> Epoch 864 finished \tANN training loss 0.002344\n",
      ">> Epoch 865 finished \tANN training loss 0.002251\n",
      ">> Epoch 866 finished \tANN training loss 0.003112\n",
      ">> Epoch 867 finished \tANN training loss 0.002837\n",
      ">> Epoch 868 finished \tANN training loss 0.002568\n",
      ">> Epoch 869 finished \tANN training loss 0.002454\n",
      ">> Epoch 870 finished \tANN training loss 0.003715\n",
      ">> Epoch 871 finished \tANN training loss 0.003167\n",
      ">> Epoch 872 finished \tANN training loss 0.003471\n",
      ">> Epoch 873 finished \tANN training loss 0.002950\n",
      ">> Epoch 874 finished \tANN training loss 0.002915\n",
      ">> Epoch 875 finished \tANN training loss 0.003578\n",
      ">> Epoch 876 finished \tANN training loss 0.003167\n",
      ">> Epoch 877 finished \tANN training loss 0.003054\n",
      ">> Epoch 878 finished \tANN training loss 0.004214\n",
      ">> Epoch 879 finished \tANN training loss 0.003183\n",
      ">> Epoch 880 finished \tANN training loss 0.003137\n",
      ">> Epoch 881 finished \tANN training loss 0.004291\n",
      ">> Epoch 882 finished \tANN training loss 0.003051\n",
      ">> Epoch 883 finished \tANN training loss 0.002727\n",
      ">> Epoch 884 finished \tANN training loss 0.002752\n",
      ">> Epoch 885 finished \tANN training loss 0.002798\n",
      ">> Epoch 886 finished \tANN training loss 0.002255\n",
      ">> Epoch 887 finished \tANN training loss 0.003677\n",
      ">> Epoch 888 finished \tANN training loss 0.004091\n",
      ">> Epoch 889 finished \tANN training loss 0.002793\n",
      ">> Epoch 890 finished \tANN training loss 0.002932\n",
      ">> Epoch 891 finished \tANN training loss 0.004076\n",
      ">> Epoch 892 finished \tANN training loss 0.005146\n",
      ">> Epoch 893 finished \tANN training loss 0.003257\n",
      ">> Epoch 894 finished \tANN training loss 0.003398\n",
      ">> Epoch 895 finished \tANN training loss 0.003064\n",
      ">> Epoch 896 finished \tANN training loss 0.003801\n",
      ">> Epoch 897 finished \tANN training loss 0.003513\n",
      ">> Epoch 898 finished \tANN training loss 0.003561\n",
      ">> Epoch 899 finished \tANN training loss 0.004780\n",
      ">> Epoch 900 finished \tANN training loss 0.003945\n",
      ">> Epoch 901 finished \tANN training loss 0.003309\n",
      ">> Epoch 902 finished \tANN training loss 0.004160\n",
      ">> Epoch 903 finished \tANN training loss 0.004841\n",
      ">> Epoch 904 finished \tANN training loss 0.003612\n",
      ">> Epoch 905 finished \tANN training loss 0.003860\n",
      ">> Epoch 906 finished \tANN training loss 0.004360\n",
      ">> Epoch 907 finished \tANN training loss 0.003708\n",
      ">> Epoch 908 finished \tANN training loss 0.003287\n",
      ">> Epoch 909 finished \tANN training loss 0.003512\n",
      ">> Epoch 910 finished \tANN training loss 0.002545\n",
      ">> Epoch 911 finished \tANN training loss 0.002523\n",
      ">> Epoch 912 finished \tANN training loss 0.002131\n",
      ">> Epoch 913 finished \tANN training loss 0.002017\n",
      ">> Epoch 914 finished \tANN training loss 0.002289\n",
      ">> Epoch 915 finished \tANN training loss 0.001894\n",
      ">> Epoch 916 finished \tANN training loss 0.001929\n",
      ">> Epoch 917 finished \tANN training loss 0.003047\n",
      ">> Epoch 918 finished \tANN training loss 0.002871\n",
      ">> Epoch 919 finished \tANN training loss 0.002816\n",
      ">> Epoch 920 finished \tANN training loss 0.002524\n",
      ">> Epoch 921 finished \tANN training loss 0.002177\n",
      ">> Epoch 922 finished \tANN training loss 0.003035\n",
      ">> Epoch 923 finished \tANN training loss 0.003294\n",
      ">> Epoch 924 finished \tANN training loss 0.002525\n",
      ">> Epoch 925 finished \tANN training loss 0.002622\n",
      ">> Epoch 926 finished \tANN training loss 0.003407\n",
      ">> Epoch 927 finished \tANN training loss 0.003456\n",
      ">> Epoch 928 finished \tANN training loss 0.003403\n",
      ">> Epoch 929 finished \tANN training loss 0.003236\n",
      ">> Epoch 930 finished \tANN training loss 0.003483\n",
      ">> Epoch 931 finished \tANN training loss 0.002934\n",
      ">> Epoch 932 finished \tANN training loss 0.002492\n",
      ">> Epoch 933 finished \tANN training loss 0.002847\n",
      ">> Epoch 934 finished \tANN training loss 0.003784\n",
      ">> Epoch 935 finished \tANN training loss 0.003043\n",
      ">> Epoch 936 finished \tANN training loss 0.003240\n",
      ">> Epoch 937 finished \tANN training loss 0.003164\n",
      ">> Epoch 938 finished \tANN training loss 0.002857\n",
      ">> Epoch 939 finished \tANN training loss 0.002551\n",
      ">> Epoch 940 finished \tANN training loss 0.002608\n",
      ">> Epoch 941 finished \tANN training loss 0.003067\n",
      ">> Epoch 942 finished \tANN training loss 0.003764\n",
      ">> Epoch 943 finished \tANN training loss 0.003524\n",
      ">> Epoch 944 finished \tANN training loss 0.002887\n",
      ">> Epoch 945 finished \tANN training loss 0.004230\n",
      ">> Epoch 946 finished \tANN training loss 0.003363\n",
      ">> Epoch 947 finished \tANN training loss 0.003157\n",
      ">> Epoch 948 finished \tANN training loss 0.002661\n",
      ">> Epoch 949 finished \tANN training loss 0.003080\n",
      ">> Epoch 950 finished \tANN training loss 0.003388\n",
      ">> Epoch 951 finished \tANN training loss 0.002928\n",
      ">> Epoch 952 finished \tANN training loss 0.002881\n",
      ">> Epoch 953 finished \tANN training loss 0.002301\n",
      ">> Epoch 954 finished \tANN training loss 0.002606\n",
      ">> Epoch 955 finished \tANN training loss 0.002109\n",
      ">> Epoch 956 finished \tANN training loss 0.002034\n",
      ">> Epoch 957 finished \tANN training loss 0.002402\n",
      ">> Epoch 958 finished \tANN training loss 0.006477\n",
      ">> Epoch 959 finished \tANN training loss 0.002590\n",
      ">> Epoch 960 finished \tANN training loss 0.001951\n",
      ">> Epoch 961 finished \tANN training loss 0.002320\n",
      ">> Epoch 962 finished \tANN training loss 0.007594\n",
      ">> Epoch 963 finished \tANN training loss 0.002490\n",
      ">> Epoch 964 finished \tANN training loss 0.001930\n",
      ">> Epoch 965 finished \tANN training loss 0.001861\n",
      ">> Epoch 966 finished \tANN training loss 0.001617\n",
      ">> Epoch 967 finished \tANN training loss 0.001799\n",
      ">> Epoch 968 finished \tANN training loss 0.002190\n",
      ">> Epoch 969 finished \tANN training loss 0.002053\n",
      ">> Epoch 970 finished \tANN training loss 0.002000\n",
      ">> Epoch 971 finished \tANN training loss 0.003102\n",
      ">> Epoch 972 finished \tANN training loss 0.003259\n",
      ">> Epoch 973 finished \tANN training loss 0.003451\n",
      ">> Epoch 974 finished \tANN training loss 0.002655\n",
      ">> Epoch 975 finished \tANN training loss 0.002858\n",
      ">> Epoch 976 finished \tANN training loss 0.002765\n",
      ">> Epoch 977 finished \tANN training loss 0.002665\n",
      ">> Epoch 978 finished \tANN training loss 0.002694\n",
      ">> Epoch 979 finished \tANN training loss 0.002333\n",
      ">> Epoch 980 finished \tANN training loss 0.002117\n",
      ">> Epoch 981 finished \tANN training loss 0.003009\n",
      ">> Epoch 982 finished \tANN training loss 0.002598\n",
      ">> Epoch 983 finished \tANN training loss 0.002109\n",
      ">> Epoch 984 finished \tANN training loss 0.002603\n",
      ">> Epoch 985 finished \tANN training loss 0.002253\n",
      ">> Epoch 986 finished \tANN training loss 0.002140\n",
      ">> Epoch 987 finished \tANN training loss 0.001959\n",
      ">> Epoch 988 finished \tANN training loss 0.001504\n",
      ">> Epoch 989 finished \tANN training loss 0.002318\n",
      ">> Epoch 990 finished \tANN training loss 0.002213\n",
      ">> Epoch 991 finished \tANN training loss 0.002796\n",
      ">> Epoch 992 finished \tANN training loss 0.002679\n",
      ">> Epoch 993 finished \tANN training loss 0.002730\n",
      ">> Epoch 994 finished \tANN training loss 0.002884\n",
      ">> Epoch 995 finished \tANN training loss 0.003107\n",
      ">> Epoch 996 finished \tANN training loss 0.003152\n",
      ">> Epoch 997 finished \tANN training loss 0.002815\n",
      ">> Epoch 998 finished \tANN training loss 0.003262\n",
      ">> Epoch 999 finished \tANN training loss 0.002899\n",
      "[END] Fine tuning step\n",
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 8.032158\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 7.919257\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 7.725555\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 7.162060\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 6.561814\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 5.814474\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 5.115719\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 4.652435\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 4.184295\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 3.890083\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 3.630346\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 3.550033\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 3.257806\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 3.105801\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 2.935721\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 2.869017\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 2.716570\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 2.575104\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 2.549196\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 2.436510\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 5.554699\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 5.317970\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 4.692127\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 3.898976\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 3.143289\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 2.528712\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 2.131707\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 1.824430\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 1.535912\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 1.293075\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 1.124125\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 1.024832\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 0.907524\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 0.836972\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 0.785866\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 0.727825\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 0.712680\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 0.711299\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 0.690662\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 0.681102\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.828246\n",
      ">> Epoch 1 finished \tANN training loss 0.720130\n",
      ">> Epoch 2 finished \tANN training loss 0.631742\n",
      ">> Epoch 3 finished \tANN training loss 0.591410\n",
      ">> Epoch 4 finished \tANN training loss 0.542075\n",
      ">> Epoch 5 finished \tANN training loss 0.517759\n",
      ">> Epoch 6 finished \tANN training loss 0.459873\n",
      ">> Epoch 7 finished \tANN training loss 0.473980\n",
      ">> Epoch 8 finished \tANN training loss 0.407511\n",
      ">> Epoch 9 finished \tANN training loss 0.399965\n",
      ">> Epoch 10 finished \tANN training loss 0.378344\n",
      ">> Epoch 11 finished \tANN training loss 0.355817\n",
      ">> Epoch 12 finished \tANN training loss 0.334404\n",
      ">> Epoch 13 finished \tANN training loss 0.335108\n",
      ">> Epoch 14 finished \tANN training loss 0.306786\n",
      ">> Epoch 15 finished \tANN training loss 0.295058\n",
      ">> Epoch 16 finished \tANN training loss 0.281724\n",
      ">> Epoch 17 finished \tANN training loss 0.268552\n",
      ">> Epoch 18 finished \tANN training loss 0.290063\n",
      ">> Epoch 19 finished \tANN training loss 0.255580\n",
      ">> Epoch 20 finished \tANN training loss 0.258736\n",
      ">> Epoch 21 finished \tANN training loss 0.263068\n",
      ">> Epoch 22 finished \tANN training loss 0.256145\n",
      ">> Epoch 23 finished \tANN training loss 0.230665\n",
      ">> Epoch 24 finished \tANN training loss 0.233399\n",
      ">> Epoch 25 finished \tANN training loss 0.225217\n",
      ">> Epoch 26 finished \tANN training loss 0.240175\n",
      ">> Epoch 27 finished \tANN training loss 0.209329\n",
      ">> Epoch 28 finished \tANN training loss 0.219116\n",
      ">> Epoch 29 finished \tANN training loss 0.226580\n",
      ">> Epoch 30 finished \tANN training loss 0.206120\n",
      ">> Epoch 31 finished \tANN training loss 0.183750\n",
      ">> Epoch 32 finished \tANN training loss 0.198263\n",
      ">> Epoch 33 finished \tANN training loss 0.180067\n",
      ">> Epoch 34 finished \tANN training loss 0.188433\n",
      ">> Epoch 35 finished \tANN training loss 0.180290\n",
      ">> Epoch 36 finished \tANN training loss 0.174229\n",
      ">> Epoch 37 finished \tANN training loss 0.173022\n",
      ">> Epoch 38 finished \tANN training loss 0.170130\n",
      ">> Epoch 39 finished \tANN training loss 0.158163\n",
      ">> Epoch 40 finished \tANN training loss 0.153157\n",
      ">> Epoch 41 finished \tANN training loss 0.150850\n",
      ">> Epoch 42 finished \tANN training loss 0.180523\n",
      ">> Epoch 43 finished \tANN training loss 0.160969\n",
      ">> Epoch 44 finished \tANN training loss 0.141620\n",
      ">> Epoch 45 finished \tANN training loss 0.141472\n",
      ">> Epoch 46 finished \tANN training loss 0.139408\n",
      ">> Epoch 47 finished \tANN training loss 0.127956\n",
      ">> Epoch 48 finished \tANN training loss 0.135888\n",
      ">> Epoch 49 finished \tANN training loss 0.126819\n",
      ">> Epoch 50 finished \tANN training loss 0.132370\n",
      ">> Epoch 51 finished \tANN training loss 0.128299\n",
      ">> Epoch 52 finished \tANN training loss 0.135397\n",
      ">> Epoch 53 finished \tANN training loss 0.114381\n",
      ">> Epoch 54 finished \tANN training loss 0.117871\n",
      ">> Epoch 55 finished \tANN training loss 0.127158\n",
      ">> Epoch 56 finished \tANN training loss 0.107682\n",
      ">> Epoch 57 finished \tANN training loss 0.118834\n",
      ">> Epoch 58 finished \tANN training loss 0.112853\n",
      ">> Epoch 59 finished \tANN training loss 0.100594\n",
      ">> Epoch 60 finished \tANN training loss 0.109014\n",
      ">> Epoch 61 finished \tANN training loss 0.097332\n",
      ">> Epoch 62 finished \tANN training loss 0.096005\n",
      ">> Epoch 63 finished \tANN training loss 0.136628\n",
      ">> Epoch 64 finished \tANN training loss 0.097741\n",
      ">> Epoch 65 finished \tANN training loss 0.088798\n",
      ">> Epoch 66 finished \tANN training loss 0.088307\n",
      ">> Epoch 67 finished \tANN training loss 0.093314\n",
      ">> Epoch 68 finished \tANN training loss 0.100981\n",
      ">> Epoch 69 finished \tANN training loss 0.080742\n",
      ">> Epoch 70 finished \tANN training loss 0.086804\n",
      ">> Epoch 71 finished \tANN training loss 0.087328\n",
      ">> Epoch 72 finished \tANN training loss 0.078744\n",
      ">> Epoch 73 finished \tANN training loss 0.116319\n",
      ">> Epoch 74 finished \tANN training loss 0.082101\n",
      ">> Epoch 75 finished \tANN training loss 0.084792\n",
      ">> Epoch 76 finished \tANN training loss 0.094914\n",
      ">> Epoch 77 finished \tANN training loss 0.075054\n",
      ">> Epoch 78 finished \tANN training loss 0.076910\n",
      ">> Epoch 79 finished \tANN training loss 0.075138\n",
      ">> Epoch 80 finished \tANN training loss 0.070729\n",
      ">> Epoch 81 finished \tANN training loss 0.076241\n",
      ">> Epoch 82 finished \tANN training loss 0.076799\n",
      ">> Epoch 83 finished \tANN training loss 0.085142\n",
      ">> Epoch 84 finished \tANN training loss 0.075340\n",
      ">> Epoch 85 finished \tANN training loss 0.070940\n",
      ">> Epoch 86 finished \tANN training loss 0.069491\n",
      ">> Epoch 87 finished \tANN training loss 0.065478\n",
      ">> Epoch 88 finished \tANN training loss 0.069115\n",
      ">> Epoch 89 finished \tANN training loss 0.066137\n",
      ">> Epoch 90 finished \tANN training loss 0.057988\n",
      ">> Epoch 91 finished \tANN training loss 0.067532\n",
      ">> Epoch 92 finished \tANN training loss 0.055140\n",
      ">> Epoch 93 finished \tANN training loss 0.064731\n",
      ">> Epoch 94 finished \tANN training loss 0.050376\n",
      ">> Epoch 95 finished \tANN training loss 0.054678\n",
      ">> Epoch 96 finished \tANN training loss 0.055684\n",
      ">> Epoch 97 finished \tANN training loss 0.052944\n",
      ">> Epoch 98 finished \tANN training loss 0.055219\n",
      ">> Epoch 99 finished \tANN training loss 0.059195\n",
      ">> Epoch 100 finished \tANN training loss 0.066682\n",
      ">> Epoch 101 finished \tANN training loss 0.052492\n",
      ">> Epoch 102 finished \tANN training loss 0.053129\n",
      ">> Epoch 103 finished \tANN training loss 0.056469\n",
      ">> Epoch 104 finished \tANN training loss 0.048413\n",
      ">> Epoch 105 finished \tANN training loss 0.045108\n",
      ">> Epoch 106 finished \tANN training loss 0.053398\n",
      ">> Epoch 107 finished \tANN training loss 0.048883\n",
      ">> Epoch 108 finished \tANN training loss 0.043138\n",
      ">> Epoch 109 finished \tANN training loss 0.038930\n",
      ">> Epoch 110 finished \tANN training loss 0.043909\n",
      ">> Epoch 111 finished \tANN training loss 0.044489\n",
      ">> Epoch 112 finished \tANN training loss 0.054932\n",
      ">> Epoch 113 finished \tANN training loss 0.042014\n",
      ">> Epoch 114 finished \tANN training loss 0.060454\n",
      ">> Epoch 115 finished \tANN training loss 0.050377\n",
      ">> Epoch 116 finished \tANN training loss 0.047833\n",
      ">> Epoch 117 finished \tANN training loss 0.043078\n",
      ">> Epoch 118 finished \tANN training loss 0.043836\n",
      ">> Epoch 119 finished \tANN training loss 0.048480\n",
      ">> Epoch 120 finished \tANN training loss 0.043593\n",
      ">> Epoch 121 finished \tANN training loss 0.059519\n",
      ">> Epoch 122 finished \tANN training loss 0.045848\n",
      ">> Epoch 123 finished \tANN training loss 0.034135\n",
      ">> Epoch 124 finished \tANN training loss 0.035256\n",
      ">> Epoch 125 finished \tANN training loss 0.040793\n",
      ">> Epoch 126 finished \tANN training loss 0.036638\n",
      ">> Epoch 127 finished \tANN training loss 0.033414\n",
      ">> Epoch 128 finished \tANN training loss 0.040637\n",
      ">> Epoch 129 finished \tANN training loss 0.044500\n",
      ">> Epoch 130 finished \tANN training loss 0.040839\n",
      ">> Epoch 131 finished \tANN training loss 0.037419\n",
      ">> Epoch 132 finished \tANN training loss 0.037533\n",
      ">> Epoch 133 finished \tANN training loss 0.034345\n",
      ">> Epoch 134 finished \tANN training loss 0.066977\n",
      ">> Epoch 135 finished \tANN training loss 0.038806\n",
      ">> Epoch 136 finished \tANN training loss 0.034684\n",
      ">> Epoch 137 finished \tANN training loss 0.049074\n",
      ">> Epoch 138 finished \tANN training loss 0.040348\n",
      ">> Epoch 139 finished \tANN training loss 0.046995\n",
      ">> Epoch 140 finished \tANN training loss 0.033576\n",
      ">> Epoch 141 finished \tANN training loss 0.029813\n",
      ">> Epoch 142 finished \tANN training loss 0.043882\n",
      ">> Epoch 143 finished \tANN training loss 0.033052\n",
      ">> Epoch 144 finished \tANN training loss 0.030757\n",
      ">> Epoch 145 finished \tANN training loss 0.033930\n",
      ">> Epoch 146 finished \tANN training loss 0.036357\n",
      ">> Epoch 147 finished \tANN training loss 0.041005\n",
      ">> Epoch 148 finished \tANN training loss 0.027291\n",
      ">> Epoch 149 finished \tANN training loss 0.030577\n",
      ">> Epoch 150 finished \tANN training loss 0.031667\n",
      ">> Epoch 151 finished \tANN training loss 0.026255\n",
      ">> Epoch 152 finished \tANN training loss 0.025966\n",
      ">> Epoch 153 finished \tANN training loss 0.026469\n",
      ">> Epoch 154 finished \tANN training loss 0.021972\n",
      ">> Epoch 155 finished \tANN training loss 0.032382\n",
      ">> Epoch 156 finished \tANN training loss 0.022214\n",
      ">> Epoch 157 finished \tANN training loss 0.026110\n",
      ">> Epoch 158 finished \tANN training loss 0.026549\n",
      ">> Epoch 159 finished \tANN training loss 0.030350\n",
      ">> Epoch 160 finished \tANN training loss 0.029292\n",
      ">> Epoch 161 finished \tANN training loss 0.026057\n",
      ">> Epoch 162 finished \tANN training loss 0.025302\n",
      ">> Epoch 163 finished \tANN training loss 0.038679\n",
      ">> Epoch 164 finished \tANN training loss 0.028320\n",
      ">> Epoch 165 finished \tANN training loss 0.034860\n",
      ">> Epoch 166 finished \tANN training loss 0.022484\n",
      ">> Epoch 167 finished \tANN training loss 0.033853\n",
      ">> Epoch 168 finished \tANN training loss 0.028061\n",
      ">> Epoch 169 finished \tANN training loss 0.031392\n",
      ">> Epoch 170 finished \tANN training loss 0.027582\n",
      ">> Epoch 171 finished \tANN training loss 0.036273\n",
      ">> Epoch 172 finished \tANN training loss 0.026031\n",
      ">> Epoch 173 finished \tANN training loss 0.024747\n",
      ">> Epoch 174 finished \tANN training loss 0.025137\n",
      ">> Epoch 175 finished \tANN training loss 0.023785\n",
      ">> Epoch 176 finished \tANN training loss 0.028672\n",
      ">> Epoch 177 finished \tANN training loss 0.022094\n",
      ">> Epoch 178 finished \tANN training loss 0.024412\n",
      ">> Epoch 179 finished \tANN training loss 0.033989\n",
      ">> Epoch 180 finished \tANN training loss 0.025118\n",
      ">> Epoch 181 finished \tANN training loss 0.025487\n",
      ">> Epoch 182 finished \tANN training loss 0.020912\n",
      ">> Epoch 183 finished \tANN training loss 0.026335\n",
      ">> Epoch 184 finished \tANN training loss 0.026665\n",
      ">> Epoch 185 finished \tANN training loss 0.031464\n",
      ">> Epoch 186 finished \tANN training loss 0.022309\n",
      ">> Epoch 187 finished \tANN training loss 0.026341\n",
      ">> Epoch 188 finished \tANN training loss 0.025685\n",
      ">> Epoch 189 finished \tANN training loss 0.023039\n",
      ">> Epoch 190 finished \tANN training loss 0.029200\n",
      ">> Epoch 191 finished \tANN training loss 0.020781\n",
      ">> Epoch 192 finished \tANN training loss 0.037801\n",
      ">> Epoch 193 finished \tANN training loss 0.024787\n",
      ">> Epoch 194 finished \tANN training loss 0.022914\n",
      ">> Epoch 195 finished \tANN training loss 0.022909\n",
      ">> Epoch 196 finished \tANN training loss 0.022004\n",
      ">> Epoch 197 finished \tANN training loss 0.020349\n",
      ">> Epoch 198 finished \tANN training loss 0.027771\n",
      ">> Epoch 199 finished \tANN training loss 0.023811\n",
      ">> Epoch 200 finished \tANN training loss 0.024894\n",
      ">> Epoch 201 finished \tANN training loss 0.031197\n",
      ">> Epoch 202 finished \tANN training loss 0.021837\n",
      ">> Epoch 203 finished \tANN training loss 0.023562\n",
      ">> Epoch 204 finished \tANN training loss 0.021712\n",
      ">> Epoch 205 finished \tANN training loss 0.019581\n",
      ">> Epoch 206 finished \tANN training loss 0.023363\n",
      ">> Epoch 207 finished \tANN training loss 0.028166\n",
      ">> Epoch 208 finished \tANN training loss 0.023193\n",
      ">> Epoch 209 finished \tANN training loss 0.025965\n",
      ">> Epoch 210 finished \tANN training loss 0.015110\n",
      ">> Epoch 211 finished \tANN training loss 0.022791\n",
      ">> Epoch 212 finished \tANN training loss 0.030253\n",
      ">> Epoch 213 finished \tANN training loss 0.025969\n",
      ">> Epoch 214 finished \tANN training loss 0.018399\n",
      ">> Epoch 215 finished \tANN training loss 0.023181\n",
      ">> Epoch 216 finished \tANN training loss 0.024228\n",
      ">> Epoch 217 finished \tANN training loss 0.038998\n",
      ">> Epoch 218 finished \tANN training loss 0.025376\n",
      ">> Epoch 219 finished \tANN training loss 0.026519\n",
      ">> Epoch 220 finished \tANN training loss 0.024245\n",
      ">> Epoch 221 finished \tANN training loss 0.019017\n",
      ">> Epoch 222 finished \tANN training loss 0.018494\n",
      ">> Epoch 223 finished \tANN training loss 0.016171\n",
      ">> Epoch 224 finished \tANN training loss 0.019766\n",
      ">> Epoch 225 finished \tANN training loss 0.018169\n",
      ">> Epoch 226 finished \tANN training loss 0.015209\n",
      ">> Epoch 227 finished \tANN training loss 0.011674\n",
      ">> Epoch 228 finished \tANN training loss 0.013554\n",
      ">> Epoch 229 finished \tANN training loss 0.013351\n",
      ">> Epoch 230 finished \tANN training loss 0.017011\n",
      ">> Epoch 231 finished \tANN training loss 0.014041\n",
      ">> Epoch 232 finished \tANN training loss 0.015779\n",
      ">> Epoch 233 finished \tANN training loss 0.015849\n",
      ">> Epoch 234 finished \tANN training loss 0.013073\n",
      ">> Epoch 235 finished \tANN training loss 0.017814\n",
      ">> Epoch 236 finished \tANN training loss 0.015870\n",
      ">> Epoch 237 finished \tANN training loss 0.015062\n",
      ">> Epoch 238 finished \tANN training loss 0.018762\n",
      ">> Epoch 239 finished \tANN training loss 0.016404\n",
      ">> Epoch 240 finished \tANN training loss 0.018131\n",
      ">> Epoch 241 finished \tANN training loss 0.014291\n",
      ">> Epoch 242 finished \tANN training loss 0.013423\n",
      ">> Epoch 243 finished \tANN training loss 0.011250\n",
      ">> Epoch 244 finished \tANN training loss 0.009610\n",
      ">> Epoch 245 finished \tANN training loss 0.009920\n",
      ">> Epoch 246 finished \tANN training loss 0.013978\n",
      ">> Epoch 247 finished \tANN training loss 0.022861\n",
      ">> Epoch 248 finished \tANN training loss 0.022359\n",
      ">> Epoch 249 finished \tANN training loss 0.013280\n",
      ">> Epoch 250 finished \tANN training loss 0.016138\n",
      ">> Epoch 251 finished \tANN training loss 0.024340\n",
      ">> Epoch 252 finished \tANN training loss 0.016264\n",
      ">> Epoch 253 finished \tANN training loss 0.015658\n",
      ">> Epoch 254 finished \tANN training loss 0.012445\n",
      ">> Epoch 255 finished \tANN training loss 0.015050\n",
      ">> Epoch 256 finished \tANN training loss 0.014518\n",
      ">> Epoch 257 finished \tANN training loss 0.011063\n",
      ">> Epoch 258 finished \tANN training loss 0.010766\n",
      ">> Epoch 259 finished \tANN training loss 0.012443\n",
      ">> Epoch 260 finished \tANN training loss 0.012655\n",
      ">> Epoch 261 finished \tANN training loss 0.014844\n",
      ">> Epoch 262 finished \tANN training loss 0.014101\n",
      ">> Epoch 263 finished \tANN training loss 0.017651\n",
      ">> Epoch 264 finished \tANN training loss 0.010073\n",
      ">> Epoch 265 finished \tANN training loss 0.011924\n",
      ">> Epoch 266 finished \tANN training loss 0.015047\n",
      ">> Epoch 267 finished \tANN training loss 0.012950\n",
      ">> Epoch 268 finished \tANN training loss 0.016052\n",
      ">> Epoch 269 finished \tANN training loss 0.018120\n",
      ">> Epoch 270 finished \tANN training loss 0.016287\n",
      ">> Epoch 271 finished \tANN training loss 0.013567\n",
      ">> Epoch 272 finished \tANN training loss 0.013295\n",
      ">> Epoch 273 finished \tANN training loss 0.009755\n",
      ">> Epoch 274 finished \tANN training loss 0.013154\n",
      ">> Epoch 275 finished \tANN training loss 0.014647\n",
      ">> Epoch 276 finished \tANN training loss 0.015657\n",
      ">> Epoch 277 finished \tANN training loss 0.015229\n",
      ">> Epoch 278 finished \tANN training loss 0.014640\n",
      ">> Epoch 279 finished \tANN training loss 0.015462\n",
      ">> Epoch 280 finished \tANN training loss 0.014349\n",
      ">> Epoch 281 finished \tANN training loss 0.015609\n",
      ">> Epoch 282 finished \tANN training loss 0.012867\n",
      ">> Epoch 283 finished \tANN training loss 0.010328\n",
      ">> Epoch 284 finished \tANN training loss 0.015831\n",
      ">> Epoch 285 finished \tANN training loss 0.014069\n",
      ">> Epoch 286 finished \tANN training loss 0.012105\n",
      ">> Epoch 287 finished \tANN training loss 0.011703\n",
      ">> Epoch 288 finished \tANN training loss 0.014692\n",
      ">> Epoch 289 finished \tANN training loss 0.012989\n",
      ">> Epoch 290 finished \tANN training loss 0.016189\n",
      ">> Epoch 291 finished \tANN training loss 0.010168\n",
      ">> Epoch 292 finished \tANN training loss 0.008939\n",
      ">> Epoch 293 finished \tANN training loss 0.010160\n",
      ">> Epoch 294 finished \tANN training loss 0.010305\n",
      ">> Epoch 295 finished \tANN training loss 0.012677\n",
      ">> Epoch 296 finished \tANN training loss 0.018882\n",
      ">> Epoch 297 finished \tANN training loss 0.019182\n",
      ">> Epoch 298 finished \tANN training loss 0.025265\n",
      ">> Epoch 299 finished \tANN training loss 0.010373\n",
      ">> Epoch 300 finished \tANN training loss 0.011691\n",
      ">> Epoch 301 finished \tANN training loss 0.014614\n",
      ">> Epoch 302 finished \tANN training loss 0.011810\n",
      ">> Epoch 303 finished \tANN training loss 0.021983\n",
      ">> Epoch 304 finished \tANN training loss 0.012583\n",
      ">> Epoch 305 finished \tANN training loss 0.012409\n",
      ">> Epoch 306 finished \tANN training loss 0.012080\n",
      ">> Epoch 307 finished \tANN training loss 0.011148\n",
      ">> Epoch 308 finished \tANN training loss 0.010479\n",
      ">> Epoch 309 finished \tANN training loss 0.008488\n",
      ">> Epoch 310 finished \tANN training loss 0.012503\n",
      ">> Epoch 311 finished \tANN training loss 0.013336\n",
      ">> Epoch 312 finished \tANN training loss 0.013641\n",
      ">> Epoch 313 finished \tANN training loss 0.012582\n",
      ">> Epoch 314 finished \tANN training loss 0.012572\n",
      ">> Epoch 315 finished \tANN training loss 0.011906\n",
      ">> Epoch 316 finished \tANN training loss 0.015700\n",
      ">> Epoch 317 finished \tANN training loss 0.014208\n",
      ">> Epoch 318 finished \tANN training loss 0.012466\n",
      ">> Epoch 319 finished \tANN training loss 0.012305\n",
      ">> Epoch 320 finished \tANN training loss 0.011866\n",
      ">> Epoch 321 finished \tANN training loss 0.012456\n",
      ">> Epoch 322 finished \tANN training loss 0.010191\n",
      ">> Epoch 323 finished \tANN training loss 0.010372\n",
      ">> Epoch 324 finished \tANN training loss 0.011727\n",
      ">> Epoch 325 finished \tANN training loss 0.012140\n",
      ">> Epoch 326 finished \tANN training loss 0.009899\n",
      ">> Epoch 327 finished \tANN training loss 0.009446\n",
      ">> Epoch 328 finished \tANN training loss 0.014063\n",
      ">> Epoch 329 finished \tANN training loss 0.012341\n",
      ">> Epoch 330 finished \tANN training loss 0.010450\n",
      ">> Epoch 331 finished \tANN training loss 0.008852\n",
      ">> Epoch 332 finished \tANN training loss 0.012366\n",
      ">> Epoch 333 finished \tANN training loss 0.013663\n",
      ">> Epoch 334 finished \tANN training loss 0.009707\n",
      ">> Epoch 335 finished \tANN training loss 0.010401\n",
      ">> Epoch 336 finished \tANN training loss 0.015066\n",
      ">> Epoch 337 finished \tANN training loss 0.017136\n",
      ">> Epoch 338 finished \tANN training loss 0.016532\n",
      ">> Epoch 339 finished \tANN training loss 0.016718\n",
      ">> Epoch 340 finished \tANN training loss 0.014271\n",
      ">> Epoch 341 finished \tANN training loss 0.012277\n",
      ">> Epoch 342 finished \tANN training loss 0.010998\n",
      ">> Epoch 343 finished \tANN training loss 0.013331\n",
      ">> Epoch 344 finished \tANN training loss 0.013191\n",
      ">> Epoch 345 finished \tANN training loss 0.020164\n",
      ">> Epoch 346 finished \tANN training loss 0.014131\n",
      ">> Epoch 347 finished \tANN training loss 0.015320\n",
      ">> Epoch 348 finished \tANN training loss 0.015058\n",
      ">> Epoch 349 finished \tANN training loss 0.011563\n",
      ">> Epoch 350 finished \tANN training loss 0.012146\n",
      ">> Epoch 351 finished \tANN training loss 0.009194\n",
      ">> Epoch 352 finished \tANN training loss 0.009510\n",
      ">> Epoch 353 finished \tANN training loss 0.009816\n",
      ">> Epoch 354 finished \tANN training loss 0.033426\n",
      ">> Epoch 355 finished \tANN training loss 0.012967\n",
      ">> Epoch 356 finished \tANN training loss 0.008731\n",
      ">> Epoch 357 finished \tANN training loss 0.008797\n",
      ">> Epoch 358 finished \tANN training loss 0.010624\n",
      ">> Epoch 359 finished \tANN training loss 0.020303\n",
      ">> Epoch 360 finished \tANN training loss 0.014004\n",
      ">> Epoch 361 finished \tANN training loss 0.011085\n",
      ">> Epoch 362 finished \tANN training loss 0.012044\n",
      ">> Epoch 363 finished \tANN training loss 0.012630\n",
      ">> Epoch 364 finished \tANN training loss 0.008847\n",
      ">> Epoch 365 finished \tANN training loss 0.010014\n",
      ">> Epoch 366 finished \tANN training loss 0.011047\n",
      ">> Epoch 367 finished \tANN training loss 0.010000\n",
      ">> Epoch 368 finished \tANN training loss 0.010613\n",
      ">> Epoch 369 finished \tANN training loss 0.011122\n",
      ">> Epoch 370 finished \tANN training loss 0.014323\n",
      ">> Epoch 371 finished \tANN training loss 0.014721\n",
      ">> Epoch 372 finished \tANN training loss 0.012188\n",
      ">> Epoch 373 finished \tANN training loss 0.015402\n",
      ">> Epoch 374 finished \tANN training loss 0.011957\n",
      ">> Epoch 375 finished \tANN training loss 0.010717\n",
      ">> Epoch 376 finished \tANN training loss 0.009592\n",
      ">> Epoch 377 finished \tANN training loss 0.010248\n",
      ">> Epoch 378 finished \tANN training loss 0.008176\n",
      ">> Epoch 379 finished \tANN training loss 0.008958\n",
      ">> Epoch 380 finished \tANN training loss 0.010528\n",
      ">> Epoch 381 finished \tANN training loss 0.010800\n",
      ">> Epoch 382 finished \tANN training loss 0.012118\n",
      ">> Epoch 383 finished \tANN training loss 0.007901\n",
      ">> Epoch 384 finished \tANN training loss 0.008632\n",
      ">> Epoch 385 finished \tANN training loss 0.009630\n",
      ">> Epoch 386 finished \tANN training loss 0.007866\n",
      ">> Epoch 387 finished \tANN training loss 0.008423\n",
      ">> Epoch 388 finished \tANN training loss 0.008259\n",
      ">> Epoch 389 finished \tANN training loss 0.011889\n",
      ">> Epoch 390 finished \tANN training loss 0.007806\n",
      ">> Epoch 391 finished \tANN training loss 0.009485\n",
      ">> Epoch 392 finished \tANN training loss 0.011303\n",
      ">> Epoch 393 finished \tANN training loss 0.013005\n",
      ">> Epoch 394 finished \tANN training loss 0.010952\n",
      ">> Epoch 395 finished \tANN training loss 0.011446\n",
      ">> Epoch 396 finished \tANN training loss 0.010764\n",
      ">> Epoch 397 finished \tANN training loss 0.011839\n",
      ">> Epoch 398 finished \tANN training loss 0.008879\n",
      ">> Epoch 399 finished \tANN training loss 0.007989\n",
      ">> Epoch 400 finished \tANN training loss 0.011171\n",
      ">> Epoch 401 finished \tANN training loss 0.007910\n",
      ">> Epoch 402 finished \tANN training loss 0.007738\n",
      ">> Epoch 403 finished \tANN training loss 0.006582\n",
      ">> Epoch 404 finished \tANN training loss 0.008724\n",
      ">> Epoch 405 finished \tANN training loss 0.009327\n",
      ">> Epoch 406 finished \tANN training loss 0.011178\n",
      ">> Epoch 407 finished \tANN training loss 0.009506\n",
      ">> Epoch 408 finished \tANN training loss 0.008018\n",
      ">> Epoch 409 finished \tANN training loss 0.012519\n",
      ">> Epoch 410 finished \tANN training loss 0.008505\n",
      ">> Epoch 411 finished \tANN training loss 0.014735\n",
      ">> Epoch 412 finished \tANN training loss 0.009259\n",
      ">> Epoch 413 finished \tANN training loss 0.009497\n",
      ">> Epoch 414 finished \tANN training loss 0.008591\n",
      ">> Epoch 415 finished \tANN training loss 0.011153\n",
      ">> Epoch 416 finished \tANN training loss 0.010491\n",
      ">> Epoch 417 finished \tANN training loss 0.013730\n",
      ">> Epoch 418 finished \tANN training loss 0.010830\n",
      ">> Epoch 419 finished \tANN training loss 0.009972\n",
      ">> Epoch 420 finished \tANN training loss 0.006634\n",
      ">> Epoch 421 finished \tANN training loss 0.008643\n",
      ">> Epoch 422 finished \tANN training loss 0.006985\n",
      ">> Epoch 423 finished \tANN training loss 0.007159\n",
      ">> Epoch 424 finished \tANN training loss 0.008791\n",
      ">> Epoch 425 finished \tANN training loss 0.008522\n",
      ">> Epoch 426 finished \tANN training loss 0.010527\n",
      ">> Epoch 427 finished \tANN training loss 0.009953\n",
      ">> Epoch 428 finished \tANN training loss 0.010258\n",
      ">> Epoch 429 finished \tANN training loss 0.008812\n",
      ">> Epoch 430 finished \tANN training loss 0.007727\n",
      ">> Epoch 431 finished \tANN training loss 0.008002\n",
      ">> Epoch 432 finished \tANN training loss 0.008152\n",
      ">> Epoch 433 finished \tANN training loss 0.009926\n",
      ">> Epoch 434 finished \tANN training loss 0.006819\n",
      ">> Epoch 435 finished \tANN training loss 0.009088\n",
      ">> Epoch 436 finished \tANN training loss 0.005748\n",
      ">> Epoch 437 finished \tANN training loss 0.007264\n",
      ">> Epoch 438 finished \tANN training loss 0.008590\n",
      ">> Epoch 439 finished \tANN training loss 0.008826\n",
      ">> Epoch 440 finished \tANN training loss 0.008131\n",
      ">> Epoch 441 finished \tANN training loss 0.013771\n",
      ">> Epoch 442 finished \tANN training loss 0.007126\n",
      ">> Epoch 443 finished \tANN training loss 0.008822\n",
      ">> Epoch 444 finished \tANN training loss 0.006479\n",
      ">> Epoch 445 finished \tANN training loss 0.007820\n",
      ">> Epoch 446 finished \tANN training loss 0.007683\n",
      ">> Epoch 447 finished \tANN training loss 0.010703\n",
      ">> Epoch 448 finished \tANN training loss 0.011865\n",
      ">> Epoch 449 finished \tANN training loss 0.007893\n",
      ">> Epoch 450 finished \tANN training loss 0.008194\n",
      ">> Epoch 451 finished \tANN training loss 0.007734\n",
      ">> Epoch 452 finished \tANN training loss 0.008961\n",
      ">> Epoch 453 finished \tANN training loss 0.007999\n",
      ">> Epoch 454 finished \tANN training loss 0.009060\n",
      ">> Epoch 455 finished \tANN training loss 0.007732\n",
      ">> Epoch 456 finished \tANN training loss 0.006721\n",
      ">> Epoch 457 finished \tANN training loss 0.007330\n",
      ">> Epoch 458 finished \tANN training loss 0.006292\n",
      ">> Epoch 459 finished \tANN training loss 0.010252\n",
      ">> Epoch 460 finished \tANN training loss 0.011802\n",
      ">> Epoch 461 finished \tANN training loss 0.008437\n",
      ">> Epoch 462 finished \tANN training loss 0.007232\n",
      ">> Epoch 463 finished \tANN training loss 0.006363\n",
      ">> Epoch 464 finished \tANN training loss 0.007386\n",
      ">> Epoch 465 finished \tANN training loss 0.007357\n",
      ">> Epoch 466 finished \tANN training loss 0.006841\n",
      ">> Epoch 467 finished \tANN training loss 0.007511\n",
      ">> Epoch 468 finished \tANN training loss 0.008088\n",
      ">> Epoch 469 finished \tANN training loss 0.008107\n",
      ">> Epoch 470 finished \tANN training loss 0.007542\n",
      ">> Epoch 471 finished \tANN training loss 0.007940\n",
      ">> Epoch 472 finished \tANN training loss 0.009005\n",
      ">> Epoch 473 finished \tANN training loss 0.008601\n",
      ">> Epoch 474 finished \tANN training loss 0.008775\n",
      ">> Epoch 475 finished \tANN training loss 0.015289\n",
      ">> Epoch 476 finished \tANN training loss 0.010176\n",
      ">> Epoch 477 finished \tANN training loss 0.009052\n",
      ">> Epoch 478 finished \tANN training loss 0.006967\n",
      ">> Epoch 479 finished \tANN training loss 0.009000\n",
      ">> Epoch 480 finished \tANN training loss 0.009508\n",
      ">> Epoch 481 finished \tANN training loss 0.006859\n",
      ">> Epoch 482 finished \tANN training loss 0.008193\n",
      ">> Epoch 483 finished \tANN training loss 0.011717\n",
      ">> Epoch 484 finished \tANN training loss 0.007934\n",
      ">> Epoch 485 finished \tANN training loss 0.007657\n",
      ">> Epoch 486 finished \tANN training loss 0.006610\n",
      ">> Epoch 487 finished \tANN training loss 0.009018\n",
      ">> Epoch 488 finished \tANN training loss 0.007225\n",
      ">> Epoch 489 finished \tANN training loss 0.006311\n",
      ">> Epoch 490 finished \tANN training loss 0.008489\n",
      ">> Epoch 491 finished \tANN training loss 0.006570\n",
      ">> Epoch 492 finished \tANN training loss 0.006917\n",
      ">> Epoch 493 finished \tANN training loss 0.008494\n",
      ">> Epoch 494 finished \tANN training loss 0.008388\n",
      ">> Epoch 495 finished \tANN training loss 0.008606\n",
      ">> Epoch 496 finished \tANN training loss 0.010504\n",
      ">> Epoch 497 finished \tANN training loss 0.007798\n",
      ">> Epoch 498 finished \tANN training loss 0.007843\n",
      ">> Epoch 499 finished \tANN training loss 0.006651\n",
      ">> Epoch 500 finished \tANN training loss 0.007375\n",
      ">> Epoch 501 finished \tANN training loss 0.005100\n",
      ">> Epoch 502 finished \tANN training loss 0.004642\n",
      ">> Epoch 503 finished \tANN training loss 0.004366\n",
      ">> Epoch 504 finished \tANN training loss 0.005255\n",
      ">> Epoch 505 finished \tANN training loss 0.005889\n",
      ">> Epoch 506 finished \tANN training loss 0.006035\n",
      ">> Epoch 507 finished \tANN training loss 0.006962\n",
      ">> Epoch 508 finished \tANN training loss 0.005709\n",
      ">> Epoch 509 finished \tANN training loss 0.006254\n",
      ">> Epoch 510 finished \tANN training loss 0.004919\n",
      ">> Epoch 511 finished \tANN training loss 0.006091\n",
      ">> Epoch 512 finished \tANN training loss 0.006645\n",
      ">> Epoch 513 finished \tANN training loss 0.008559\n",
      ">> Epoch 514 finished \tANN training loss 0.006035\n",
      ">> Epoch 515 finished \tANN training loss 0.005571\n",
      ">> Epoch 516 finished \tANN training loss 0.006842\n",
      ">> Epoch 517 finished \tANN training loss 0.009076\n",
      ">> Epoch 518 finished \tANN training loss 0.007908\n",
      ">> Epoch 519 finished \tANN training loss 0.006441\n",
      ">> Epoch 520 finished \tANN training loss 0.005979\n",
      ">> Epoch 521 finished \tANN training loss 0.012616\n",
      ">> Epoch 522 finished \tANN training loss 0.009998\n",
      ">> Epoch 523 finished \tANN training loss 0.007953\n",
      ">> Epoch 524 finished \tANN training loss 0.010502\n",
      ">> Epoch 525 finished \tANN training loss 0.009002\n",
      ">> Epoch 526 finished \tANN training loss 0.010813\n",
      ">> Epoch 527 finished \tANN training loss 0.009763\n",
      ">> Epoch 528 finished \tANN training loss 0.007423\n",
      ">> Epoch 529 finished \tANN training loss 0.006302\n",
      ">> Epoch 530 finished \tANN training loss 0.007125\n",
      ">> Epoch 531 finished \tANN training loss 0.008101\n",
      ">> Epoch 532 finished \tANN training loss 0.009954\n",
      ">> Epoch 533 finished \tANN training loss 0.006702\n",
      ">> Epoch 534 finished \tANN training loss 0.005567\n",
      ">> Epoch 535 finished \tANN training loss 0.005273\n",
      ">> Epoch 536 finished \tANN training loss 0.006686\n",
      ">> Epoch 537 finished \tANN training loss 0.007259\n",
      ">> Epoch 538 finished \tANN training loss 0.006224\n",
      ">> Epoch 539 finished \tANN training loss 0.004710\n",
      ">> Epoch 540 finished \tANN training loss 0.004871\n",
      ">> Epoch 541 finished \tANN training loss 0.005529\n",
      ">> Epoch 542 finished \tANN training loss 0.005378\n",
      ">> Epoch 543 finished \tANN training loss 0.005143\n",
      ">> Epoch 544 finished \tANN training loss 0.007020\n",
      ">> Epoch 545 finished \tANN training loss 0.005487\n",
      ">> Epoch 546 finished \tANN training loss 0.006549\n",
      ">> Epoch 547 finished \tANN training loss 0.007101\n",
      ">> Epoch 548 finished \tANN training loss 0.005296\n",
      ">> Epoch 549 finished \tANN training loss 0.004900\n",
      ">> Epoch 550 finished \tANN training loss 0.006664\n",
      ">> Epoch 551 finished \tANN training loss 0.006060\n",
      ">> Epoch 552 finished \tANN training loss 0.008395\n",
      ">> Epoch 553 finished \tANN training loss 0.005712\n",
      ">> Epoch 554 finished \tANN training loss 0.005205\n",
      ">> Epoch 555 finished \tANN training loss 0.011685\n",
      ">> Epoch 556 finished \tANN training loss 0.006589\n",
      ">> Epoch 557 finished \tANN training loss 0.006289\n",
      ">> Epoch 558 finished \tANN training loss 0.006943\n",
      ">> Epoch 559 finished \tANN training loss 0.006727\n",
      ">> Epoch 560 finished \tANN training loss 0.004924\n",
      ">> Epoch 561 finished \tANN training loss 0.006086\n",
      ">> Epoch 562 finished \tANN training loss 0.004954\n",
      ">> Epoch 563 finished \tANN training loss 0.004700\n",
      ">> Epoch 564 finished \tANN training loss 0.006260\n",
      ">> Epoch 565 finished \tANN training loss 0.004625\n",
      ">> Epoch 566 finished \tANN training loss 0.004366\n",
      ">> Epoch 567 finished \tANN training loss 0.004201\n",
      ">> Epoch 568 finished \tANN training loss 0.003462\n",
      ">> Epoch 569 finished \tANN training loss 0.003518\n",
      ">> Epoch 570 finished \tANN training loss 0.005793\n",
      ">> Epoch 571 finished \tANN training loss 0.010691\n",
      ">> Epoch 572 finished \tANN training loss 0.004844\n",
      ">> Epoch 573 finished \tANN training loss 0.003776\n",
      ">> Epoch 574 finished \tANN training loss 0.005251\n",
      ">> Epoch 575 finished \tANN training loss 0.005430\n",
      ">> Epoch 576 finished \tANN training loss 0.006211\n",
      ">> Epoch 577 finished \tANN training loss 0.004819\n",
      ">> Epoch 578 finished \tANN training loss 0.007231\n",
      ">> Epoch 579 finished \tANN training loss 0.005987\n",
      ">> Epoch 580 finished \tANN training loss 0.005326\n",
      ">> Epoch 581 finished \tANN training loss 0.005203\n",
      ">> Epoch 582 finished \tANN training loss 0.005032\n",
      ">> Epoch 583 finished \tANN training loss 0.005412\n",
      ">> Epoch 584 finished \tANN training loss 0.004856\n",
      ">> Epoch 585 finished \tANN training loss 0.006524\n",
      ">> Epoch 586 finished \tANN training loss 0.005623\n",
      ">> Epoch 587 finished \tANN training loss 0.006694\n",
      ">> Epoch 588 finished \tANN training loss 0.004549\n",
      ">> Epoch 589 finished \tANN training loss 0.004867\n",
      ">> Epoch 590 finished \tANN training loss 0.004635\n",
      ">> Epoch 591 finished \tANN training loss 0.008643\n",
      ">> Epoch 592 finished \tANN training loss 0.006355\n",
      ">> Epoch 593 finished \tANN training loss 0.005479\n",
      ">> Epoch 594 finished \tANN training loss 0.006805\n",
      ">> Epoch 595 finished \tANN training loss 0.006200\n",
      ">> Epoch 596 finished \tANN training loss 0.005099\n",
      ">> Epoch 597 finished \tANN training loss 0.005410\n",
      ">> Epoch 598 finished \tANN training loss 0.009338\n",
      ">> Epoch 599 finished \tANN training loss 0.007782\n",
      ">> Epoch 600 finished \tANN training loss 0.006033\n",
      ">> Epoch 601 finished \tANN training loss 0.005317\n",
      ">> Epoch 602 finished \tANN training loss 0.006250\n",
      ">> Epoch 603 finished \tANN training loss 0.005246\n",
      ">> Epoch 604 finished \tANN training loss 0.005475\n",
      ">> Epoch 605 finished \tANN training loss 0.004510\n",
      ">> Epoch 606 finished \tANN training loss 0.008375\n",
      ">> Epoch 607 finished \tANN training loss 0.007128\n",
      ">> Epoch 608 finished \tANN training loss 0.005323\n",
      ">> Epoch 609 finished \tANN training loss 0.007705\n",
      ">> Epoch 610 finished \tANN training loss 0.007534\n",
      ">> Epoch 611 finished \tANN training loss 0.007171\n",
      ">> Epoch 612 finished \tANN training loss 0.008036\n",
      ">> Epoch 613 finished \tANN training loss 0.007537\n",
      ">> Epoch 614 finished \tANN training loss 0.006504\n",
      ">> Epoch 615 finished \tANN training loss 0.008612\n",
      ">> Epoch 616 finished \tANN training loss 0.006877\n",
      ">> Epoch 617 finished \tANN training loss 0.005585\n",
      ">> Epoch 618 finished \tANN training loss 0.006041\n",
      ">> Epoch 619 finished \tANN training loss 0.009845\n",
      ">> Epoch 620 finished \tANN training loss 0.007561\n",
      ">> Epoch 621 finished \tANN training loss 0.008790\n",
      ">> Epoch 622 finished \tANN training loss 0.008724\n",
      ">> Epoch 623 finished \tANN training loss 0.006414\n",
      ">> Epoch 624 finished \tANN training loss 0.006051\n",
      ">> Epoch 625 finished \tANN training loss 0.004443\n",
      ">> Epoch 626 finished \tANN training loss 0.005350\n",
      ">> Epoch 627 finished \tANN training loss 0.008050\n",
      ">> Epoch 628 finished \tANN training loss 0.006773\n",
      ">> Epoch 629 finished \tANN training loss 0.006742\n",
      ">> Epoch 630 finished \tANN training loss 0.006824\n",
      ">> Epoch 631 finished \tANN training loss 0.007929\n",
      ">> Epoch 632 finished \tANN training loss 0.006285\n",
      ">> Epoch 633 finished \tANN training loss 0.006047\n",
      ">> Epoch 634 finished \tANN training loss 0.004947\n",
      ">> Epoch 635 finished \tANN training loss 0.005760\n",
      ">> Epoch 636 finished \tANN training loss 0.008001\n",
      ">> Epoch 637 finished \tANN training loss 0.006394\n",
      ">> Epoch 638 finished \tANN training loss 0.005816\n",
      ">> Epoch 639 finished \tANN training loss 0.004856\n",
      ">> Epoch 640 finished \tANN training loss 0.005346\n",
      ">> Epoch 641 finished \tANN training loss 0.004102\n",
      ">> Epoch 642 finished \tANN training loss 0.004660\n",
      ">> Epoch 643 finished \tANN training loss 0.007819\n",
      ">> Epoch 644 finished \tANN training loss 0.008505\n",
      ">> Epoch 645 finished \tANN training loss 0.007258\n",
      ">> Epoch 646 finished \tANN training loss 0.008755\n",
      ">> Epoch 647 finished \tANN training loss 0.005763\n",
      ">> Epoch 648 finished \tANN training loss 0.006678\n",
      ">> Epoch 649 finished \tANN training loss 0.006168\n",
      ">> Epoch 650 finished \tANN training loss 0.004092\n",
      ">> Epoch 651 finished \tANN training loss 0.005470\n",
      ">> Epoch 652 finished \tANN training loss 0.004396\n",
      ">> Epoch 653 finished \tANN training loss 0.004317\n",
      ">> Epoch 654 finished \tANN training loss 0.005240\n",
      ">> Epoch 655 finished \tANN training loss 0.005617\n",
      ">> Epoch 656 finished \tANN training loss 0.005667\n",
      ">> Epoch 657 finished \tANN training loss 0.006083\n",
      ">> Epoch 658 finished \tANN training loss 0.006254\n",
      ">> Epoch 659 finished \tANN training loss 0.005947\n",
      ">> Epoch 660 finished \tANN training loss 0.008786\n",
      ">> Epoch 661 finished \tANN training loss 0.007234\n",
      ">> Epoch 662 finished \tANN training loss 0.006748\n",
      ">> Epoch 663 finished \tANN training loss 0.007102\n",
      ">> Epoch 664 finished \tANN training loss 0.009448\n",
      ">> Epoch 665 finished \tANN training loss 0.007853\n",
      ">> Epoch 666 finished \tANN training loss 0.007081\n",
      ">> Epoch 667 finished \tANN training loss 0.008082\n",
      ">> Epoch 668 finished \tANN training loss 0.008077\n",
      ">> Epoch 669 finished \tANN training loss 0.009129\n",
      ">> Epoch 670 finished \tANN training loss 0.007191\n",
      ">> Epoch 671 finished \tANN training loss 0.007542\n",
      ">> Epoch 672 finished \tANN training loss 0.009159\n",
      ">> Epoch 673 finished \tANN training loss 0.010238\n",
      ">> Epoch 674 finished \tANN training loss 0.008830\n",
      ">> Epoch 675 finished \tANN training loss 0.006082\n",
      ">> Epoch 676 finished \tANN training loss 0.007246\n",
      ">> Epoch 677 finished \tANN training loss 0.007780\n",
      ">> Epoch 678 finished \tANN training loss 0.008128\n",
      ">> Epoch 679 finished \tANN training loss 0.007226\n",
      ">> Epoch 680 finished \tANN training loss 0.006005\n",
      ">> Epoch 681 finished \tANN training loss 0.005689\n",
      ">> Epoch 682 finished \tANN training loss 0.005733\n",
      ">> Epoch 683 finished \tANN training loss 0.004141\n",
      ">> Epoch 684 finished \tANN training loss 0.004089\n",
      ">> Epoch 685 finished \tANN training loss 0.005383\n",
      ">> Epoch 686 finished \tANN training loss 0.006793\n",
      ">> Epoch 687 finished \tANN training loss 0.007451\n",
      ">> Epoch 688 finished \tANN training loss 0.007231\n",
      ">> Epoch 689 finished \tANN training loss 0.005679\n",
      ">> Epoch 690 finished \tANN training loss 0.005108\n",
      ">> Epoch 691 finished \tANN training loss 0.006008\n",
      ">> Epoch 692 finished \tANN training loss 0.005950\n",
      ">> Epoch 693 finished \tANN training loss 0.005042\n",
      ">> Epoch 694 finished \tANN training loss 0.005580\n",
      ">> Epoch 695 finished \tANN training loss 0.005692\n",
      ">> Epoch 696 finished \tANN training loss 0.004596\n",
      ">> Epoch 697 finished \tANN training loss 0.005473\n",
      ">> Epoch 698 finished \tANN training loss 0.006743\n",
      ">> Epoch 699 finished \tANN training loss 0.007372\n",
      ">> Epoch 700 finished \tANN training loss 0.004608\n",
      ">> Epoch 701 finished \tANN training loss 0.004786\n",
      ">> Epoch 702 finished \tANN training loss 0.004609\n",
      ">> Epoch 703 finished \tANN training loss 0.005222\n",
      ">> Epoch 704 finished \tANN training loss 0.004843\n",
      ">> Epoch 705 finished \tANN training loss 0.004174\n",
      ">> Epoch 706 finished \tANN training loss 0.005587\n",
      ">> Epoch 707 finished \tANN training loss 0.004384\n",
      ">> Epoch 708 finished \tANN training loss 0.004774\n",
      ">> Epoch 709 finished \tANN training loss 0.005265\n",
      ">> Epoch 710 finished \tANN training loss 0.004796\n",
      ">> Epoch 711 finished \tANN training loss 0.005457\n",
      ">> Epoch 712 finished \tANN training loss 0.005110\n",
      ">> Epoch 713 finished \tANN training loss 0.005170\n",
      ">> Epoch 714 finished \tANN training loss 0.005292\n",
      ">> Epoch 715 finished \tANN training loss 0.005493\n",
      ">> Epoch 716 finished \tANN training loss 0.005592\n",
      ">> Epoch 717 finished \tANN training loss 0.004160\n",
      ">> Epoch 718 finished \tANN training loss 0.004404\n",
      ">> Epoch 719 finished \tANN training loss 0.005253\n",
      ">> Epoch 720 finished \tANN training loss 0.006622\n",
      ">> Epoch 721 finished \tANN training loss 0.007539\n",
      ">> Epoch 722 finished \tANN training loss 0.006470\n",
      ">> Epoch 723 finished \tANN training loss 0.005558\n",
      ">> Epoch 724 finished \tANN training loss 0.004443\n",
      ">> Epoch 725 finished \tANN training loss 0.005643\n",
      ">> Epoch 726 finished \tANN training loss 0.005014\n",
      ">> Epoch 727 finished \tANN training loss 0.004843\n",
      ">> Epoch 728 finished \tANN training loss 0.004331\n",
      ">> Epoch 729 finished \tANN training loss 0.004835\n",
      ">> Epoch 730 finished \tANN training loss 0.006965\n",
      ">> Epoch 731 finished \tANN training loss 0.003860\n",
      ">> Epoch 732 finished \tANN training loss 0.005511\n",
      ">> Epoch 733 finished \tANN training loss 0.004248\n",
      ">> Epoch 734 finished \tANN training loss 0.005287\n",
      ">> Epoch 735 finished \tANN training loss 0.005145\n",
      ">> Epoch 736 finished \tANN training loss 0.005695\n",
      ">> Epoch 737 finished \tANN training loss 0.006079\n",
      ">> Epoch 738 finished \tANN training loss 0.006849\n",
      ">> Epoch 739 finished \tANN training loss 0.005149\n",
      ">> Epoch 740 finished \tANN training loss 0.005703\n",
      ">> Epoch 741 finished \tANN training loss 0.006325\n",
      ">> Epoch 742 finished \tANN training loss 0.005167\n",
      ">> Epoch 743 finished \tANN training loss 0.006399\n",
      ">> Epoch 744 finished \tANN training loss 0.008847\n",
      ">> Epoch 745 finished \tANN training loss 0.004087\n",
      ">> Epoch 746 finished \tANN training loss 0.005649\n",
      ">> Epoch 747 finished \tANN training loss 0.004552\n",
      ">> Epoch 748 finished \tANN training loss 0.004090\n",
      ">> Epoch 749 finished \tANN training loss 0.005460\n",
      ">> Epoch 750 finished \tANN training loss 0.004501\n",
      ">> Epoch 751 finished \tANN training loss 0.005043\n",
      ">> Epoch 752 finished \tANN training loss 0.004117\n",
      ">> Epoch 753 finished \tANN training loss 0.003694\n",
      ">> Epoch 754 finished \tANN training loss 0.003020\n",
      ">> Epoch 755 finished \tANN training loss 0.002358\n",
      ">> Epoch 756 finished \tANN training loss 0.002666\n",
      ">> Epoch 757 finished \tANN training loss 0.003834\n",
      ">> Epoch 758 finished \tANN training loss 0.003887\n",
      ">> Epoch 759 finished \tANN training loss 0.003753\n",
      ">> Epoch 760 finished \tANN training loss 0.003234\n",
      ">> Epoch 761 finished \tANN training loss 0.004534\n",
      ">> Epoch 762 finished \tANN training loss 0.004357\n",
      ">> Epoch 763 finished \tANN training loss 0.004710\n",
      ">> Epoch 764 finished \tANN training loss 0.004652\n",
      ">> Epoch 765 finished \tANN training loss 0.005220\n",
      ">> Epoch 766 finished \tANN training loss 0.005124\n",
      ">> Epoch 767 finished \tANN training loss 0.006167\n",
      ">> Epoch 768 finished \tANN training loss 0.005963\n",
      ">> Epoch 769 finished \tANN training loss 0.004377\n",
      ">> Epoch 770 finished \tANN training loss 0.004064\n",
      ">> Epoch 771 finished \tANN training loss 0.003612\n",
      ">> Epoch 772 finished \tANN training loss 0.004219\n",
      ">> Epoch 773 finished \tANN training loss 0.003099\n",
      ">> Epoch 774 finished \tANN training loss 0.002978\n",
      ">> Epoch 775 finished \tANN training loss 0.004675\n",
      ">> Epoch 776 finished \tANN training loss 0.004870\n",
      ">> Epoch 777 finished \tANN training loss 0.003656\n",
      ">> Epoch 778 finished \tANN training loss 0.004600\n",
      ">> Epoch 779 finished \tANN training loss 0.004708\n",
      ">> Epoch 780 finished \tANN training loss 0.003563\n",
      ">> Epoch 781 finished \tANN training loss 0.003048\n",
      ">> Epoch 782 finished \tANN training loss 0.003853\n",
      ">> Epoch 783 finished \tANN training loss 0.005037\n",
      ">> Epoch 784 finished \tANN training loss 0.004784\n",
      ">> Epoch 785 finished \tANN training loss 0.004935\n",
      ">> Epoch 786 finished \tANN training loss 0.005892\n",
      ">> Epoch 787 finished \tANN training loss 0.004883\n",
      ">> Epoch 788 finished \tANN training loss 0.004987\n",
      ">> Epoch 789 finished \tANN training loss 0.005540\n",
      ">> Epoch 790 finished \tANN training loss 0.006029\n",
      ">> Epoch 791 finished \tANN training loss 0.006152\n",
      ">> Epoch 792 finished \tANN training loss 0.006059\n",
      ">> Epoch 793 finished \tANN training loss 0.006643\n",
      ">> Epoch 794 finished \tANN training loss 0.006239\n",
      ">> Epoch 795 finished \tANN training loss 0.005823\n",
      ">> Epoch 796 finished \tANN training loss 0.010853\n",
      ">> Epoch 797 finished \tANN training loss 0.006634\n",
      ">> Epoch 798 finished \tANN training loss 0.006018\n",
      ">> Epoch 799 finished \tANN training loss 0.005396\n",
      ">> Epoch 800 finished \tANN training loss 0.004746\n",
      ">> Epoch 801 finished \tANN training loss 0.006998\n",
      ">> Epoch 802 finished \tANN training loss 0.006408\n",
      ">> Epoch 803 finished \tANN training loss 0.006476\n",
      ">> Epoch 804 finished \tANN training loss 0.005204\n",
      ">> Epoch 805 finished \tANN training loss 0.004256\n",
      ">> Epoch 806 finished \tANN training loss 0.003847\n",
      ">> Epoch 807 finished \tANN training loss 0.005131\n",
      ">> Epoch 808 finished \tANN training loss 0.003539\n",
      ">> Epoch 809 finished \tANN training loss 0.003058\n",
      ">> Epoch 810 finished \tANN training loss 0.002668\n",
      ">> Epoch 811 finished \tANN training loss 0.002818\n",
      ">> Epoch 812 finished \tANN training loss 0.002979\n",
      ">> Epoch 813 finished \tANN training loss 0.005084\n",
      ">> Epoch 814 finished \tANN training loss 0.004665\n",
      ">> Epoch 815 finished \tANN training loss 0.005305\n",
      ">> Epoch 816 finished \tANN training loss 0.003489\n",
      ">> Epoch 817 finished \tANN training loss 0.004233\n",
      ">> Epoch 818 finished \tANN training loss 0.003595\n",
      ">> Epoch 819 finished \tANN training loss 0.003670\n",
      ">> Epoch 820 finished \tANN training loss 0.003759\n",
      ">> Epoch 821 finished \tANN training loss 0.003535\n",
      ">> Epoch 822 finished \tANN training loss 0.005396\n",
      ">> Epoch 823 finished \tANN training loss 0.003860\n",
      ">> Epoch 824 finished \tANN training loss 0.004330\n",
      ">> Epoch 825 finished \tANN training loss 0.004778\n",
      ">> Epoch 826 finished \tANN training loss 0.005172\n",
      ">> Epoch 827 finished \tANN training loss 0.004594\n",
      ">> Epoch 828 finished \tANN training loss 0.003811\n",
      ">> Epoch 829 finished \tANN training loss 0.004308\n",
      ">> Epoch 830 finished \tANN training loss 0.005248\n",
      ">> Epoch 831 finished \tANN training loss 0.003796\n",
      ">> Epoch 832 finished \tANN training loss 0.004945\n",
      ">> Epoch 833 finished \tANN training loss 0.003213\n",
      ">> Epoch 834 finished \tANN training loss 0.003971\n",
      ">> Epoch 835 finished \tANN training loss 0.005929\n",
      ">> Epoch 836 finished \tANN training loss 0.004424\n",
      ">> Epoch 837 finished \tANN training loss 0.002882\n",
      ">> Epoch 838 finished \tANN training loss 0.003472\n",
      ">> Epoch 839 finished \tANN training loss 0.003264\n",
      ">> Epoch 840 finished \tANN training loss 0.003600\n",
      ">> Epoch 841 finished \tANN training loss 0.002917\n",
      ">> Epoch 842 finished \tANN training loss 0.005170\n",
      ">> Epoch 843 finished \tANN training loss 0.004263\n",
      ">> Epoch 844 finished \tANN training loss 0.003672\n",
      ">> Epoch 845 finished \tANN training loss 0.003458\n",
      ">> Epoch 846 finished \tANN training loss 0.003257\n",
      ">> Epoch 847 finished \tANN training loss 0.003866\n",
      ">> Epoch 848 finished \tANN training loss 0.003022\n",
      ">> Epoch 849 finished \tANN training loss 0.002764\n",
      ">> Epoch 850 finished \tANN training loss 0.003220\n",
      ">> Epoch 851 finished \tANN training loss 0.003434\n",
      ">> Epoch 852 finished \tANN training loss 0.003407\n",
      ">> Epoch 853 finished \tANN training loss 0.003330\n",
      ">> Epoch 854 finished \tANN training loss 0.003550\n",
      ">> Epoch 855 finished \tANN training loss 0.002771\n",
      ">> Epoch 856 finished \tANN training loss 0.002773\n",
      ">> Epoch 857 finished \tANN training loss 0.002320\n",
      ">> Epoch 858 finished \tANN training loss 0.003015\n",
      ">> Epoch 859 finished \tANN training loss 0.002740\n",
      ">> Epoch 860 finished \tANN training loss 0.003872\n",
      ">> Epoch 861 finished \tANN training loss 0.005068\n",
      ">> Epoch 862 finished \tANN training loss 0.003581\n",
      ">> Epoch 863 finished \tANN training loss 0.004286\n",
      ">> Epoch 864 finished \tANN training loss 0.003310\n",
      ">> Epoch 865 finished \tANN training loss 0.003536\n",
      ">> Epoch 866 finished \tANN training loss 0.004028\n",
      ">> Epoch 867 finished \tANN training loss 0.003840\n",
      ">> Epoch 868 finished \tANN training loss 0.005477\n",
      ">> Epoch 869 finished \tANN training loss 0.003603\n",
      ">> Epoch 870 finished \tANN training loss 0.003925\n",
      ">> Epoch 871 finished \tANN training loss 0.004118\n",
      ">> Epoch 872 finished \tANN training loss 0.003283\n",
      ">> Epoch 873 finished \tANN training loss 0.003205\n",
      ">> Epoch 874 finished \tANN training loss 0.004504\n",
      ">> Epoch 875 finished \tANN training loss 0.003678\n",
      ">> Epoch 876 finished \tANN training loss 0.003311\n",
      ">> Epoch 877 finished \tANN training loss 0.002835\n",
      ">> Epoch 878 finished \tANN training loss 0.003375\n",
      ">> Epoch 879 finished \tANN training loss 0.004387\n",
      ">> Epoch 880 finished \tANN training loss 0.003660\n",
      ">> Epoch 881 finished \tANN training loss 0.003430\n",
      ">> Epoch 882 finished \tANN training loss 0.003890\n",
      ">> Epoch 883 finished \tANN training loss 0.005131\n",
      ">> Epoch 884 finished \tANN training loss 0.005775\n",
      ">> Epoch 885 finished \tANN training loss 0.004540\n",
      ">> Epoch 886 finished \tANN training loss 0.003458\n",
      ">> Epoch 887 finished \tANN training loss 0.003586\n",
      ">> Epoch 888 finished \tANN training loss 0.003027\n",
      ">> Epoch 889 finished \tANN training loss 0.002636\n",
      ">> Epoch 890 finished \tANN training loss 0.003431\n",
      ">> Epoch 891 finished \tANN training loss 0.003723\n",
      ">> Epoch 892 finished \tANN training loss 0.003944\n",
      ">> Epoch 893 finished \tANN training loss 0.003517\n",
      ">> Epoch 894 finished \tANN training loss 0.003399\n",
      ">> Epoch 895 finished \tANN training loss 0.005098\n",
      ">> Epoch 896 finished \tANN training loss 0.004144\n",
      ">> Epoch 897 finished \tANN training loss 0.004642\n",
      ">> Epoch 898 finished \tANN training loss 0.005100\n",
      ">> Epoch 899 finished \tANN training loss 0.004248\n",
      ">> Epoch 900 finished \tANN training loss 0.003477\n",
      ">> Epoch 901 finished \tANN training loss 0.003923\n",
      ">> Epoch 902 finished \tANN training loss 0.004135\n",
      ">> Epoch 903 finished \tANN training loss 0.003410\n",
      ">> Epoch 904 finished \tANN training loss 0.003320\n",
      ">> Epoch 905 finished \tANN training loss 0.004413\n",
      ">> Epoch 906 finished \tANN training loss 0.004001\n",
      ">> Epoch 907 finished \tANN training loss 0.004214\n",
      ">> Epoch 908 finished \tANN training loss 0.003775\n",
      ">> Epoch 909 finished \tANN training loss 0.003524\n",
      ">> Epoch 910 finished \tANN training loss 0.004369\n",
      ">> Epoch 911 finished \tANN training loss 0.004403\n",
      ">> Epoch 912 finished \tANN training loss 0.003826\n",
      ">> Epoch 913 finished \tANN training loss 0.003608\n",
      ">> Epoch 914 finished \tANN training loss 0.003142\n",
      ">> Epoch 915 finished \tANN training loss 0.002686\n",
      ">> Epoch 916 finished \tANN training loss 0.003260\n",
      ">> Epoch 917 finished \tANN training loss 0.003117\n",
      ">> Epoch 918 finished \tANN training loss 0.005085\n",
      ">> Epoch 919 finished \tANN training loss 0.005095\n",
      ">> Epoch 920 finished \tANN training loss 0.004682\n",
      ">> Epoch 921 finished \tANN training loss 0.003975\n",
      ">> Epoch 922 finished \tANN training loss 0.004280\n",
      ">> Epoch 923 finished \tANN training loss 0.004595\n",
      ">> Epoch 924 finished \tANN training loss 0.004133\n",
      ">> Epoch 925 finished \tANN training loss 0.004730\n",
      ">> Epoch 926 finished \tANN training loss 0.004176\n",
      ">> Epoch 927 finished \tANN training loss 0.004222\n",
      ">> Epoch 928 finished \tANN training loss 0.003496\n",
      ">> Epoch 929 finished \tANN training loss 0.003831\n",
      ">> Epoch 930 finished \tANN training loss 0.003169\n",
      ">> Epoch 931 finished \tANN training loss 0.002932\n",
      ">> Epoch 932 finished \tANN training loss 0.005438\n",
      ">> Epoch 933 finished \tANN training loss 0.003868\n",
      ">> Epoch 934 finished \tANN training loss 0.003915\n",
      ">> Epoch 935 finished \tANN training loss 0.003488\n",
      ">> Epoch 936 finished \tANN training loss 0.003013\n",
      ">> Epoch 937 finished \tANN training loss 0.003725\n",
      ">> Epoch 938 finished \tANN training loss 0.004794\n",
      ">> Epoch 939 finished \tANN training loss 0.006535\n",
      ">> Epoch 940 finished \tANN training loss 0.004762\n",
      ">> Epoch 941 finished \tANN training loss 0.003413\n",
      ">> Epoch 942 finished \tANN training loss 0.003248\n",
      ">> Epoch 943 finished \tANN training loss 0.003187\n",
      ">> Epoch 944 finished \tANN training loss 0.002999\n",
      ">> Epoch 945 finished \tANN training loss 0.003316\n",
      ">> Epoch 946 finished \tANN training loss 0.003772\n",
      ">> Epoch 947 finished \tANN training loss 0.003444\n",
      ">> Epoch 948 finished \tANN training loss 0.002518\n",
      ">> Epoch 949 finished \tANN training loss 0.002439\n",
      ">> Epoch 950 finished \tANN training loss 0.003121\n",
      ">> Epoch 951 finished \tANN training loss 0.002308\n",
      ">> Epoch 952 finished \tANN training loss 0.003816\n",
      ">> Epoch 953 finished \tANN training loss 0.002253\n",
      ">> Epoch 954 finished \tANN training loss 0.002226\n",
      ">> Epoch 955 finished \tANN training loss 0.002800\n",
      ">> Epoch 956 finished \tANN training loss 0.003536\n",
      ">> Epoch 957 finished \tANN training loss 0.003436\n",
      ">> Epoch 958 finished \tANN training loss 0.003200\n",
      ">> Epoch 959 finished \tANN training loss 0.002751\n",
      ">> Epoch 960 finished \tANN training loss 0.003256\n",
      ">> Epoch 961 finished \tANN training loss 0.002271\n",
      ">> Epoch 962 finished \tANN training loss 0.002135\n",
      ">> Epoch 963 finished \tANN training loss 0.002360\n",
      ">> Epoch 964 finished \tANN training loss 0.002072\n",
      ">> Epoch 965 finished \tANN training loss 0.002648\n",
      ">> Epoch 966 finished \tANN training loss 0.002414\n",
      ">> Epoch 967 finished \tANN training loss 0.002256\n",
      ">> Epoch 968 finished \tANN training loss 0.002691\n",
      ">> Epoch 969 finished \tANN training loss 0.003392\n",
      ">> Epoch 970 finished \tANN training loss 0.002768\n",
      ">> Epoch 971 finished \tANN training loss 0.004151\n",
      ">> Epoch 972 finished \tANN training loss 0.003232\n",
      ">> Epoch 973 finished \tANN training loss 0.003780\n",
      ">> Epoch 974 finished \tANN training loss 0.004409\n",
      ">> Epoch 975 finished \tANN training loss 0.003863\n",
      ">> Epoch 976 finished \tANN training loss 0.004984\n",
      ">> Epoch 977 finished \tANN training loss 0.004855\n",
      ">> Epoch 978 finished \tANN training loss 0.004642\n",
      ">> Epoch 979 finished \tANN training loss 0.005390\n",
      ">> Epoch 980 finished \tANN training loss 0.004554\n",
      ">> Epoch 981 finished \tANN training loss 0.004313\n",
      ">> Epoch 982 finished \tANN training loss 0.004477\n",
      ">> Epoch 983 finished \tANN training loss 0.004391\n",
      ">> Epoch 984 finished \tANN training loss 0.004495\n",
      ">> Epoch 985 finished \tANN training loss 0.005236\n",
      ">> Epoch 986 finished \tANN training loss 0.003804\n",
      ">> Epoch 987 finished \tANN training loss 0.004232\n",
      ">> Epoch 988 finished \tANN training loss 0.004316\n",
      ">> Epoch 989 finished \tANN training loss 0.002723\n",
      ">> Epoch 990 finished \tANN training loss 0.002734\n",
      ">> Epoch 991 finished \tANN training loss 0.003465\n",
      ">> Epoch 992 finished \tANN training loss 0.003380\n",
      ">> Epoch 993 finished \tANN training loss 0.003258\n",
      ">> Epoch 994 finished \tANN training loss 0.002719\n",
      ">> Epoch 995 finished \tANN training loss 0.005936\n",
      ">> Epoch 996 finished \tANN training loss 0.004470\n",
      ">> Epoch 997 finished \tANN training loss 0.002997\n",
      ">> Epoch 998 finished \tANN training loss 0.003685\n",
      ">> Epoch 999 finished \tANN training loss 0.003236\n",
      "[END] Fine tuning step\n",
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 7.923216\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 7.879568\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 7.605730\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 7.072086\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 6.432382\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 5.670035\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 5.152479\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 4.626490\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 4.305560\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 3.964083\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 3.739432\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 3.509390\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 3.360844\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 3.188164\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 3.043104\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 2.884848\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 2.763461\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 2.691365\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 2.558435\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 2.475425\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 5.395651\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 5.191466\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 4.839898\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 3.963656\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 3.065214\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 2.390023\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 1.995175\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 1.800539\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 1.508078\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 1.379617\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 1.342187\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 1.117192\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 1.112504\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 0.927185\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 0.930732\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 0.879582\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 0.848981\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 0.829884\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 0.784930\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 0.764917\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.883100\n",
      ">> Epoch 1 finished \tANN training loss 0.770430\n",
      ">> Epoch 2 finished \tANN training loss 0.705976\n",
      ">> Epoch 3 finished \tANN training loss 0.627977\n",
      ">> Epoch 4 finished \tANN training loss 0.601167\n",
      ">> Epoch 5 finished \tANN training loss 0.540239\n",
      ">> Epoch 6 finished \tANN training loss 0.507643\n",
      ">> Epoch 7 finished \tANN training loss 0.482516\n",
      ">> Epoch 8 finished \tANN training loss 0.456589\n",
      ">> Epoch 9 finished \tANN training loss 0.441042\n",
      ">> Epoch 10 finished \tANN training loss 0.426030\n",
      ">> Epoch 11 finished \tANN training loss 0.473472\n",
      ">> Epoch 12 finished \tANN training loss 0.392719\n",
      ">> Epoch 13 finished \tANN training loss 0.358948\n",
      ">> Epoch 14 finished \tANN training loss 0.358411\n",
      ">> Epoch 15 finished \tANN training loss 0.339409\n",
      ">> Epoch 16 finished \tANN training loss 0.329184\n",
      ">> Epoch 17 finished \tANN training loss 0.339094\n",
      ">> Epoch 18 finished \tANN training loss 0.316966\n",
      ">> Epoch 19 finished \tANN training loss 0.297558\n",
      ">> Epoch 20 finished \tANN training loss 0.280334\n",
      ">> Epoch 21 finished \tANN training loss 0.276189\n",
      ">> Epoch 22 finished \tANN training loss 0.287989\n",
      ">> Epoch 23 finished \tANN training loss 0.262067\n",
      ">> Epoch 24 finished \tANN training loss 0.274489\n",
      ">> Epoch 25 finished \tANN training loss 0.243720\n",
      ">> Epoch 26 finished \tANN training loss 0.245697\n",
      ">> Epoch 27 finished \tANN training loss 0.231152\n",
      ">> Epoch 28 finished \tANN training loss 0.276390\n",
      ">> Epoch 29 finished \tANN training loss 0.221829\n",
      ">> Epoch 30 finished \tANN training loss 0.215526\n",
      ">> Epoch 31 finished \tANN training loss 0.203401\n",
      ">> Epoch 32 finished \tANN training loss 0.197002\n",
      ">> Epoch 33 finished \tANN training loss 0.187812\n",
      ">> Epoch 34 finished \tANN training loss 0.187100\n",
      ">> Epoch 35 finished \tANN training loss 0.178047\n",
      ">> Epoch 36 finished \tANN training loss 0.184072\n",
      ">> Epoch 37 finished \tANN training loss 0.168970\n",
      ">> Epoch 38 finished \tANN training loss 0.164223\n",
      ">> Epoch 39 finished \tANN training loss 0.165991\n",
      ">> Epoch 40 finished \tANN training loss 0.160233\n",
      ">> Epoch 41 finished \tANN training loss 0.162822\n",
      ">> Epoch 42 finished \tANN training loss 0.147409\n",
      ">> Epoch 43 finished \tANN training loss 0.142507\n",
      ">> Epoch 44 finished \tANN training loss 0.169358\n",
      ">> Epoch 45 finished \tANN training loss 0.140223\n",
      ">> Epoch 46 finished \tANN training loss 0.134902\n",
      ">> Epoch 47 finished \tANN training loss 0.148939\n",
      ">> Epoch 48 finished \tANN training loss 0.132425\n",
      ">> Epoch 49 finished \tANN training loss 0.124328\n",
      ">> Epoch 50 finished \tANN training loss 0.134328\n",
      ">> Epoch 51 finished \tANN training loss 0.145181\n",
      ">> Epoch 52 finished \tANN training loss 0.142456\n",
      ">> Epoch 53 finished \tANN training loss 0.123136\n",
      ">> Epoch 54 finished \tANN training loss 0.103767\n",
      ">> Epoch 55 finished \tANN training loss 0.105037\n",
      ">> Epoch 56 finished \tANN training loss 0.125026\n",
      ">> Epoch 57 finished \tANN training loss 0.109964\n",
      ">> Epoch 58 finished \tANN training loss 0.112226\n",
      ">> Epoch 59 finished \tANN training loss 0.114073\n",
      ">> Epoch 60 finished \tANN training loss 0.108476\n",
      ">> Epoch 61 finished \tANN training loss 0.108882\n",
      ">> Epoch 62 finished \tANN training loss 0.112033\n",
      ">> Epoch 63 finished \tANN training loss 0.102020\n",
      ">> Epoch 64 finished \tANN training loss 0.095450\n",
      ">> Epoch 65 finished \tANN training loss 0.086708\n",
      ">> Epoch 66 finished \tANN training loss 0.115934\n",
      ">> Epoch 67 finished \tANN training loss 0.093641\n",
      ">> Epoch 68 finished \tANN training loss 0.092563\n",
      ">> Epoch 69 finished \tANN training loss 0.087390\n",
      ">> Epoch 70 finished \tANN training loss 0.089045\n",
      ">> Epoch 71 finished \tANN training loss 0.083293\n",
      ">> Epoch 72 finished \tANN training loss 0.093605\n",
      ">> Epoch 73 finished \tANN training loss 0.082785\n",
      ">> Epoch 74 finished \tANN training loss 0.078854\n",
      ">> Epoch 75 finished \tANN training loss 0.070868\n",
      ">> Epoch 76 finished \tANN training loss 0.082372\n",
      ">> Epoch 77 finished \tANN training loss 0.068621\n",
      ">> Epoch 78 finished \tANN training loss 0.085222\n",
      ">> Epoch 79 finished \tANN training loss 0.067818\n",
      ">> Epoch 80 finished \tANN training loss 0.080675\n",
      ">> Epoch 81 finished \tANN training loss 0.064650\n",
      ">> Epoch 82 finished \tANN training loss 0.065866\n",
      ">> Epoch 83 finished \tANN training loss 0.064773\n",
      ">> Epoch 84 finished \tANN training loss 0.051969\n",
      ">> Epoch 85 finished \tANN training loss 0.057554\n",
      ">> Epoch 86 finished \tANN training loss 0.063275\n",
      ">> Epoch 87 finished \tANN training loss 0.058796\n",
      ">> Epoch 88 finished \tANN training loss 0.057732\n",
      ">> Epoch 89 finished \tANN training loss 0.057557\n",
      ">> Epoch 90 finished \tANN training loss 0.054628\n",
      ">> Epoch 91 finished \tANN training loss 0.059110\n",
      ">> Epoch 92 finished \tANN training loss 0.048939\n",
      ">> Epoch 93 finished \tANN training loss 0.051332\n",
      ">> Epoch 94 finished \tANN training loss 0.049227\n",
      ">> Epoch 95 finished \tANN training loss 0.060603\n",
      ">> Epoch 96 finished \tANN training loss 0.053936\n",
      ">> Epoch 97 finished \tANN training loss 0.051115\n",
      ">> Epoch 98 finished \tANN training loss 0.069578\n",
      ">> Epoch 99 finished \tANN training loss 0.048432\n",
      ">> Epoch 100 finished \tANN training loss 0.052573\n",
      ">> Epoch 101 finished \tANN training loss 0.066428\n",
      ">> Epoch 102 finished \tANN training loss 0.076292\n",
      ">> Epoch 103 finished \tANN training loss 0.054587\n",
      ">> Epoch 104 finished \tANN training loss 0.052621\n",
      ">> Epoch 105 finished \tANN training loss 0.077950\n",
      ">> Epoch 106 finished \tANN training loss 0.041394\n",
      ">> Epoch 107 finished \tANN training loss 0.045692\n",
      ">> Epoch 108 finished \tANN training loss 0.057003\n",
      ">> Epoch 109 finished \tANN training loss 0.040073\n",
      ">> Epoch 110 finished \tANN training loss 0.040763\n",
      ">> Epoch 111 finished \tANN training loss 0.034363\n",
      ">> Epoch 112 finished \tANN training loss 0.038213\n",
      ">> Epoch 113 finished \tANN training loss 0.037568\n",
      ">> Epoch 114 finished \tANN training loss 0.039394\n",
      ">> Epoch 115 finished \tANN training loss 0.042704\n",
      ">> Epoch 116 finished \tANN training loss 0.043603\n",
      ">> Epoch 117 finished \tANN training loss 0.049939\n",
      ">> Epoch 118 finished \tANN training loss 0.038416\n",
      ">> Epoch 119 finished \tANN training loss 0.054399\n",
      ">> Epoch 120 finished \tANN training loss 0.045118\n",
      ">> Epoch 121 finished \tANN training loss 0.041104\n",
      ">> Epoch 122 finished \tANN training loss 0.032378\n",
      ">> Epoch 123 finished \tANN training loss 0.041771\n",
      ">> Epoch 124 finished \tANN training loss 0.042407\n",
      ">> Epoch 125 finished \tANN training loss 0.038866\n",
      ">> Epoch 126 finished \tANN training loss 0.044532\n",
      ">> Epoch 127 finished \tANN training loss 0.045526\n",
      ">> Epoch 128 finished \tANN training loss 0.043850\n",
      ">> Epoch 129 finished \tANN training loss 0.038436\n",
      ">> Epoch 130 finished \tANN training loss 0.037725\n",
      ">> Epoch 131 finished \tANN training loss 0.042814\n",
      ">> Epoch 132 finished \tANN training loss 0.069829\n",
      ">> Epoch 133 finished \tANN training loss 0.029721\n",
      ">> Epoch 134 finished \tANN training loss 0.036562\n",
      ">> Epoch 135 finished \tANN training loss 0.031726\n",
      ">> Epoch 136 finished \tANN training loss 0.047112\n",
      ">> Epoch 137 finished \tANN training loss 0.037560\n",
      ">> Epoch 138 finished \tANN training loss 0.036761\n",
      ">> Epoch 139 finished \tANN training loss 0.033706\n",
      ">> Epoch 140 finished \tANN training loss 0.031323\n",
      ">> Epoch 141 finished \tANN training loss 0.031168\n",
      ">> Epoch 142 finished \tANN training loss 0.026577\n",
      ">> Epoch 143 finished \tANN training loss 0.033781\n",
      ">> Epoch 144 finished \tANN training loss 0.033668\n",
      ">> Epoch 145 finished \tANN training loss 0.038855\n",
      ">> Epoch 146 finished \tANN training loss 0.033559\n",
      ">> Epoch 147 finished \tANN training loss 0.030089\n",
      ">> Epoch 148 finished \tANN training loss 0.030900\n",
      ">> Epoch 149 finished \tANN training loss 0.028202\n",
      ">> Epoch 150 finished \tANN training loss 0.022682\n",
      ">> Epoch 151 finished \tANN training loss 0.026570\n",
      ">> Epoch 152 finished \tANN training loss 0.028786\n",
      ">> Epoch 153 finished \tANN training loss 0.026531\n",
      ">> Epoch 154 finished \tANN training loss 0.049041\n",
      ">> Epoch 155 finished \tANN training loss 0.025253\n",
      ">> Epoch 156 finished \tANN training loss 0.026129\n",
      ">> Epoch 157 finished \tANN training loss 0.028095\n",
      ">> Epoch 158 finished \tANN training loss 0.025782\n",
      ">> Epoch 159 finished \tANN training loss 0.029814\n",
      ">> Epoch 160 finished \tANN training loss 0.024378\n",
      ">> Epoch 161 finished \tANN training loss 0.038138\n",
      ">> Epoch 162 finished \tANN training loss 0.025266\n",
      ">> Epoch 163 finished \tANN training loss 0.021990\n",
      ">> Epoch 164 finished \tANN training loss 0.057123\n",
      ">> Epoch 165 finished \tANN training loss 0.025590\n",
      ">> Epoch 166 finished \tANN training loss 0.022771\n",
      ">> Epoch 167 finished \tANN training loss 0.026044\n",
      ">> Epoch 168 finished \tANN training loss 0.025643\n",
      ">> Epoch 169 finished \tANN training loss 0.018370\n",
      ">> Epoch 170 finished \tANN training loss 0.016773\n",
      ">> Epoch 171 finished \tANN training loss 0.018521\n",
      ">> Epoch 172 finished \tANN training loss 0.024211\n",
      ">> Epoch 173 finished \tANN training loss 0.019276\n",
      ">> Epoch 174 finished \tANN training loss 0.027557\n",
      ">> Epoch 175 finished \tANN training loss 0.021357\n",
      ">> Epoch 176 finished \tANN training loss 0.030832\n",
      ">> Epoch 177 finished \tANN training loss 0.030068\n",
      ">> Epoch 178 finished \tANN training loss 0.040757\n",
      ">> Epoch 179 finished \tANN training loss 0.031675\n",
      ">> Epoch 180 finished \tANN training loss 0.032223\n",
      ">> Epoch 181 finished \tANN training loss 0.026681\n",
      ">> Epoch 182 finished \tANN training loss 0.022929\n",
      ">> Epoch 183 finished \tANN training loss 0.023724\n",
      ">> Epoch 184 finished \tANN training loss 0.014834\n",
      ">> Epoch 185 finished \tANN training loss 0.017355\n",
      ">> Epoch 186 finished \tANN training loss 0.015402\n",
      ">> Epoch 187 finished \tANN training loss 0.020469\n",
      ">> Epoch 188 finished \tANN training loss 0.018650\n",
      ">> Epoch 189 finished \tANN training loss 0.025324\n",
      ">> Epoch 190 finished \tANN training loss 0.018010\n",
      ">> Epoch 191 finished \tANN training loss 0.014694\n",
      ">> Epoch 192 finished \tANN training loss 0.017141\n",
      ">> Epoch 193 finished \tANN training loss 0.016848\n",
      ">> Epoch 194 finished \tANN training loss 0.031128\n",
      ">> Epoch 195 finished \tANN training loss 0.015375\n",
      ">> Epoch 196 finished \tANN training loss 0.017873\n",
      ">> Epoch 197 finished \tANN training loss 0.020873\n",
      ">> Epoch 198 finished \tANN training loss 0.021783\n",
      ">> Epoch 199 finished \tANN training loss 0.021329\n",
      ">> Epoch 200 finished \tANN training loss 0.020218\n",
      ">> Epoch 201 finished \tANN training loss 0.018381\n",
      ">> Epoch 202 finished \tANN training loss 0.018559\n",
      ">> Epoch 203 finished \tANN training loss 0.023197\n",
      ">> Epoch 204 finished \tANN training loss 0.039886\n",
      ">> Epoch 205 finished \tANN training loss 0.022315\n",
      ">> Epoch 206 finished \tANN training loss 0.021387\n",
      ">> Epoch 207 finished \tANN training loss 0.019141\n",
      ">> Epoch 208 finished \tANN training loss 0.019502\n",
      ">> Epoch 209 finished \tANN training loss 0.015233\n",
      ">> Epoch 210 finished \tANN training loss 0.013244\n",
      ">> Epoch 211 finished \tANN training loss 0.012371\n",
      ">> Epoch 212 finished \tANN training loss 0.015232\n",
      ">> Epoch 213 finished \tANN training loss 0.013937\n",
      ">> Epoch 214 finished \tANN training loss 0.021629\n",
      ">> Epoch 215 finished \tANN training loss 0.015076\n",
      ">> Epoch 216 finished \tANN training loss 0.014344\n",
      ">> Epoch 217 finished \tANN training loss 0.017760\n",
      ">> Epoch 218 finished \tANN training loss 0.015804\n",
      ">> Epoch 219 finished \tANN training loss 0.018880\n",
      ">> Epoch 220 finished \tANN training loss 0.018586\n",
      ">> Epoch 221 finished \tANN training loss 0.015456\n",
      ">> Epoch 222 finished \tANN training loss 0.020756\n",
      ">> Epoch 223 finished \tANN training loss 0.018877\n",
      ">> Epoch 224 finished \tANN training loss 0.015098\n",
      ">> Epoch 225 finished \tANN training loss 0.018581\n",
      ">> Epoch 226 finished \tANN training loss 0.016468\n",
      ">> Epoch 227 finished \tANN training loss 0.017617\n",
      ">> Epoch 228 finished \tANN training loss 0.015303\n",
      ">> Epoch 229 finished \tANN training loss 0.018537\n",
      ">> Epoch 230 finished \tANN training loss 0.022417\n",
      ">> Epoch 231 finished \tANN training loss 0.013767\n",
      ">> Epoch 232 finished \tANN training loss 0.013752\n",
      ">> Epoch 233 finished \tANN training loss 0.008947\n",
      ">> Epoch 234 finished \tANN training loss 0.021898\n",
      ">> Epoch 235 finished \tANN training loss 0.014752\n",
      ">> Epoch 236 finished \tANN training loss 0.014805\n",
      ">> Epoch 237 finished \tANN training loss 0.013339\n",
      ">> Epoch 238 finished \tANN training loss 0.029300\n",
      ">> Epoch 239 finished \tANN training loss 0.016166\n",
      ">> Epoch 240 finished \tANN training loss 0.015932\n",
      ">> Epoch 241 finished \tANN training loss 0.017432\n",
      ">> Epoch 242 finished \tANN training loss 0.023224\n",
      ">> Epoch 243 finished \tANN training loss 0.021326\n",
      ">> Epoch 244 finished \tANN training loss 0.016350\n",
      ">> Epoch 245 finished \tANN training loss 0.013514\n",
      ">> Epoch 246 finished \tANN training loss 0.013952\n",
      ">> Epoch 247 finished \tANN training loss 0.013032\n",
      ">> Epoch 248 finished \tANN training loss 0.014011\n",
      ">> Epoch 249 finished \tANN training loss 0.013910\n",
      ">> Epoch 250 finished \tANN training loss 0.013995\n",
      ">> Epoch 251 finished \tANN training loss 0.016056\n",
      ">> Epoch 252 finished \tANN training loss 0.014799\n",
      ">> Epoch 253 finished \tANN training loss 0.014517\n",
      ">> Epoch 254 finished \tANN training loss 0.015467\n",
      ">> Epoch 255 finished \tANN training loss 0.017584\n",
      ">> Epoch 256 finished \tANN training loss 0.020929\n",
      ">> Epoch 257 finished \tANN training loss 0.013808\n",
      ">> Epoch 258 finished \tANN training loss 0.012571\n",
      ">> Epoch 259 finished \tANN training loss 0.011825\n",
      ">> Epoch 260 finished \tANN training loss 0.016392\n",
      ">> Epoch 261 finished \tANN training loss 0.013145\n",
      ">> Epoch 262 finished \tANN training loss 0.016021\n",
      ">> Epoch 263 finished \tANN training loss 0.015804\n",
      ">> Epoch 264 finished \tANN training loss 0.016838\n",
      ">> Epoch 265 finished \tANN training loss 0.027031\n",
      ">> Epoch 266 finished \tANN training loss 0.014629\n",
      ">> Epoch 267 finished \tANN training loss 0.017376\n",
      ">> Epoch 268 finished \tANN training loss 0.015105\n",
      ">> Epoch 269 finished \tANN training loss 0.013251\n",
      ">> Epoch 270 finished \tANN training loss 0.009153\n",
      ">> Epoch 271 finished \tANN training loss 0.014583\n",
      ">> Epoch 272 finished \tANN training loss 0.019483\n",
      ">> Epoch 273 finished \tANN training loss 0.016580\n",
      ">> Epoch 274 finished \tANN training loss 0.017108\n",
      ">> Epoch 275 finished \tANN training loss 0.013999\n",
      ">> Epoch 276 finished \tANN training loss 0.015798\n",
      ">> Epoch 277 finished \tANN training loss 0.011499\n",
      ">> Epoch 278 finished \tANN training loss 0.019586\n",
      ">> Epoch 279 finished \tANN training loss 0.010525\n",
      ">> Epoch 280 finished \tANN training loss 0.011681\n",
      ">> Epoch 281 finished \tANN training loss 0.008488\n",
      ">> Epoch 282 finished \tANN training loss 0.011667\n",
      ">> Epoch 283 finished \tANN training loss 0.012425\n",
      ">> Epoch 284 finished \tANN training loss 0.011055\n",
      ">> Epoch 285 finished \tANN training loss 0.013199\n",
      ">> Epoch 286 finished \tANN training loss 0.012052\n",
      ">> Epoch 287 finished \tANN training loss 0.009756\n",
      ">> Epoch 288 finished \tANN training loss 0.013004\n",
      ">> Epoch 289 finished \tANN training loss 0.009499\n",
      ">> Epoch 290 finished \tANN training loss 0.010302\n",
      ">> Epoch 291 finished \tANN training loss 0.010009\n",
      ">> Epoch 292 finished \tANN training loss 0.010818\n",
      ">> Epoch 293 finished \tANN training loss 0.011537\n",
      ">> Epoch 294 finished \tANN training loss 0.012488\n",
      ">> Epoch 295 finished \tANN training loss 0.012657\n",
      ">> Epoch 296 finished \tANN training loss 0.010965\n",
      ">> Epoch 297 finished \tANN training loss 0.011492\n",
      ">> Epoch 298 finished \tANN training loss 0.012859\n",
      ">> Epoch 299 finished \tANN training loss 0.011936\n",
      ">> Epoch 300 finished \tANN training loss 0.013117\n",
      ">> Epoch 301 finished \tANN training loss 0.011145\n",
      ">> Epoch 302 finished \tANN training loss 0.008659\n",
      ">> Epoch 303 finished \tANN training loss 0.009060\n",
      ">> Epoch 304 finished \tANN training loss 0.008897\n",
      ">> Epoch 305 finished \tANN training loss 0.010666\n",
      ">> Epoch 306 finished \tANN training loss 0.012456\n",
      ">> Epoch 307 finished \tANN training loss 0.009178\n",
      ">> Epoch 308 finished \tANN training loss 0.010471\n",
      ">> Epoch 309 finished \tANN training loss 0.014469\n",
      ">> Epoch 310 finished \tANN training loss 0.015566\n",
      ">> Epoch 311 finished \tANN training loss 0.017039\n",
      ">> Epoch 312 finished \tANN training loss 0.011635\n",
      ">> Epoch 313 finished \tANN training loss 0.016152\n",
      ">> Epoch 314 finished \tANN training loss 0.023318\n",
      ">> Epoch 315 finished \tANN training loss 0.025073\n",
      ">> Epoch 316 finished \tANN training loss 0.014025\n",
      ">> Epoch 317 finished \tANN training loss 0.010165\n",
      ">> Epoch 318 finished \tANN training loss 0.011481\n",
      ">> Epoch 319 finished \tANN training loss 0.009091\n",
      ">> Epoch 320 finished \tANN training loss 0.009397\n",
      ">> Epoch 321 finished \tANN training loss 0.011007\n",
      ">> Epoch 322 finished \tANN training loss 0.012163\n",
      ">> Epoch 323 finished \tANN training loss 0.010858\n",
      ">> Epoch 324 finished \tANN training loss 0.009069\n",
      ">> Epoch 325 finished \tANN training loss 0.007928\n",
      ">> Epoch 326 finished \tANN training loss 0.007664\n",
      ">> Epoch 327 finished \tANN training loss 0.010616\n",
      ">> Epoch 328 finished \tANN training loss 0.015030\n",
      ">> Epoch 329 finished \tANN training loss 0.013670\n",
      ">> Epoch 330 finished \tANN training loss 0.011728\n",
      ">> Epoch 331 finished \tANN training loss 0.019708\n",
      ">> Epoch 332 finished \tANN training loss 0.012104\n",
      ">> Epoch 333 finished \tANN training loss 0.009675\n",
      ">> Epoch 334 finished \tANN training loss 0.010983\n",
      ">> Epoch 335 finished \tANN training loss 0.009528\n",
      ">> Epoch 336 finished \tANN training loss 0.008420\n",
      ">> Epoch 337 finished \tANN training loss 0.009997\n",
      ">> Epoch 338 finished \tANN training loss 0.010994\n",
      ">> Epoch 339 finished \tANN training loss 0.007965\n",
      ">> Epoch 340 finished \tANN training loss 0.011202\n",
      ">> Epoch 341 finished \tANN training loss 0.008899\n",
      ">> Epoch 342 finished \tANN training loss 0.008064\n",
      ">> Epoch 343 finished \tANN training loss 0.021396\n",
      ">> Epoch 344 finished \tANN training loss 0.010248\n",
      ">> Epoch 345 finished \tANN training loss 0.009048\n",
      ">> Epoch 346 finished \tANN training loss 0.009073\n",
      ">> Epoch 347 finished \tANN training loss 0.009974\n",
      ">> Epoch 348 finished \tANN training loss 0.010289\n",
      ">> Epoch 349 finished \tANN training loss 0.010935\n",
      ">> Epoch 350 finished \tANN training loss 0.010042\n",
      ">> Epoch 351 finished \tANN training loss 0.010683\n",
      ">> Epoch 352 finished \tANN training loss 0.010252\n",
      ">> Epoch 353 finished \tANN training loss 0.009033\n",
      ">> Epoch 354 finished \tANN training loss 0.008824\n",
      ">> Epoch 355 finished \tANN training loss 0.012956\n",
      ">> Epoch 356 finished \tANN training loss 0.007870\n",
      ">> Epoch 357 finished \tANN training loss 0.009136\n",
      ">> Epoch 358 finished \tANN training loss 0.008743\n",
      ">> Epoch 359 finished \tANN training loss 0.006674\n",
      ">> Epoch 360 finished \tANN training loss 0.006786\n",
      ">> Epoch 361 finished \tANN training loss 0.006774\n",
      ">> Epoch 362 finished \tANN training loss 0.006727\n",
      ">> Epoch 363 finished \tANN training loss 0.006377\n",
      ">> Epoch 364 finished \tANN training loss 0.007613\n",
      ">> Epoch 365 finished \tANN training loss 0.009268\n",
      ">> Epoch 366 finished \tANN training loss 0.006646\n",
      ">> Epoch 367 finished \tANN training loss 0.006242\n",
      ">> Epoch 368 finished \tANN training loss 0.005754\n",
      ">> Epoch 369 finished \tANN training loss 0.005492\n",
      ">> Epoch 370 finished \tANN training loss 0.008139\n",
      ">> Epoch 371 finished \tANN training loss 0.009956\n",
      ">> Epoch 372 finished \tANN training loss 0.007906\n",
      ">> Epoch 373 finished \tANN training loss 0.008457\n",
      ">> Epoch 374 finished \tANN training loss 0.008525\n",
      ">> Epoch 375 finished \tANN training loss 0.008179\n",
      ">> Epoch 376 finished \tANN training loss 0.010649\n",
      ">> Epoch 377 finished \tANN training loss 0.009953\n",
      ">> Epoch 378 finished \tANN training loss 0.009959\n",
      ">> Epoch 379 finished \tANN training loss 0.008801\n",
      ">> Epoch 380 finished \tANN training loss 0.010059\n",
      ">> Epoch 381 finished \tANN training loss 0.008770\n",
      ">> Epoch 382 finished \tANN training loss 0.007027\n",
      ">> Epoch 383 finished \tANN training loss 0.005864\n",
      ">> Epoch 384 finished \tANN training loss 0.007294\n",
      ">> Epoch 385 finished \tANN training loss 0.007098\n",
      ">> Epoch 386 finished \tANN training loss 0.009218\n",
      ">> Epoch 387 finished \tANN training loss 0.005688\n",
      ">> Epoch 388 finished \tANN training loss 0.007633\n",
      ">> Epoch 389 finished \tANN training loss 0.007140\n",
      ">> Epoch 390 finished \tANN training loss 0.007171\n",
      ">> Epoch 391 finished \tANN training loss 0.006597\n",
      ">> Epoch 392 finished \tANN training loss 0.005270\n",
      ">> Epoch 393 finished \tANN training loss 0.006895\n",
      ">> Epoch 394 finished \tANN training loss 0.007507\n",
      ">> Epoch 395 finished \tANN training loss 0.007263\n",
      ">> Epoch 396 finished \tANN training loss 0.007511\n",
      ">> Epoch 397 finished \tANN training loss 0.008254\n",
      ">> Epoch 398 finished \tANN training loss 0.013569\n",
      ">> Epoch 399 finished \tANN training loss 0.011276\n",
      ">> Epoch 400 finished \tANN training loss 0.009091\n",
      ">> Epoch 401 finished \tANN training loss 0.007775\n",
      ">> Epoch 402 finished \tANN training loss 0.009524\n",
      ">> Epoch 403 finished \tANN training loss 0.006874\n",
      ">> Epoch 404 finished \tANN training loss 0.009366\n",
      ">> Epoch 405 finished \tANN training loss 0.008631\n",
      ">> Epoch 406 finished \tANN training loss 0.006078\n",
      ">> Epoch 407 finished \tANN training loss 0.008551\n",
      ">> Epoch 408 finished \tANN training loss 0.009282\n",
      ">> Epoch 409 finished \tANN training loss 0.006819\n",
      ">> Epoch 410 finished \tANN training loss 0.007109\n",
      ">> Epoch 411 finished \tANN training loss 0.006077\n",
      ">> Epoch 412 finished \tANN training loss 0.006417\n",
      ">> Epoch 413 finished \tANN training loss 0.005021\n",
      ">> Epoch 414 finished \tANN training loss 0.007076\n",
      ">> Epoch 415 finished \tANN training loss 0.005518\n",
      ">> Epoch 416 finished \tANN training loss 0.007298\n",
      ">> Epoch 417 finished \tANN training loss 0.007571\n",
      ">> Epoch 418 finished \tANN training loss 0.005341\n",
      ">> Epoch 419 finished \tANN training loss 0.005300\n",
      ">> Epoch 420 finished \tANN training loss 0.004432\n",
      ">> Epoch 421 finished \tANN training loss 0.006599\n",
      ">> Epoch 422 finished \tANN training loss 0.009402\n",
      ">> Epoch 423 finished \tANN training loss 0.008621\n",
      ">> Epoch 424 finished \tANN training loss 0.006449\n",
      ">> Epoch 425 finished \tANN training loss 0.006104\n",
      ">> Epoch 426 finished \tANN training loss 0.007284\n",
      ">> Epoch 427 finished \tANN training loss 0.008832\n",
      ">> Epoch 428 finished \tANN training loss 0.007744\n",
      ">> Epoch 429 finished \tANN training loss 0.008470\n",
      ">> Epoch 430 finished \tANN training loss 0.006970\n",
      ">> Epoch 431 finished \tANN training loss 0.007960\n",
      ">> Epoch 432 finished \tANN training loss 0.007429\n",
      ">> Epoch 433 finished \tANN training loss 0.008229\n",
      ">> Epoch 434 finished \tANN training loss 0.008818\n",
      ">> Epoch 435 finished \tANN training loss 0.005633\n",
      ">> Epoch 436 finished \tANN training loss 0.008302\n",
      ">> Epoch 437 finished \tANN training loss 0.011033\n",
      ">> Epoch 438 finished \tANN training loss 0.008260\n",
      ">> Epoch 439 finished \tANN training loss 0.007613\n",
      ">> Epoch 440 finished \tANN training loss 0.007579\n",
      ">> Epoch 441 finished \tANN training loss 0.007946\n",
      ">> Epoch 442 finished \tANN training loss 0.009616\n",
      ">> Epoch 443 finished \tANN training loss 0.007289\n",
      ">> Epoch 444 finished \tANN training loss 0.006453\n",
      ">> Epoch 445 finished \tANN training loss 0.006523\n",
      ">> Epoch 446 finished \tANN training loss 0.007291\n",
      ">> Epoch 447 finished \tANN training loss 0.010780\n",
      ">> Epoch 448 finished \tANN training loss 0.006009\n",
      ">> Epoch 449 finished \tANN training loss 0.004810\n",
      ">> Epoch 450 finished \tANN training loss 0.005702\n",
      ">> Epoch 451 finished \tANN training loss 0.005149\n",
      ">> Epoch 452 finished \tANN training loss 0.004993\n",
      ">> Epoch 453 finished \tANN training loss 0.005326\n",
      ">> Epoch 454 finished \tANN training loss 0.006848\n",
      ">> Epoch 455 finished \tANN training loss 0.007250\n",
      ">> Epoch 456 finished \tANN training loss 0.006296\n",
      ">> Epoch 457 finished \tANN training loss 0.007438\n",
      ">> Epoch 458 finished \tANN training loss 0.006158\n",
      ">> Epoch 459 finished \tANN training loss 0.006055\n",
      ">> Epoch 460 finished \tANN training loss 0.007328\n",
      ">> Epoch 461 finished \tANN training loss 0.007866\n",
      ">> Epoch 462 finished \tANN training loss 0.007084\n",
      ">> Epoch 463 finished \tANN training loss 0.007521\n",
      ">> Epoch 464 finished \tANN training loss 0.006865\n",
      ">> Epoch 465 finished \tANN training loss 0.007813\n",
      ">> Epoch 466 finished \tANN training loss 0.007955\n",
      ">> Epoch 467 finished \tANN training loss 0.008720\n",
      ">> Epoch 468 finished \tANN training loss 0.007992\n",
      ">> Epoch 469 finished \tANN training loss 0.010339\n",
      ">> Epoch 470 finished \tANN training loss 0.007621\n",
      ">> Epoch 471 finished \tANN training loss 0.012399\n",
      ">> Epoch 472 finished \tANN training loss 0.010712\n",
      ">> Epoch 473 finished \tANN training loss 0.008686\n",
      ">> Epoch 474 finished \tANN training loss 0.006477\n",
      ">> Epoch 475 finished \tANN training loss 0.005940\n",
      ">> Epoch 476 finished \tANN training loss 0.005951\n",
      ">> Epoch 477 finished \tANN training loss 0.005416\n",
      ">> Epoch 478 finished \tANN training loss 0.005533\n",
      ">> Epoch 479 finished \tANN training loss 0.006814\n",
      ">> Epoch 480 finished \tANN training loss 0.005100\n",
      ">> Epoch 481 finished \tANN training loss 0.006382\n",
      ">> Epoch 482 finished \tANN training loss 0.006718\n",
      ">> Epoch 483 finished \tANN training loss 0.007171\n",
      ">> Epoch 484 finished \tANN training loss 0.008651\n",
      ">> Epoch 485 finished \tANN training loss 0.007640\n",
      ">> Epoch 486 finished \tANN training loss 0.007928\n",
      ">> Epoch 487 finished \tANN training loss 0.009329\n",
      ">> Epoch 488 finished \tANN training loss 0.007457\n",
      ">> Epoch 489 finished \tANN training loss 0.007154\n",
      ">> Epoch 490 finished \tANN training loss 0.007035\n",
      ">> Epoch 491 finished \tANN training loss 0.006923\n",
      ">> Epoch 492 finished \tANN training loss 0.006003\n",
      ">> Epoch 493 finished \tANN training loss 0.005548\n",
      ">> Epoch 494 finished \tANN training loss 0.004959\n",
      ">> Epoch 495 finished \tANN training loss 0.003956\n",
      ">> Epoch 496 finished \tANN training loss 0.007562\n",
      ">> Epoch 497 finished \tANN training loss 0.005861\n",
      ">> Epoch 498 finished \tANN training loss 0.005083\n",
      ">> Epoch 499 finished \tANN training loss 0.008056\n",
      ">> Epoch 500 finished \tANN training loss 0.005310\n",
      ">> Epoch 501 finished \tANN training loss 0.005335\n",
      ">> Epoch 502 finished \tANN training loss 0.004740\n",
      ">> Epoch 503 finished \tANN training loss 0.006335\n",
      ">> Epoch 504 finished \tANN training loss 0.004870\n",
      ">> Epoch 505 finished \tANN training loss 0.006270\n",
      ">> Epoch 506 finished \tANN training loss 0.010004\n",
      ">> Epoch 507 finished \tANN training loss 0.006365\n",
      ">> Epoch 508 finished \tANN training loss 0.007360\n",
      ">> Epoch 509 finished \tANN training loss 0.008509\n",
      ">> Epoch 510 finished \tANN training loss 0.006603\n",
      ">> Epoch 511 finished \tANN training loss 0.005350\n",
      ">> Epoch 512 finished \tANN training loss 0.006512\n",
      ">> Epoch 513 finished \tANN training loss 0.004695\n",
      ">> Epoch 514 finished \tANN training loss 0.006334\n",
      ">> Epoch 515 finished \tANN training loss 0.005278\n",
      ">> Epoch 516 finished \tANN training loss 0.007886\n",
      ">> Epoch 517 finished \tANN training loss 0.009802\n",
      ">> Epoch 518 finished \tANN training loss 0.006058\n",
      ">> Epoch 519 finished \tANN training loss 0.006025\n",
      ">> Epoch 520 finished \tANN training loss 0.004270\n",
      ">> Epoch 521 finished \tANN training loss 0.005562\n",
      ">> Epoch 522 finished \tANN training loss 0.005396\n",
      ">> Epoch 523 finished \tANN training loss 0.006639\n",
      ">> Epoch 524 finished \tANN training loss 0.007452\n",
      ">> Epoch 525 finished \tANN training loss 0.005805\n",
      ">> Epoch 526 finished \tANN training loss 0.004608\n",
      ">> Epoch 527 finished \tANN training loss 0.004990\n",
      ">> Epoch 528 finished \tANN training loss 0.004300\n",
      ">> Epoch 529 finished \tANN training loss 0.006406\n",
      ">> Epoch 530 finished \tANN training loss 0.005079\n",
      ">> Epoch 531 finished \tANN training loss 0.006587\n",
      ">> Epoch 532 finished \tANN training loss 0.008165\n",
      ">> Epoch 533 finished \tANN training loss 0.005510\n",
      ">> Epoch 534 finished \tANN training loss 0.005015\n",
      ">> Epoch 535 finished \tANN training loss 0.004035\n",
      ">> Epoch 536 finished \tANN training loss 0.005591\n",
      ">> Epoch 537 finished \tANN training loss 0.006383\n",
      ">> Epoch 538 finished \tANN training loss 0.004009\n",
      ">> Epoch 539 finished \tANN training loss 0.003866\n",
      ">> Epoch 540 finished \tANN training loss 0.005261\n",
      ">> Epoch 541 finished \tANN training loss 0.007747\n",
      ">> Epoch 542 finished \tANN training loss 0.006184\n",
      ">> Epoch 543 finished \tANN training loss 0.006238\n",
      ">> Epoch 544 finished \tANN training loss 0.010402\n",
      ">> Epoch 545 finished \tANN training loss 0.007222\n",
      ">> Epoch 546 finished \tANN training loss 0.004874\n",
      ">> Epoch 547 finished \tANN training loss 0.003389\n",
      ">> Epoch 548 finished \tANN training loss 0.004283\n",
      ">> Epoch 549 finished \tANN training loss 0.003849\n",
      ">> Epoch 550 finished \tANN training loss 0.003692\n",
      ">> Epoch 551 finished \tANN training loss 0.004291\n",
      ">> Epoch 552 finished \tANN training loss 0.003529\n",
      ">> Epoch 553 finished \tANN training loss 0.004124\n",
      ">> Epoch 554 finished \tANN training loss 0.004600\n",
      ">> Epoch 555 finished \tANN training loss 0.009179\n",
      ">> Epoch 556 finished \tANN training loss 0.004433\n",
      ">> Epoch 557 finished \tANN training loss 0.005323\n",
      ">> Epoch 558 finished \tANN training loss 0.004414\n",
      ">> Epoch 559 finished \tANN training loss 0.005166\n",
      ">> Epoch 560 finished \tANN training loss 0.005253\n",
      ">> Epoch 561 finished \tANN training loss 0.003698\n",
      ">> Epoch 562 finished \tANN training loss 0.009513\n",
      ">> Epoch 563 finished \tANN training loss 0.004626\n",
      ">> Epoch 564 finished \tANN training loss 0.005078\n",
      ">> Epoch 565 finished \tANN training loss 0.004772\n",
      ">> Epoch 566 finished \tANN training loss 0.005493\n",
      ">> Epoch 567 finished \tANN training loss 0.008931\n",
      ">> Epoch 568 finished \tANN training loss 0.004037\n",
      ">> Epoch 569 finished \tANN training loss 0.005152\n",
      ">> Epoch 570 finished \tANN training loss 0.003341\n",
      ">> Epoch 571 finished \tANN training loss 0.005952\n",
      ">> Epoch 572 finished \tANN training loss 0.003793\n",
      ">> Epoch 573 finished \tANN training loss 0.004977\n",
      ">> Epoch 574 finished \tANN training loss 0.005304\n",
      ">> Epoch 575 finished \tANN training loss 0.005782\n",
      ">> Epoch 576 finished \tANN training loss 0.012952\n",
      ">> Epoch 577 finished \tANN training loss 0.005132\n",
      ">> Epoch 578 finished \tANN training loss 0.003665\n",
      ">> Epoch 579 finished \tANN training loss 0.003118\n",
      ">> Epoch 580 finished \tANN training loss 0.004112\n",
      ">> Epoch 581 finished \tANN training loss 0.003990\n",
      ">> Epoch 582 finished \tANN training loss 0.003373\n",
      ">> Epoch 583 finished \tANN training loss 0.004049\n",
      ">> Epoch 584 finished \tANN training loss 0.003895\n",
      ">> Epoch 585 finished \tANN training loss 0.003134\n",
      ">> Epoch 586 finished \tANN training loss 0.003699\n",
      ">> Epoch 587 finished \tANN training loss 0.003725\n",
      ">> Epoch 588 finished \tANN training loss 0.004305\n",
      ">> Epoch 589 finished \tANN training loss 0.006774\n",
      ">> Epoch 590 finished \tANN training loss 0.004296\n",
      ">> Epoch 591 finished \tANN training loss 0.004671\n",
      ">> Epoch 592 finished \tANN training loss 0.005294\n",
      ">> Epoch 593 finished \tANN training loss 0.005349\n",
      ">> Epoch 594 finished \tANN training loss 0.005770\n",
      ">> Epoch 595 finished \tANN training loss 0.004777\n",
      ">> Epoch 596 finished \tANN training loss 0.008334\n",
      ">> Epoch 597 finished \tANN training loss 0.007684\n",
      ">> Epoch 598 finished \tANN training loss 0.007108\n",
      ">> Epoch 599 finished \tANN training loss 0.004309\n",
      ">> Epoch 600 finished \tANN training loss 0.004030\n",
      ">> Epoch 601 finished \tANN training loss 0.003340\n",
      ">> Epoch 602 finished \tANN training loss 0.003072\n",
      ">> Epoch 603 finished \tANN training loss 0.002865\n",
      ">> Epoch 604 finished \tANN training loss 0.002683\n",
      ">> Epoch 605 finished \tANN training loss 0.004709\n",
      ">> Epoch 606 finished \tANN training loss 0.004306\n",
      ">> Epoch 607 finished \tANN training loss 0.003719\n",
      ">> Epoch 608 finished \tANN training loss 0.004990\n",
      ">> Epoch 609 finished \tANN training loss 0.004099\n",
      ">> Epoch 610 finished \tANN training loss 0.005316\n",
      ">> Epoch 611 finished \tANN training loss 0.005536\n",
      ">> Epoch 612 finished \tANN training loss 0.004727\n",
      ">> Epoch 613 finished \tANN training loss 0.004435\n",
      ">> Epoch 614 finished \tANN training loss 0.003893\n",
      ">> Epoch 615 finished \tANN training loss 0.004384\n",
      ">> Epoch 616 finished \tANN training loss 0.004366\n",
      ">> Epoch 617 finished \tANN training loss 0.003977\n",
      ">> Epoch 618 finished \tANN training loss 0.004976\n",
      ">> Epoch 619 finished \tANN training loss 0.005186\n",
      ">> Epoch 620 finished \tANN training loss 0.005713\n",
      ">> Epoch 621 finished \tANN training loss 0.004282\n",
      ">> Epoch 622 finished \tANN training loss 0.005047\n",
      ">> Epoch 623 finished \tANN training loss 0.003965\n",
      ">> Epoch 624 finished \tANN training loss 0.003836\n",
      ">> Epoch 625 finished \tANN training loss 0.004875\n",
      ">> Epoch 626 finished \tANN training loss 0.004538\n",
      ">> Epoch 627 finished \tANN training loss 0.006431\n",
      ">> Epoch 628 finished \tANN training loss 0.005283\n",
      ">> Epoch 629 finished \tANN training loss 0.006410\n",
      ">> Epoch 630 finished \tANN training loss 0.004665\n",
      ">> Epoch 631 finished \tANN training loss 0.004044\n",
      ">> Epoch 632 finished \tANN training loss 0.006154\n",
      ">> Epoch 633 finished \tANN training loss 0.003317\n",
      ">> Epoch 634 finished \tANN training loss 0.004375\n",
      ">> Epoch 635 finished \tANN training loss 0.007543\n",
      ">> Epoch 636 finished \tANN training loss 0.004353\n",
      ">> Epoch 637 finished \tANN training loss 0.004298\n",
      ">> Epoch 638 finished \tANN training loss 0.004394\n",
      ">> Epoch 639 finished \tANN training loss 0.003659\n",
      ">> Epoch 640 finished \tANN training loss 0.005501\n",
      ">> Epoch 641 finished \tANN training loss 0.003993\n",
      ">> Epoch 642 finished \tANN training loss 0.003938\n",
      ">> Epoch 643 finished \tANN training loss 0.005223\n",
      ">> Epoch 644 finished \tANN training loss 0.005453\n",
      ">> Epoch 645 finished \tANN training loss 0.004874\n",
      ">> Epoch 646 finished \tANN training loss 0.005732\n",
      ">> Epoch 647 finished \tANN training loss 0.005416\n",
      ">> Epoch 648 finished \tANN training loss 0.006954\n",
      ">> Epoch 649 finished \tANN training loss 0.005533\n",
      ">> Epoch 650 finished \tANN training loss 0.003913\n",
      ">> Epoch 651 finished \tANN training loss 0.005206\n",
      ">> Epoch 652 finished \tANN training loss 0.006046\n",
      ">> Epoch 653 finished \tANN training loss 0.004225\n",
      ">> Epoch 654 finished \tANN training loss 0.005425\n",
      ">> Epoch 655 finished \tANN training loss 0.005380\n",
      ">> Epoch 656 finished \tANN training loss 0.005903\n",
      ">> Epoch 657 finished \tANN training loss 0.003994\n",
      ">> Epoch 658 finished \tANN training loss 0.003238\n",
      ">> Epoch 659 finished \tANN training loss 0.004021\n",
      ">> Epoch 660 finished \tANN training loss 0.004039\n",
      ">> Epoch 661 finished \tANN training loss 0.004299\n",
      ">> Epoch 662 finished \tANN training loss 0.004589\n",
      ">> Epoch 663 finished \tANN training loss 0.004687\n",
      ">> Epoch 664 finished \tANN training loss 0.004477\n",
      ">> Epoch 665 finished \tANN training loss 0.004421\n",
      ">> Epoch 666 finished \tANN training loss 0.007257\n",
      ">> Epoch 667 finished \tANN training loss 0.003858\n",
      ">> Epoch 668 finished \tANN training loss 0.005766\n",
      ">> Epoch 669 finished \tANN training loss 0.004657\n",
      ">> Epoch 670 finished \tANN training loss 0.005146\n",
      ">> Epoch 671 finished \tANN training loss 0.005118\n",
      ">> Epoch 672 finished \tANN training loss 0.004181\n",
      ">> Epoch 673 finished \tANN training loss 0.003570\n",
      ">> Epoch 674 finished \tANN training loss 0.004408\n",
      ">> Epoch 675 finished \tANN training loss 0.006024\n",
      ">> Epoch 676 finished \tANN training loss 0.005528\n",
      ">> Epoch 677 finished \tANN training loss 0.006540\n",
      ">> Epoch 678 finished \tANN training loss 0.005868\n",
      ">> Epoch 679 finished \tANN training loss 0.005004\n",
      ">> Epoch 680 finished \tANN training loss 0.006105\n",
      ">> Epoch 681 finished \tANN training loss 0.004641\n",
      ">> Epoch 682 finished \tANN training loss 0.004373\n",
      ">> Epoch 683 finished \tANN training loss 0.003148\n",
      ">> Epoch 684 finished \tANN training loss 0.002811\n",
      ">> Epoch 685 finished \tANN training loss 0.002792\n",
      ">> Epoch 686 finished \tANN training loss 0.003583\n",
      ">> Epoch 687 finished \tANN training loss 0.003231\n",
      ">> Epoch 688 finished \tANN training loss 0.003015\n",
      ">> Epoch 689 finished \tANN training loss 0.003351\n",
      ">> Epoch 690 finished \tANN training loss 0.003551\n",
      ">> Epoch 691 finished \tANN training loss 0.003267\n",
      ">> Epoch 692 finished \tANN training loss 0.003416\n",
      ">> Epoch 693 finished \tANN training loss 0.004105\n",
      ">> Epoch 694 finished \tANN training loss 0.003199\n",
      ">> Epoch 695 finished \tANN training loss 0.004581\n",
      ">> Epoch 696 finished \tANN training loss 0.003435\n",
      ">> Epoch 697 finished \tANN training loss 0.002854\n",
      ">> Epoch 698 finished \tANN training loss 0.002654\n",
      ">> Epoch 699 finished \tANN training loss 0.003760\n",
      ">> Epoch 700 finished \tANN training loss 0.003356\n",
      ">> Epoch 701 finished \tANN training loss 0.005945\n",
      ">> Epoch 702 finished \tANN training loss 0.003518\n",
      ">> Epoch 703 finished \tANN training loss 0.003812\n",
      ">> Epoch 704 finished \tANN training loss 0.002806\n",
      ">> Epoch 705 finished \tANN training loss 0.003056\n",
      ">> Epoch 706 finished \tANN training loss 0.003275\n",
      ">> Epoch 707 finished \tANN training loss 0.002680\n",
      ">> Epoch 708 finished \tANN training loss 0.002245\n",
      ">> Epoch 709 finished \tANN training loss 0.002400\n",
      ">> Epoch 710 finished \tANN training loss 0.002716\n",
      ">> Epoch 711 finished \tANN training loss 0.004074\n",
      ">> Epoch 712 finished \tANN training loss 0.004512\n",
      ">> Epoch 713 finished \tANN training loss 0.003699\n",
      ">> Epoch 714 finished \tANN training loss 0.003083\n",
      ">> Epoch 715 finished \tANN training loss 0.003429\n",
      ">> Epoch 716 finished \tANN training loss 0.003381\n",
      ">> Epoch 717 finished \tANN training loss 0.002846\n",
      ">> Epoch 718 finished \tANN training loss 0.003658\n",
      ">> Epoch 719 finished \tANN training loss 0.002969\n",
      ">> Epoch 720 finished \tANN training loss 0.003478\n",
      ">> Epoch 721 finished \tANN training loss 0.003309\n",
      ">> Epoch 722 finished \tANN training loss 0.004130\n",
      ">> Epoch 723 finished \tANN training loss 0.005536\n",
      ">> Epoch 724 finished \tANN training loss 0.004954\n",
      ">> Epoch 725 finished \tANN training loss 0.003988\n",
      ">> Epoch 726 finished \tANN training loss 0.004664\n",
      ">> Epoch 727 finished \tANN training loss 0.005789\n",
      ">> Epoch 728 finished \tANN training loss 0.005175\n",
      ">> Epoch 729 finished \tANN training loss 0.004089\n",
      ">> Epoch 730 finished \tANN training loss 0.005033\n",
      ">> Epoch 731 finished \tANN training loss 0.003926\n",
      ">> Epoch 732 finished \tANN training loss 0.004933\n",
      ">> Epoch 733 finished \tANN training loss 0.003652\n",
      ">> Epoch 734 finished \tANN training loss 0.003694\n",
      ">> Epoch 735 finished \tANN training loss 0.004738\n",
      ">> Epoch 736 finished \tANN training loss 0.004502\n",
      ">> Epoch 737 finished \tANN training loss 0.004035\n",
      ">> Epoch 738 finished \tANN training loss 0.003420\n",
      ">> Epoch 739 finished \tANN training loss 0.003048\n",
      ">> Epoch 740 finished \tANN training loss 0.002682\n",
      ">> Epoch 741 finished \tANN training loss 0.003454\n",
      ">> Epoch 742 finished \tANN training loss 0.004175\n",
      ">> Epoch 743 finished \tANN training loss 0.003969\n",
      ">> Epoch 744 finished \tANN training loss 0.005886\n",
      ">> Epoch 745 finished \tANN training loss 0.005443\n",
      ">> Epoch 746 finished \tANN training loss 0.004230\n",
      ">> Epoch 747 finished \tANN training loss 0.004024\n",
      ">> Epoch 748 finished \tANN training loss 0.004400\n",
      ">> Epoch 749 finished \tANN training loss 0.004121\n",
      ">> Epoch 750 finished \tANN training loss 0.004637\n",
      ">> Epoch 751 finished \tANN training loss 0.005315\n",
      ">> Epoch 752 finished \tANN training loss 0.005375\n",
      ">> Epoch 753 finished \tANN training loss 0.004983\n",
      ">> Epoch 754 finished \tANN training loss 0.003948\n",
      ">> Epoch 755 finished \tANN training loss 0.005677\n",
      ">> Epoch 756 finished \tANN training loss 0.004271\n",
      ">> Epoch 757 finished \tANN training loss 0.005537\n",
      ">> Epoch 758 finished \tANN training loss 0.005675\n",
      ">> Epoch 759 finished \tANN training loss 0.004500\n",
      ">> Epoch 760 finished \tANN training loss 0.004991\n",
      ">> Epoch 761 finished \tANN training loss 0.004410\n",
      ">> Epoch 762 finished \tANN training loss 0.003970\n",
      ">> Epoch 763 finished \tANN training loss 0.003973\n",
      ">> Epoch 764 finished \tANN training loss 0.003620\n",
      ">> Epoch 765 finished \tANN training loss 0.004556\n",
      ">> Epoch 766 finished \tANN training loss 0.004279\n",
      ">> Epoch 767 finished \tANN training loss 0.004160\n",
      ">> Epoch 768 finished \tANN training loss 0.003848\n",
      ">> Epoch 769 finished \tANN training loss 0.004605\n",
      ">> Epoch 770 finished \tANN training loss 0.004095\n",
      ">> Epoch 771 finished \tANN training loss 0.004300\n",
      ">> Epoch 772 finished \tANN training loss 0.003248\n",
      ">> Epoch 773 finished \tANN training loss 0.004312\n",
      ">> Epoch 774 finished \tANN training loss 0.003944\n",
      ">> Epoch 775 finished \tANN training loss 0.004532\n",
      ">> Epoch 776 finished \tANN training loss 0.003100\n",
      ">> Epoch 777 finished \tANN training loss 0.003318\n",
      ">> Epoch 778 finished \tANN training loss 0.002825\n",
      ">> Epoch 779 finished \tANN training loss 0.006721\n",
      ">> Epoch 780 finished \tANN training loss 0.005062\n",
      ">> Epoch 781 finished \tANN training loss 0.004860\n",
      ">> Epoch 782 finished \tANN training loss 0.004939\n",
      ">> Epoch 783 finished \tANN training loss 0.004997\n",
      ">> Epoch 784 finished \tANN training loss 0.006247\n",
      ">> Epoch 785 finished \tANN training loss 0.005480\n",
      ">> Epoch 786 finished \tANN training loss 0.003397\n",
      ">> Epoch 787 finished \tANN training loss 0.004110\n",
      ">> Epoch 788 finished \tANN training loss 0.004847\n",
      ">> Epoch 789 finished \tANN training loss 0.004732\n",
      ">> Epoch 790 finished \tANN training loss 0.004716\n",
      ">> Epoch 791 finished \tANN training loss 0.004225\n",
      ">> Epoch 792 finished \tANN training loss 0.004167\n",
      ">> Epoch 793 finished \tANN training loss 0.005751\n",
      ">> Epoch 794 finished \tANN training loss 0.004592\n",
      ">> Epoch 795 finished \tANN training loss 0.004621\n",
      ">> Epoch 796 finished \tANN training loss 0.004179\n",
      ">> Epoch 797 finished \tANN training loss 0.004042\n",
      ">> Epoch 798 finished \tANN training loss 0.003648\n",
      ">> Epoch 799 finished \tANN training loss 0.003409\n",
      ">> Epoch 800 finished \tANN training loss 0.003211\n",
      ">> Epoch 801 finished \tANN training loss 0.002746\n",
      ">> Epoch 802 finished \tANN training loss 0.003589\n",
      ">> Epoch 803 finished \tANN training loss 0.005159\n",
      ">> Epoch 804 finished \tANN training loss 0.003879\n",
      ">> Epoch 805 finished \tANN training loss 0.004308\n",
      ">> Epoch 806 finished \tANN training loss 0.007056\n",
      ">> Epoch 807 finished \tANN training loss 0.008034\n",
      ">> Epoch 808 finished \tANN training loss 0.003495\n",
      ">> Epoch 809 finished \tANN training loss 0.002960\n",
      ">> Epoch 810 finished \tANN training loss 0.003005\n",
      ">> Epoch 811 finished \tANN training loss 0.003074\n",
      ">> Epoch 812 finished \tANN training loss 0.002751\n",
      ">> Epoch 813 finished \tANN training loss 0.002573\n",
      ">> Epoch 814 finished \tANN training loss 0.003281\n",
      ">> Epoch 815 finished \tANN training loss 0.003356\n",
      ">> Epoch 816 finished \tANN training loss 0.002685\n",
      ">> Epoch 817 finished \tANN training loss 0.003601\n",
      ">> Epoch 818 finished \tANN training loss 0.003054\n",
      ">> Epoch 819 finished \tANN training loss 0.002862\n",
      ">> Epoch 820 finished \tANN training loss 0.003262\n",
      ">> Epoch 821 finished \tANN training loss 0.003139\n",
      ">> Epoch 822 finished \tANN training loss 0.004326\n",
      ">> Epoch 823 finished \tANN training loss 0.002307\n",
      ">> Epoch 824 finished \tANN training loss 0.002706\n",
      ">> Epoch 825 finished \tANN training loss 0.003140\n",
      ">> Epoch 826 finished \tANN training loss 0.003109\n",
      ">> Epoch 827 finished \tANN training loss 0.002840\n",
      ">> Epoch 828 finished \tANN training loss 0.002574\n",
      ">> Epoch 829 finished \tANN training loss 0.002263\n",
      ">> Epoch 830 finished \tANN training loss 0.003049\n",
      ">> Epoch 831 finished \tANN training loss 0.002723\n",
      ">> Epoch 832 finished \tANN training loss 0.003995\n",
      ">> Epoch 833 finished \tANN training loss 0.005019\n",
      ">> Epoch 834 finished \tANN training loss 0.003342\n",
      ">> Epoch 835 finished \tANN training loss 0.003274\n",
      ">> Epoch 836 finished \tANN training loss 0.003212\n",
      ">> Epoch 837 finished \tANN training loss 0.003479\n",
      ">> Epoch 838 finished \tANN training loss 0.003216\n",
      ">> Epoch 839 finished \tANN training loss 0.002863\n",
      ">> Epoch 840 finished \tANN training loss 0.003407\n",
      ">> Epoch 841 finished \tANN training loss 0.003907\n",
      ">> Epoch 842 finished \tANN training loss 0.004328\n",
      ">> Epoch 843 finished \tANN training loss 0.003465\n",
      ">> Epoch 844 finished \tANN training loss 0.006072\n",
      ">> Epoch 845 finished \tANN training loss 0.004063\n",
      ">> Epoch 846 finished \tANN training loss 0.002888\n",
      ">> Epoch 847 finished \tANN training loss 0.002587\n",
      ">> Epoch 848 finished \tANN training loss 0.003285\n",
      ">> Epoch 849 finished \tANN training loss 0.003137\n",
      ">> Epoch 850 finished \tANN training loss 0.003261\n",
      ">> Epoch 851 finished \tANN training loss 0.002913\n",
      ">> Epoch 852 finished \tANN training loss 0.002573\n",
      ">> Epoch 853 finished \tANN training loss 0.002363\n",
      ">> Epoch 854 finished \tANN training loss 0.002367\n",
      ">> Epoch 855 finished \tANN training loss 0.002427\n",
      ">> Epoch 856 finished \tANN training loss 0.002512\n",
      ">> Epoch 857 finished \tANN training loss 0.001986\n",
      ">> Epoch 858 finished \tANN training loss 0.001984\n",
      ">> Epoch 859 finished \tANN training loss 0.002605\n",
      ">> Epoch 860 finished \tANN training loss 0.003025\n",
      ">> Epoch 861 finished \tANN training loss 0.002661\n",
      ">> Epoch 862 finished \tANN training loss 0.002897\n",
      ">> Epoch 863 finished \tANN training loss 0.003252\n",
      ">> Epoch 864 finished \tANN training loss 0.003710\n",
      ">> Epoch 865 finished \tANN training loss 0.003674\n",
      ">> Epoch 866 finished \tANN training loss 0.002534\n",
      ">> Epoch 867 finished \tANN training loss 0.002726\n",
      ">> Epoch 868 finished \tANN training loss 0.003133\n",
      ">> Epoch 869 finished \tANN training loss 0.003154\n",
      ">> Epoch 870 finished \tANN training loss 0.003211\n",
      ">> Epoch 871 finished \tANN training loss 0.003459\n",
      ">> Epoch 872 finished \tANN training loss 0.002917\n",
      ">> Epoch 873 finished \tANN training loss 0.003444\n",
      ">> Epoch 874 finished \tANN training loss 0.005269\n",
      ">> Epoch 875 finished \tANN training loss 0.004711\n",
      ">> Epoch 876 finished \tANN training loss 0.002952\n",
      ">> Epoch 877 finished \tANN training loss 0.003233\n",
      ">> Epoch 878 finished \tANN training loss 0.003760\n",
      ">> Epoch 879 finished \tANN training loss 0.003601\n",
      ">> Epoch 880 finished \tANN training loss 0.003718\n",
      ">> Epoch 881 finished \tANN training loss 0.004853\n",
      ">> Epoch 882 finished \tANN training loss 0.003410\n",
      ">> Epoch 883 finished \tANN training loss 0.003764\n",
      ">> Epoch 884 finished \tANN training loss 0.003312\n",
      ">> Epoch 885 finished \tANN training loss 0.004121\n",
      ">> Epoch 886 finished \tANN training loss 0.003710\n",
      ">> Epoch 887 finished \tANN training loss 0.003258\n",
      ">> Epoch 888 finished \tANN training loss 0.004018\n",
      ">> Epoch 889 finished \tANN training loss 0.004106\n",
      ">> Epoch 890 finished \tANN training loss 0.003281\n",
      ">> Epoch 891 finished \tANN training loss 0.003142\n",
      ">> Epoch 892 finished \tANN training loss 0.002485\n",
      ">> Epoch 893 finished \tANN training loss 0.002702\n",
      ">> Epoch 894 finished \tANN training loss 0.004309\n",
      ">> Epoch 895 finished \tANN training loss 0.003631\n",
      ">> Epoch 896 finished \tANN training loss 0.002815\n",
      ">> Epoch 897 finished \tANN training loss 0.002083\n",
      ">> Epoch 898 finished \tANN training loss 0.001995\n",
      ">> Epoch 899 finished \tANN training loss 0.002530\n",
      ">> Epoch 900 finished \tANN training loss 0.002122\n",
      ">> Epoch 901 finished \tANN training loss 0.002295\n",
      ">> Epoch 902 finished \tANN training loss 0.002551\n",
      ">> Epoch 903 finished \tANN training loss 0.002467\n",
      ">> Epoch 904 finished \tANN training loss 0.002085\n",
      ">> Epoch 905 finished \tANN training loss 0.003091\n",
      ">> Epoch 906 finished \tANN training loss 0.003606\n",
      ">> Epoch 907 finished \tANN training loss 0.003218\n",
      ">> Epoch 908 finished \tANN training loss 0.003594\n",
      ">> Epoch 909 finished \tANN training loss 0.002750\n",
      ">> Epoch 910 finished \tANN training loss 0.003957\n",
      ">> Epoch 911 finished \tANN training loss 0.004193\n",
      ">> Epoch 912 finished \tANN training loss 0.004056\n",
      ">> Epoch 913 finished \tANN training loss 0.003391\n",
      ">> Epoch 914 finished \tANN training loss 0.003390\n",
      ">> Epoch 915 finished \tANN training loss 0.003293\n",
      ">> Epoch 916 finished \tANN training loss 0.005088\n",
      ">> Epoch 917 finished \tANN training loss 0.003096\n",
      ">> Epoch 918 finished \tANN training loss 0.002627\n",
      ">> Epoch 919 finished \tANN training loss 0.002540\n",
      ">> Epoch 920 finished \tANN training loss 0.003107\n",
      ">> Epoch 921 finished \tANN training loss 0.003322\n",
      ">> Epoch 922 finished \tANN training loss 0.003283\n",
      ">> Epoch 923 finished \tANN training loss 0.003212\n",
      ">> Epoch 924 finished \tANN training loss 0.003624\n",
      ">> Epoch 925 finished \tANN training loss 0.003580\n",
      ">> Epoch 926 finished \tANN training loss 0.004835\n",
      ">> Epoch 927 finished \tANN training loss 0.005107\n",
      ">> Epoch 928 finished \tANN training loss 0.004321\n",
      ">> Epoch 929 finished \tANN training loss 0.003959\n",
      ">> Epoch 930 finished \tANN training loss 0.003824\n",
      ">> Epoch 931 finished \tANN training loss 0.004461\n",
      ">> Epoch 932 finished \tANN training loss 0.003984\n",
      ">> Epoch 933 finished \tANN training loss 0.003849\n",
      ">> Epoch 934 finished \tANN training loss 0.003426\n",
      ">> Epoch 935 finished \tANN training loss 0.003744\n",
      ">> Epoch 936 finished \tANN training loss 0.003589\n",
      ">> Epoch 937 finished \tANN training loss 0.004750\n",
      ">> Epoch 938 finished \tANN training loss 0.004957\n",
      ">> Epoch 939 finished \tANN training loss 0.004146\n",
      ">> Epoch 940 finished \tANN training loss 0.004514\n",
      ">> Epoch 941 finished \tANN training loss 0.004121\n",
      ">> Epoch 942 finished \tANN training loss 0.003389\n",
      ">> Epoch 943 finished \tANN training loss 0.003154\n",
      ">> Epoch 944 finished \tANN training loss 0.003850\n",
      ">> Epoch 945 finished \tANN training loss 0.004612\n",
      ">> Epoch 946 finished \tANN training loss 0.003656\n",
      ">> Epoch 947 finished \tANN training loss 0.003953\n",
      ">> Epoch 948 finished \tANN training loss 0.004191\n",
      ">> Epoch 949 finished \tANN training loss 0.003420\n",
      ">> Epoch 950 finished \tANN training loss 0.004547\n",
      ">> Epoch 951 finished \tANN training loss 0.003709\n",
      ">> Epoch 952 finished \tANN training loss 0.003883\n",
      ">> Epoch 953 finished \tANN training loss 0.003720\n",
      ">> Epoch 954 finished \tANN training loss 0.002635\n",
      ">> Epoch 955 finished \tANN training loss 0.002245\n",
      ">> Epoch 956 finished \tANN training loss 0.002307\n",
      ">> Epoch 957 finished \tANN training loss 0.002745\n",
      ">> Epoch 958 finished \tANN training loss 0.003335\n",
      ">> Epoch 959 finished \tANN training loss 0.003638\n",
      ">> Epoch 960 finished \tANN training loss 0.005564\n",
      ">> Epoch 961 finished \tANN training loss 0.004918\n",
      ">> Epoch 962 finished \tANN training loss 0.004759\n",
      ">> Epoch 963 finished \tANN training loss 0.004085\n",
      ">> Epoch 964 finished \tANN training loss 0.004568\n",
      ">> Epoch 965 finished \tANN training loss 0.003957\n",
      ">> Epoch 966 finished \tANN training loss 0.003360\n",
      ">> Epoch 967 finished \tANN training loss 0.003057\n",
      ">> Epoch 968 finished \tANN training loss 0.002620\n",
      ">> Epoch 969 finished \tANN training loss 0.002585\n",
      ">> Epoch 970 finished \tANN training loss 0.002661\n",
      ">> Epoch 971 finished \tANN training loss 0.002180\n",
      ">> Epoch 972 finished \tANN training loss 0.001739\n",
      ">> Epoch 973 finished \tANN training loss 0.002914\n",
      ">> Epoch 974 finished \tANN training loss 0.003300\n",
      ">> Epoch 975 finished \tANN training loss 0.002507\n",
      ">> Epoch 976 finished \tANN training loss 0.002425\n",
      ">> Epoch 977 finished \tANN training loss 0.003000\n",
      ">> Epoch 978 finished \tANN training loss 0.003833\n",
      ">> Epoch 979 finished \tANN training loss 0.003993\n",
      ">> Epoch 980 finished \tANN training loss 0.004678\n",
      ">> Epoch 981 finished \tANN training loss 0.004051\n",
      ">> Epoch 982 finished \tANN training loss 0.003573\n",
      ">> Epoch 983 finished \tANN training loss 0.003327\n",
      ">> Epoch 984 finished \tANN training loss 0.003321\n",
      ">> Epoch 985 finished \tANN training loss 0.003145\n",
      ">> Epoch 986 finished \tANN training loss 0.002781\n",
      ">> Epoch 987 finished \tANN training loss 0.003275\n",
      ">> Epoch 988 finished \tANN training loss 0.003913\n",
      ">> Epoch 989 finished \tANN training loss 0.002921\n",
      ">> Epoch 990 finished \tANN training loss 0.003104\n",
      ">> Epoch 991 finished \tANN training loss 0.003898\n",
      ">> Epoch 992 finished \tANN training loss 0.003131\n",
      ">> Epoch 993 finished \tANN training loss 0.003103\n",
      ">> Epoch 994 finished \tANN training loss 0.002999\n",
      ">> Epoch 995 finished \tANN training loss 0.002577\n",
      ">> Epoch 996 finished \tANN training loss 0.002702\n",
      ">> Epoch 997 finished \tANN training loss 0.002419\n",
      ">> Epoch 998 finished \tANN training loss 0.003253\n",
      ">> Epoch 999 finished \tANN training loss 0.002866\n",
      "[END] Fine tuning step\n",
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 7.944475\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 7.857605\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 7.547908\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 7.177561\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 6.511585\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 5.751936\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 5.227886\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 4.590822\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 4.256170\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 3.947730\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 3.746526\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 3.439982\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 3.290167\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 3.101957\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 2.980930\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 2.831870\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 2.718866\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 2.624883\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 2.536133\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 2.403811\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 5.652259\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 5.452790\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 4.847142\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 3.948993\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 3.011773\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 2.358094\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 1.899939\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 1.621132\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 1.438895\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 1.335482\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 1.146445\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 1.077645\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 0.974865\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 0.906433\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 0.842523\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 0.853187\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 0.821866\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 0.750990\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 0.742963\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 0.653048\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.881945\n",
      ">> Epoch 1 finished \tANN training loss 0.754254\n",
      ">> Epoch 2 finished \tANN training loss 0.659843\n",
      ">> Epoch 3 finished \tANN training loss 0.596020\n",
      ">> Epoch 4 finished \tANN training loss 0.531669\n",
      ">> Epoch 5 finished \tANN training loss 0.506284\n",
      ">> Epoch 6 finished \tANN training loss 0.452294\n",
      ">> Epoch 7 finished \tANN training loss 0.431938\n",
      ">> Epoch 8 finished \tANN training loss 0.412363\n",
      ">> Epoch 9 finished \tANN training loss 0.383039\n",
      ">> Epoch 10 finished \tANN training loss 0.368751\n",
      ">> Epoch 11 finished \tANN training loss 0.375586\n",
      ">> Epoch 12 finished \tANN training loss 0.336303\n",
      ">> Epoch 13 finished \tANN training loss 0.345099\n",
      ">> Epoch 14 finished \tANN training loss 0.386765\n",
      ">> Epoch 15 finished \tANN training loss 0.297651\n",
      ">> Epoch 16 finished \tANN training loss 0.290563\n",
      ">> Epoch 17 finished \tANN training loss 0.288094\n",
      ">> Epoch 18 finished \tANN training loss 0.265455\n",
      ">> Epoch 19 finished \tANN training loss 0.265259\n",
      ">> Epoch 20 finished \tANN training loss 0.255048\n",
      ">> Epoch 21 finished \tANN training loss 0.237002\n",
      ">> Epoch 22 finished \tANN training loss 0.237771\n",
      ">> Epoch 23 finished \tANN training loss 0.248485\n",
      ">> Epoch 24 finished \tANN training loss 0.220926\n",
      ">> Epoch 25 finished \tANN training loss 0.214037\n",
      ">> Epoch 26 finished \tANN training loss 0.210951\n",
      ">> Epoch 27 finished \tANN training loss 0.195874\n",
      ">> Epoch 28 finished \tANN training loss 0.195564\n",
      ">> Epoch 29 finished \tANN training loss 0.236347\n",
      ">> Epoch 30 finished \tANN training loss 0.205494\n",
      ">> Epoch 31 finished \tANN training loss 0.190714\n",
      ">> Epoch 32 finished \tANN training loss 0.180969\n",
      ">> Epoch 33 finished \tANN training loss 0.185899\n",
      ">> Epoch 34 finished \tANN training loss 0.166388\n",
      ">> Epoch 35 finished \tANN training loss 0.160108\n",
      ">> Epoch 36 finished \tANN training loss 0.159896\n",
      ">> Epoch 37 finished \tANN training loss 0.180715\n",
      ">> Epoch 38 finished \tANN training loss 0.163131\n",
      ">> Epoch 39 finished \tANN training loss 0.161645\n",
      ">> Epoch 40 finished \tANN training loss 0.153754\n",
      ">> Epoch 41 finished \tANN training loss 0.143864\n",
      ">> Epoch 42 finished \tANN training loss 0.139715\n",
      ">> Epoch 43 finished \tANN training loss 0.137655\n",
      ">> Epoch 44 finished \tANN training loss 0.128553\n",
      ">> Epoch 45 finished \tANN training loss 0.140480\n",
      ">> Epoch 46 finished \tANN training loss 0.143472\n",
      ">> Epoch 47 finished \tANN training loss 0.143729\n",
      ">> Epoch 48 finished \tANN training loss 0.131921\n",
      ">> Epoch 49 finished \tANN training loss 0.129317\n",
      ">> Epoch 50 finished \tANN training loss 0.126321\n",
      ">> Epoch 51 finished \tANN training loss 0.120241\n",
      ">> Epoch 52 finished \tANN training loss 0.110136\n",
      ">> Epoch 53 finished \tANN training loss 0.103795\n",
      ">> Epoch 54 finished \tANN training loss 0.109439\n",
      ">> Epoch 55 finished \tANN training loss 0.109716\n",
      ">> Epoch 56 finished \tANN training loss 0.113759\n",
      ">> Epoch 57 finished \tANN training loss 0.104325\n",
      ">> Epoch 58 finished \tANN training loss 0.110654\n",
      ">> Epoch 59 finished \tANN training loss 0.113870\n",
      ">> Epoch 60 finished \tANN training loss 0.091530\n",
      ">> Epoch 61 finished \tANN training loss 0.104312\n",
      ">> Epoch 62 finished \tANN training loss 0.103451\n",
      ">> Epoch 63 finished \tANN training loss 0.112218\n",
      ">> Epoch 64 finished \tANN training loss 0.090799\n",
      ">> Epoch 65 finished \tANN training loss 0.104744\n",
      ">> Epoch 66 finished \tANN training loss 0.102302\n",
      ">> Epoch 67 finished \tANN training loss 0.088615\n",
      ">> Epoch 68 finished \tANN training loss 0.097906\n",
      ">> Epoch 69 finished \tANN training loss 0.110031\n",
      ">> Epoch 70 finished \tANN training loss 0.091372\n",
      ">> Epoch 71 finished \tANN training loss 0.091838\n",
      ">> Epoch 72 finished \tANN training loss 0.106284\n",
      ">> Epoch 73 finished \tANN training loss 0.087463\n",
      ">> Epoch 74 finished \tANN training loss 0.106613\n",
      ">> Epoch 75 finished \tANN training loss 0.077910\n",
      ">> Epoch 76 finished \tANN training loss 0.079139\n",
      ">> Epoch 77 finished \tANN training loss 0.075892\n",
      ">> Epoch 78 finished \tANN training loss 0.073829\n",
      ">> Epoch 79 finished \tANN training loss 0.083169\n",
      ">> Epoch 80 finished \tANN training loss 0.071596\n",
      ">> Epoch 81 finished \tANN training loss 0.077459\n",
      ">> Epoch 82 finished \tANN training loss 0.071683\n",
      ">> Epoch 83 finished \tANN training loss 0.062103\n",
      ">> Epoch 84 finished \tANN training loss 0.065349\n",
      ">> Epoch 85 finished \tANN training loss 0.061421\n",
      ">> Epoch 86 finished \tANN training loss 0.067056\n",
      ">> Epoch 87 finished \tANN training loss 0.060793\n",
      ">> Epoch 88 finished \tANN training loss 0.078126\n",
      ">> Epoch 89 finished \tANN training loss 0.065309\n",
      ">> Epoch 90 finished \tANN training loss 0.074377\n",
      ">> Epoch 91 finished \tANN training loss 0.064417\n",
      ">> Epoch 92 finished \tANN training loss 0.086651\n",
      ">> Epoch 93 finished \tANN training loss 0.069160\n",
      ">> Epoch 94 finished \tANN training loss 0.069107\n",
      ">> Epoch 95 finished \tANN training loss 0.067301\n",
      ">> Epoch 96 finished \tANN training loss 0.060792\n",
      ">> Epoch 97 finished \tANN training loss 0.054309\n",
      ">> Epoch 98 finished \tANN training loss 0.055686\n",
      ">> Epoch 99 finished \tANN training loss 0.048763\n",
      ">> Epoch 100 finished \tANN training loss 0.050552\n",
      ">> Epoch 101 finished \tANN training loss 0.065732\n",
      ">> Epoch 102 finished \tANN training loss 0.048052\n",
      ">> Epoch 103 finished \tANN training loss 0.053617\n",
      ">> Epoch 104 finished \tANN training loss 0.038734\n",
      ">> Epoch 105 finished \tANN training loss 0.050322\n",
      ">> Epoch 106 finished \tANN training loss 0.047239\n",
      ">> Epoch 107 finished \tANN training loss 0.040397\n",
      ">> Epoch 108 finished \tANN training loss 0.062814\n",
      ">> Epoch 109 finished \tANN training loss 0.040470\n",
      ">> Epoch 110 finished \tANN training loss 0.046958\n",
      ">> Epoch 111 finished \tANN training loss 0.041214\n",
      ">> Epoch 112 finished \tANN training loss 0.043737\n",
      ">> Epoch 113 finished \tANN training loss 0.044669\n",
      ">> Epoch 114 finished \tANN training loss 0.045242\n",
      ">> Epoch 115 finished \tANN training loss 0.060244\n",
      ">> Epoch 116 finished \tANN training loss 0.056155\n",
      ">> Epoch 117 finished \tANN training loss 0.041697\n",
      ">> Epoch 118 finished \tANN training loss 0.032795\n",
      ">> Epoch 119 finished \tANN training loss 0.049939\n",
      ">> Epoch 120 finished \tANN training loss 0.041737\n",
      ">> Epoch 121 finished \tANN training loss 0.042536\n",
      ">> Epoch 122 finished \tANN training loss 0.038318\n",
      ">> Epoch 123 finished \tANN training loss 0.036088\n",
      ">> Epoch 124 finished \tANN training loss 0.049459\n",
      ">> Epoch 125 finished \tANN training loss 0.037206\n",
      ">> Epoch 126 finished \tANN training loss 0.038286\n",
      ">> Epoch 127 finished \tANN training loss 0.040115\n",
      ">> Epoch 128 finished \tANN training loss 0.050491\n",
      ">> Epoch 129 finished \tANN training loss 0.036075\n",
      ">> Epoch 130 finished \tANN training loss 0.039696\n",
      ">> Epoch 131 finished \tANN training loss 0.060689\n",
      ">> Epoch 132 finished \tANN training loss 0.045467\n",
      ">> Epoch 133 finished \tANN training loss 0.035860\n",
      ">> Epoch 134 finished \tANN training loss 0.056006\n",
      ">> Epoch 135 finished \tANN training loss 0.042581\n",
      ">> Epoch 136 finished \tANN training loss 0.036775\n",
      ">> Epoch 137 finished \tANN training loss 0.039895\n",
      ">> Epoch 138 finished \tANN training loss 0.034463\n",
      ">> Epoch 139 finished \tANN training loss 0.034086\n",
      ">> Epoch 140 finished \tANN training loss 0.028494\n",
      ">> Epoch 141 finished \tANN training loss 0.031069\n",
      ">> Epoch 142 finished \tANN training loss 0.039292\n",
      ">> Epoch 143 finished \tANN training loss 0.042201\n",
      ">> Epoch 144 finished \tANN training loss 0.038707\n",
      ">> Epoch 145 finished \tANN training loss 0.035338\n",
      ">> Epoch 146 finished \tANN training loss 0.046667\n",
      ">> Epoch 147 finished \tANN training loss 0.032969\n",
      ">> Epoch 148 finished \tANN training loss 0.059899\n",
      ">> Epoch 149 finished \tANN training loss 0.026023\n",
      ">> Epoch 150 finished \tANN training loss 0.036204\n",
      ">> Epoch 151 finished \tANN training loss 0.038414\n",
      ">> Epoch 152 finished \tANN training loss 0.030598\n",
      ">> Epoch 153 finished \tANN training loss 0.040523\n",
      ">> Epoch 154 finished \tANN training loss 0.039270\n",
      ">> Epoch 155 finished \tANN training loss 0.035733\n",
      ">> Epoch 156 finished \tANN training loss 0.028260\n",
      ">> Epoch 157 finished \tANN training loss 0.042274\n",
      ">> Epoch 158 finished \tANN training loss 0.026941\n",
      ">> Epoch 159 finished \tANN training loss 0.040653\n",
      ">> Epoch 160 finished \tANN training loss 0.035605\n",
      ">> Epoch 161 finished \tANN training loss 0.026782\n",
      ">> Epoch 162 finished \tANN training loss 0.029306\n",
      ">> Epoch 163 finished \tANN training loss 0.027865\n",
      ">> Epoch 164 finished \tANN training loss 0.034218\n",
      ">> Epoch 165 finished \tANN training loss 0.024539\n",
      ">> Epoch 166 finished \tANN training loss 0.032416\n",
      ">> Epoch 167 finished \tANN training loss 0.023102\n",
      ">> Epoch 168 finished \tANN training loss 0.017587\n",
      ">> Epoch 169 finished \tANN training loss 0.025453\n",
      ">> Epoch 170 finished \tANN training loss 0.021003\n",
      ">> Epoch 171 finished \tANN training loss 0.021674\n",
      ">> Epoch 172 finished \tANN training loss 0.027348\n",
      ">> Epoch 173 finished \tANN training loss 0.026290\n",
      ">> Epoch 174 finished \tANN training loss 0.017601\n",
      ">> Epoch 175 finished \tANN training loss 0.023888\n",
      ">> Epoch 176 finished \tANN training loss 0.028817\n",
      ">> Epoch 177 finished \tANN training loss 0.025501\n",
      ">> Epoch 178 finished \tANN training loss 0.023047\n",
      ">> Epoch 179 finished \tANN training loss 0.025914\n",
      ">> Epoch 180 finished \tANN training loss 0.026461\n",
      ">> Epoch 181 finished \tANN training loss 0.020885\n",
      ">> Epoch 182 finished \tANN training loss 0.041990\n",
      ">> Epoch 183 finished \tANN training loss 0.018051\n",
      ">> Epoch 184 finished \tANN training loss 0.015948\n",
      ">> Epoch 185 finished \tANN training loss 0.029631\n",
      ">> Epoch 186 finished \tANN training loss 0.031064\n",
      ">> Epoch 187 finished \tANN training loss 0.021467\n",
      ">> Epoch 188 finished \tANN training loss 0.020823\n",
      ">> Epoch 189 finished \tANN training loss 0.021866\n",
      ">> Epoch 190 finished \tANN training loss 0.017965\n",
      ">> Epoch 191 finished \tANN training loss 0.019343\n",
      ">> Epoch 192 finished \tANN training loss 0.020635\n",
      ">> Epoch 193 finished \tANN training loss 0.013581\n",
      ">> Epoch 194 finished \tANN training loss 0.016458\n",
      ">> Epoch 195 finished \tANN training loss 0.020003\n",
      ">> Epoch 196 finished \tANN training loss 0.018616\n",
      ">> Epoch 197 finished \tANN training loss 0.020167\n",
      ">> Epoch 198 finished \tANN training loss 0.022305\n",
      ">> Epoch 199 finished \tANN training loss 0.019224\n",
      ">> Epoch 200 finished \tANN training loss 0.019026\n",
      ">> Epoch 201 finished \tANN training loss 0.021842\n",
      ">> Epoch 202 finished \tANN training loss 0.020681\n",
      ">> Epoch 203 finished \tANN training loss 0.015494\n",
      ">> Epoch 204 finished \tANN training loss 0.014567\n",
      ">> Epoch 205 finished \tANN training loss 0.019428\n",
      ">> Epoch 206 finished \tANN training loss 0.024872\n",
      ">> Epoch 207 finished \tANN training loss 0.019990\n",
      ">> Epoch 208 finished \tANN training loss 0.029642\n",
      ">> Epoch 209 finished \tANN training loss 0.028217\n",
      ">> Epoch 210 finished \tANN training loss 0.023959\n",
      ">> Epoch 211 finished \tANN training loss 0.026456\n",
      ">> Epoch 212 finished \tANN training loss 0.027436\n",
      ">> Epoch 213 finished \tANN training loss 0.020529\n",
      ">> Epoch 214 finished \tANN training loss 0.028007\n",
      ">> Epoch 215 finished \tANN training loss 0.019835\n",
      ">> Epoch 216 finished \tANN training loss 0.023075\n",
      ">> Epoch 217 finished \tANN training loss 0.027006\n",
      ">> Epoch 218 finished \tANN training loss 0.019712\n",
      ">> Epoch 219 finished \tANN training loss 0.018128\n",
      ">> Epoch 220 finished \tANN training loss 0.029413\n",
      ">> Epoch 221 finished \tANN training loss 0.027170\n",
      ">> Epoch 222 finished \tANN training loss 0.021600\n",
      ">> Epoch 223 finished \tANN training loss 0.020351\n",
      ">> Epoch 224 finished \tANN training loss 0.020626\n",
      ">> Epoch 225 finished \tANN training loss 0.025487\n",
      ">> Epoch 226 finished \tANN training loss 0.024884\n",
      ">> Epoch 227 finished \tANN training loss 0.023693\n",
      ">> Epoch 228 finished \tANN training loss 0.023176\n",
      ">> Epoch 229 finished \tANN training loss 0.020401\n",
      ">> Epoch 230 finished \tANN training loss 0.021270\n",
      ">> Epoch 231 finished \tANN training loss 0.018688\n",
      ">> Epoch 232 finished \tANN training loss 0.020690\n",
      ">> Epoch 233 finished \tANN training loss 0.022508\n",
      ">> Epoch 234 finished \tANN training loss 0.019397\n",
      ">> Epoch 235 finished \tANN training loss 0.024241\n",
      ">> Epoch 236 finished \tANN training loss 0.016611\n",
      ">> Epoch 237 finished \tANN training loss 0.019573\n",
      ">> Epoch 238 finished \tANN training loss 0.019284\n",
      ">> Epoch 239 finished \tANN training loss 0.014747\n",
      ">> Epoch 240 finished \tANN training loss 0.016519\n",
      ">> Epoch 241 finished \tANN training loss 0.018158\n",
      ">> Epoch 242 finished \tANN training loss 0.015525\n",
      ">> Epoch 243 finished \tANN training loss 0.019417\n",
      ">> Epoch 244 finished \tANN training loss 0.013777\n",
      ">> Epoch 245 finished \tANN training loss 0.013446\n",
      ">> Epoch 246 finished \tANN training loss 0.016061\n",
      ">> Epoch 247 finished \tANN training loss 0.024299\n",
      ">> Epoch 248 finished \tANN training loss 0.014953\n",
      ">> Epoch 249 finished \tANN training loss 0.021521\n",
      ">> Epoch 250 finished \tANN training loss 0.015766\n",
      ">> Epoch 251 finished \tANN training loss 0.013094\n",
      ">> Epoch 252 finished \tANN training loss 0.013959\n",
      ">> Epoch 253 finished \tANN training loss 0.017653\n",
      ">> Epoch 254 finished \tANN training loss 0.013468\n",
      ">> Epoch 255 finished \tANN training loss 0.018507\n",
      ">> Epoch 256 finished \tANN training loss 0.019049\n",
      ">> Epoch 257 finished \tANN training loss 0.022141\n",
      ">> Epoch 258 finished \tANN training loss 0.020202\n",
      ">> Epoch 259 finished \tANN training loss 0.019477\n",
      ">> Epoch 260 finished \tANN training loss 0.021590\n",
      ">> Epoch 261 finished \tANN training loss 0.021100\n",
      ">> Epoch 262 finished \tANN training loss 0.017655\n",
      ">> Epoch 263 finished \tANN training loss 0.021650\n",
      ">> Epoch 264 finished \tANN training loss 0.012850\n",
      ">> Epoch 265 finished \tANN training loss 0.015608\n",
      ">> Epoch 266 finished \tANN training loss 0.014627\n",
      ">> Epoch 267 finished \tANN training loss 0.013839\n",
      ">> Epoch 268 finished \tANN training loss 0.016841\n",
      ">> Epoch 269 finished \tANN training loss 0.017042\n",
      ">> Epoch 270 finished \tANN training loss 0.015149\n",
      ">> Epoch 271 finished \tANN training loss 0.013955\n",
      ">> Epoch 272 finished \tANN training loss 0.014936\n",
      ">> Epoch 273 finished \tANN training loss 0.015699\n",
      ">> Epoch 274 finished \tANN training loss 0.016982\n",
      ">> Epoch 275 finished \tANN training loss 0.011807\n",
      ">> Epoch 276 finished \tANN training loss 0.014007\n",
      ">> Epoch 277 finished \tANN training loss 0.028508\n",
      ">> Epoch 278 finished \tANN training loss 0.010986\n",
      ">> Epoch 279 finished \tANN training loss 0.011723\n",
      ">> Epoch 280 finished \tANN training loss 0.013513\n",
      ">> Epoch 281 finished \tANN training loss 0.015199\n",
      ">> Epoch 282 finished \tANN training loss 0.015579\n",
      ">> Epoch 283 finished \tANN training loss 0.016636\n",
      ">> Epoch 284 finished \tANN training loss 0.018236\n",
      ">> Epoch 285 finished \tANN training loss 0.014505\n",
      ">> Epoch 286 finished \tANN training loss 0.014090\n",
      ">> Epoch 287 finished \tANN training loss 0.011902\n",
      ">> Epoch 288 finished \tANN training loss 0.014079\n",
      ">> Epoch 289 finished \tANN training loss 0.013620\n",
      ">> Epoch 290 finished \tANN training loss 0.015575\n",
      ">> Epoch 291 finished \tANN training loss 0.015035\n",
      ">> Epoch 292 finished \tANN training loss 0.018804\n",
      ">> Epoch 293 finished \tANN training loss 0.020403\n",
      ">> Epoch 294 finished \tANN training loss 0.012564\n",
      ">> Epoch 295 finished \tANN training loss 0.013386\n",
      ">> Epoch 296 finished \tANN training loss 0.013839\n",
      ">> Epoch 297 finished \tANN training loss 0.016572\n",
      ">> Epoch 298 finished \tANN training loss 0.012826\n",
      ">> Epoch 299 finished \tANN training loss 0.011540\n",
      ">> Epoch 300 finished \tANN training loss 0.020044\n",
      ">> Epoch 301 finished \tANN training loss 0.017564\n",
      ">> Epoch 302 finished \tANN training loss 0.014070\n",
      ">> Epoch 303 finished \tANN training loss 0.013635\n",
      ">> Epoch 304 finished \tANN training loss 0.011816\n",
      ">> Epoch 305 finished \tANN training loss 0.012163\n",
      ">> Epoch 306 finished \tANN training loss 0.012758\n",
      ">> Epoch 307 finished \tANN training loss 0.011527\n",
      ">> Epoch 308 finished \tANN training loss 0.011771\n",
      ">> Epoch 309 finished \tANN training loss 0.008441\n",
      ">> Epoch 310 finished \tANN training loss 0.012410\n",
      ">> Epoch 311 finished \tANN training loss 0.011764\n",
      ">> Epoch 312 finished \tANN training loss 0.016054\n",
      ">> Epoch 313 finished \tANN training loss 0.017350\n",
      ">> Epoch 314 finished \tANN training loss 0.012132\n",
      ">> Epoch 315 finished \tANN training loss 0.010711\n",
      ">> Epoch 316 finished \tANN training loss 0.014667\n",
      ">> Epoch 317 finished \tANN training loss 0.012757\n",
      ">> Epoch 318 finished \tANN training loss 0.009251\n",
      ">> Epoch 319 finished \tANN training loss 0.013249\n",
      ">> Epoch 320 finished \tANN training loss 0.013756\n",
      ">> Epoch 321 finished \tANN training loss 0.012037\n",
      ">> Epoch 322 finished \tANN training loss 0.014463\n",
      ">> Epoch 323 finished \tANN training loss 0.012565\n",
      ">> Epoch 324 finished \tANN training loss 0.010968\n",
      ">> Epoch 325 finished \tANN training loss 0.009151\n",
      ">> Epoch 326 finished \tANN training loss 0.010537\n",
      ">> Epoch 327 finished \tANN training loss 0.013144\n",
      ">> Epoch 328 finished \tANN training loss 0.012387\n",
      ">> Epoch 329 finished \tANN training loss 0.019330\n",
      ">> Epoch 330 finished \tANN training loss 0.017212\n",
      ">> Epoch 331 finished \tANN training loss 0.016718\n",
      ">> Epoch 332 finished \tANN training loss 0.016497\n",
      ">> Epoch 333 finished \tANN training loss 0.008488\n",
      ">> Epoch 334 finished \tANN training loss 0.009663\n",
      ">> Epoch 335 finished \tANN training loss 0.012112\n",
      ">> Epoch 336 finished \tANN training loss 0.017411\n",
      ">> Epoch 337 finished \tANN training loss 0.011129\n",
      ">> Epoch 338 finished \tANN training loss 0.009468\n",
      ">> Epoch 339 finished \tANN training loss 0.009096\n",
      ">> Epoch 340 finished \tANN training loss 0.013742\n",
      ">> Epoch 341 finished \tANN training loss 0.015045\n",
      ">> Epoch 342 finished \tANN training loss 0.007937\n",
      ">> Epoch 343 finished \tANN training loss 0.010697\n",
      ">> Epoch 344 finished \tANN training loss 0.010512\n",
      ">> Epoch 345 finished \tANN training loss 0.010932\n",
      ">> Epoch 346 finished \tANN training loss 0.012503\n",
      ">> Epoch 347 finished \tANN training loss 0.011988\n",
      ">> Epoch 348 finished \tANN training loss 0.009897\n",
      ">> Epoch 349 finished \tANN training loss 0.011669\n",
      ">> Epoch 350 finished \tANN training loss 0.011729\n",
      ">> Epoch 351 finished \tANN training loss 0.010735\n",
      ">> Epoch 352 finished \tANN training loss 0.009958\n",
      ">> Epoch 353 finished \tANN training loss 0.008059\n",
      ">> Epoch 354 finished \tANN training loss 0.007175\n",
      ">> Epoch 355 finished \tANN training loss 0.009585\n",
      ">> Epoch 356 finished \tANN training loss 0.008240\n",
      ">> Epoch 357 finished \tANN training loss 0.008613\n",
      ">> Epoch 358 finished \tANN training loss 0.009917\n",
      ">> Epoch 359 finished \tANN training loss 0.008831\n",
      ">> Epoch 360 finished \tANN training loss 0.010522\n",
      ">> Epoch 361 finished \tANN training loss 0.007641\n",
      ">> Epoch 362 finished \tANN training loss 0.008113\n",
      ">> Epoch 363 finished \tANN training loss 0.008069\n",
      ">> Epoch 364 finished \tANN training loss 0.009467\n",
      ">> Epoch 365 finished \tANN training loss 0.011574\n",
      ">> Epoch 366 finished \tANN training loss 0.012232\n",
      ">> Epoch 367 finished \tANN training loss 0.010075\n",
      ">> Epoch 368 finished \tANN training loss 0.010226\n",
      ">> Epoch 369 finished \tANN training loss 0.011013\n",
      ">> Epoch 370 finished \tANN training loss 0.013905\n",
      ">> Epoch 371 finished \tANN training loss 0.008425\n",
      ">> Epoch 372 finished \tANN training loss 0.008875\n",
      ">> Epoch 373 finished \tANN training loss 0.012899\n",
      ">> Epoch 374 finished \tANN training loss 0.010742\n",
      ">> Epoch 375 finished \tANN training loss 0.008999\n",
      ">> Epoch 376 finished \tANN training loss 0.007858\n",
      ">> Epoch 377 finished \tANN training loss 0.008826\n",
      ">> Epoch 378 finished \tANN training loss 0.008431\n",
      ">> Epoch 379 finished \tANN training loss 0.009534\n",
      ">> Epoch 380 finished \tANN training loss 0.011166\n",
      ">> Epoch 381 finished \tANN training loss 0.013932\n",
      ">> Epoch 382 finished \tANN training loss 0.015345\n",
      ">> Epoch 383 finished \tANN training loss 0.009183\n",
      ">> Epoch 384 finished \tANN training loss 0.008488\n",
      ">> Epoch 385 finished \tANN training loss 0.010506\n",
      ">> Epoch 386 finished \tANN training loss 0.011101\n",
      ">> Epoch 387 finished \tANN training loss 0.011083\n",
      ">> Epoch 388 finished \tANN training loss 0.013522\n",
      ">> Epoch 389 finished \tANN training loss 0.009878\n",
      ">> Epoch 390 finished \tANN training loss 0.009667\n",
      ">> Epoch 391 finished \tANN training loss 0.013335\n",
      ">> Epoch 392 finished \tANN training loss 0.009391\n",
      ">> Epoch 393 finished \tANN training loss 0.007651\n",
      ">> Epoch 394 finished \tANN training loss 0.007411\n",
      ">> Epoch 395 finished \tANN training loss 0.007715\n",
      ">> Epoch 396 finished \tANN training loss 0.013030\n",
      ">> Epoch 397 finished \tANN training loss 0.009611\n",
      ">> Epoch 398 finished \tANN training loss 0.010112\n",
      ">> Epoch 399 finished \tANN training loss 0.007787\n",
      ">> Epoch 400 finished \tANN training loss 0.008916\n",
      ">> Epoch 401 finished \tANN training loss 0.010079\n",
      ">> Epoch 402 finished \tANN training loss 0.008860\n",
      ">> Epoch 403 finished \tANN training loss 0.009585\n",
      ">> Epoch 404 finished \tANN training loss 0.009509\n",
      ">> Epoch 405 finished \tANN training loss 0.007676\n",
      ">> Epoch 406 finished \tANN training loss 0.006604\n",
      ">> Epoch 407 finished \tANN training loss 0.005772\n",
      ">> Epoch 408 finished \tANN training loss 0.006278\n",
      ">> Epoch 409 finished \tANN training loss 0.009730\n",
      ">> Epoch 410 finished \tANN training loss 0.008954\n",
      ">> Epoch 411 finished \tANN training loss 0.007353\n",
      ">> Epoch 412 finished \tANN training loss 0.010642\n",
      ">> Epoch 413 finished \tANN training loss 0.011730\n",
      ">> Epoch 414 finished \tANN training loss 0.014098\n",
      ">> Epoch 415 finished \tANN training loss 0.006911\n",
      ">> Epoch 416 finished \tANN training loss 0.006213\n",
      ">> Epoch 417 finished \tANN training loss 0.007586\n",
      ">> Epoch 418 finished \tANN training loss 0.014787\n",
      ">> Epoch 419 finished \tANN training loss 0.008475\n",
      ">> Epoch 420 finished \tANN training loss 0.007669\n",
      ">> Epoch 421 finished \tANN training loss 0.008826\n",
      ">> Epoch 422 finished \tANN training loss 0.007824\n",
      ">> Epoch 423 finished \tANN training loss 0.005880\n",
      ">> Epoch 424 finished \tANN training loss 0.006016\n",
      ">> Epoch 425 finished \tANN training loss 0.005147\n",
      ">> Epoch 426 finished \tANN training loss 0.008330\n",
      ">> Epoch 427 finished \tANN training loss 0.008061\n",
      ">> Epoch 428 finished \tANN training loss 0.006306\n",
      ">> Epoch 429 finished \tANN training loss 0.012620\n",
      ">> Epoch 430 finished \tANN training loss 0.007584\n",
      ">> Epoch 431 finished \tANN training loss 0.011469\n",
      ">> Epoch 432 finished \tANN training loss 0.009404\n",
      ">> Epoch 433 finished \tANN training loss 0.013464\n",
      ">> Epoch 434 finished \tANN training loss 0.011063\n",
      ">> Epoch 435 finished \tANN training loss 0.012031\n",
      ">> Epoch 436 finished \tANN training loss 0.015794\n",
      ">> Epoch 437 finished \tANN training loss 0.008595\n",
      ">> Epoch 438 finished \tANN training loss 0.009451\n",
      ">> Epoch 439 finished \tANN training loss 0.006722\n",
      ">> Epoch 440 finished \tANN training loss 0.013913\n",
      ">> Epoch 441 finished \tANN training loss 0.009193\n",
      ">> Epoch 442 finished \tANN training loss 0.007822\n",
      ">> Epoch 443 finished \tANN training loss 0.007513\n",
      ">> Epoch 444 finished \tANN training loss 0.010460\n",
      ">> Epoch 445 finished \tANN training loss 0.005692\n",
      ">> Epoch 446 finished \tANN training loss 0.008729\n",
      ">> Epoch 447 finished \tANN training loss 0.011484\n",
      ">> Epoch 448 finished \tANN training loss 0.009984\n",
      ">> Epoch 449 finished \tANN training loss 0.011044\n",
      ">> Epoch 450 finished \tANN training loss 0.013039\n",
      ">> Epoch 451 finished \tANN training loss 0.011698\n",
      ">> Epoch 452 finished \tANN training loss 0.008615\n",
      ">> Epoch 453 finished \tANN training loss 0.010435\n",
      ">> Epoch 454 finished \tANN training loss 0.009071\n",
      ">> Epoch 455 finished \tANN training loss 0.008096\n",
      ">> Epoch 456 finished \tANN training loss 0.006999\n",
      ">> Epoch 457 finished \tANN training loss 0.009818\n",
      ">> Epoch 458 finished \tANN training loss 0.005371\n",
      ">> Epoch 459 finished \tANN training loss 0.004861\n",
      ">> Epoch 460 finished \tANN training loss 0.005984\n",
      ">> Epoch 461 finished \tANN training loss 0.008802\n",
      ">> Epoch 462 finished \tANN training loss 0.007255\n",
      ">> Epoch 463 finished \tANN training loss 0.006838\n",
      ">> Epoch 464 finished \tANN training loss 0.006728\n",
      ">> Epoch 465 finished \tANN training loss 0.008881\n",
      ">> Epoch 466 finished \tANN training loss 0.007563\n",
      ">> Epoch 467 finished \tANN training loss 0.007652\n",
      ">> Epoch 468 finished \tANN training loss 0.007411\n",
      ">> Epoch 469 finished \tANN training loss 0.006502\n",
      ">> Epoch 470 finished \tANN training loss 0.006332\n",
      ">> Epoch 471 finished \tANN training loss 0.005428\n",
      ">> Epoch 472 finished \tANN training loss 0.007173\n",
      ">> Epoch 473 finished \tANN training loss 0.012925\n",
      ">> Epoch 474 finished \tANN training loss 0.010971\n",
      ">> Epoch 475 finished \tANN training loss 0.007019\n",
      ">> Epoch 476 finished \tANN training loss 0.007026\n",
      ">> Epoch 477 finished \tANN training loss 0.007488\n",
      ">> Epoch 478 finished \tANN training loss 0.006419\n",
      ">> Epoch 479 finished \tANN training loss 0.007391\n",
      ">> Epoch 480 finished \tANN training loss 0.006455\n",
      ">> Epoch 481 finished \tANN training loss 0.006489\n",
      ">> Epoch 482 finished \tANN training loss 0.006760\n",
      ">> Epoch 483 finished \tANN training loss 0.007154\n",
      ">> Epoch 484 finished \tANN training loss 0.010685\n",
      ">> Epoch 485 finished \tANN training loss 0.006513\n",
      ">> Epoch 486 finished \tANN training loss 0.007176\n",
      ">> Epoch 487 finished \tANN training loss 0.006840\n",
      ">> Epoch 488 finished \tANN training loss 0.010798\n",
      ">> Epoch 489 finished \tANN training loss 0.007133\n",
      ">> Epoch 490 finished \tANN training loss 0.015498\n",
      ">> Epoch 491 finished \tANN training loss 0.009204\n",
      ">> Epoch 492 finished \tANN training loss 0.006587\n",
      ">> Epoch 493 finished \tANN training loss 0.006983\n",
      ">> Epoch 494 finished \tANN training loss 0.007121\n",
      ">> Epoch 495 finished \tANN training loss 0.009233\n",
      ">> Epoch 496 finished \tANN training loss 0.008663\n",
      ">> Epoch 497 finished \tANN training loss 0.006159\n",
      ">> Epoch 498 finished \tANN training loss 0.011380\n",
      ">> Epoch 499 finished \tANN training loss 0.006605\n",
      ">> Epoch 500 finished \tANN training loss 0.008085\n",
      ">> Epoch 501 finished \tANN training loss 0.007359\n",
      ">> Epoch 502 finished \tANN training loss 0.011818\n",
      ">> Epoch 503 finished \tANN training loss 0.007773\n",
      ">> Epoch 504 finished \tANN training loss 0.007434\n",
      ">> Epoch 505 finished \tANN training loss 0.007602\n",
      ">> Epoch 506 finished \tANN training loss 0.008554\n",
      ">> Epoch 507 finished \tANN training loss 0.008749\n",
      ">> Epoch 508 finished \tANN training loss 0.007328\n",
      ">> Epoch 509 finished \tANN training loss 0.006219\n",
      ">> Epoch 510 finished \tANN training loss 0.006216\n",
      ">> Epoch 511 finished \tANN training loss 0.006965\n",
      ">> Epoch 512 finished \tANN training loss 0.004861\n",
      ">> Epoch 513 finished \tANN training loss 0.004857\n",
      ">> Epoch 514 finished \tANN training loss 0.011319\n",
      ">> Epoch 515 finished \tANN training loss 0.006273\n",
      ">> Epoch 516 finished \tANN training loss 0.005475\n",
      ">> Epoch 517 finished \tANN training loss 0.012075\n",
      ">> Epoch 518 finished \tANN training loss 0.008329\n",
      ">> Epoch 519 finished \tANN training loss 0.009363\n",
      ">> Epoch 520 finished \tANN training loss 0.007164\n",
      ">> Epoch 521 finished \tANN training loss 0.007119\n",
      ">> Epoch 522 finished \tANN training loss 0.005020\n",
      ">> Epoch 523 finished \tANN training loss 0.006740\n",
      ">> Epoch 524 finished \tANN training loss 0.006617\n",
      ">> Epoch 525 finished \tANN training loss 0.006158\n",
      ">> Epoch 526 finished \tANN training loss 0.011881\n",
      ">> Epoch 527 finished \tANN training loss 0.005197\n",
      ">> Epoch 528 finished \tANN training loss 0.004681\n",
      ">> Epoch 529 finished \tANN training loss 0.005360\n",
      ">> Epoch 530 finished \tANN training loss 0.005059\n",
      ">> Epoch 531 finished \tANN training loss 0.005143\n",
      ">> Epoch 532 finished \tANN training loss 0.008710\n",
      ">> Epoch 533 finished \tANN training loss 0.006984\n",
      ">> Epoch 534 finished \tANN training loss 0.012779\n",
      ">> Epoch 535 finished \tANN training loss 0.006219\n",
      ">> Epoch 536 finished \tANN training loss 0.009488\n",
      ">> Epoch 537 finished \tANN training loss 0.013372\n",
      ">> Epoch 538 finished \tANN training loss 0.009986\n",
      ">> Epoch 539 finished \tANN training loss 0.007318\n",
      ">> Epoch 540 finished \tANN training loss 0.005866\n",
      ">> Epoch 541 finished \tANN training loss 0.007224\n",
      ">> Epoch 542 finished \tANN training loss 0.008817\n",
      ">> Epoch 543 finished \tANN training loss 0.006665\n",
      ">> Epoch 544 finished \tANN training loss 0.004418\n",
      ">> Epoch 545 finished \tANN training loss 0.006234\n",
      ">> Epoch 546 finished \tANN training loss 0.005770\n",
      ">> Epoch 547 finished \tANN training loss 0.004990\n",
      ">> Epoch 548 finished \tANN training loss 0.006804\n",
      ">> Epoch 549 finished \tANN training loss 0.008002\n",
      ">> Epoch 550 finished \tANN training loss 0.008037\n",
      ">> Epoch 551 finished \tANN training loss 0.007029\n",
      ">> Epoch 552 finished \tANN training loss 0.005016\n",
      ">> Epoch 553 finished \tANN training loss 0.006687\n",
      ">> Epoch 554 finished \tANN training loss 0.006733\n",
      ">> Epoch 555 finished \tANN training loss 0.011513\n",
      ">> Epoch 556 finished \tANN training loss 0.010029\n",
      ">> Epoch 557 finished \tANN training loss 0.008308\n",
      ">> Epoch 558 finished \tANN training loss 0.010774\n",
      ">> Epoch 559 finished \tANN training loss 0.016294\n",
      ">> Epoch 560 finished \tANN training loss 0.010041\n",
      ">> Epoch 561 finished \tANN training loss 0.007914\n",
      ">> Epoch 562 finished \tANN training loss 0.007458\n",
      ">> Epoch 563 finished \tANN training loss 0.008844\n",
      ">> Epoch 564 finished \tANN training loss 0.007075\n",
      ">> Epoch 565 finished \tANN training loss 0.006987\n",
      ">> Epoch 566 finished \tANN training loss 0.009281\n",
      ">> Epoch 567 finished \tANN training loss 0.007409\n",
      ">> Epoch 568 finished \tANN training loss 0.010143\n",
      ">> Epoch 569 finished \tANN training loss 0.007269\n",
      ">> Epoch 570 finished \tANN training loss 0.007595\n",
      ">> Epoch 571 finished \tANN training loss 0.006528\n",
      ">> Epoch 572 finished \tANN training loss 0.006206\n",
      ">> Epoch 573 finished \tANN training loss 0.006899\n",
      ">> Epoch 574 finished \tANN training loss 0.005820\n",
      ">> Epoch 575 finished \tANN training loss 0.009655\n",
      ">> Epoch 576 finished \tANN training loss 0.007821\n",
      ">> Epoch 577 finished \tANN training loss 0.006724\n",
      ">> Epoch 578 finished \tANN training loss 0.006564\n",
      ">> Epoch 579 finished \tANN training loss 0.008123\n",
      ">> Epoch 580 finished \tANN training loss 0.007261\n",
      ">> Epoch 581 finished \tANN training loss 0.005796\n",
      ">> Epoch 582 finished \tANN training loss 0.006324\n",
      ">> Epoch 583 finished \tANN training loss 0.006722\n",
      ">> Epoch 584 finished \tANN training loss 0.006455\n",
      ">> Epoch 585 finished \tANN training loss 0.004816\n",
      ">> Epoch 586 finished \tANN training loss 0.006143\n",
      ">> Epoch 587 finished \tANN training loss 0.008988\n",
      ">> Epoch 588 finished \tANN training loss 0.006024\n",
      ">> Epoch 589 finished \tANN training loss 0.004340\n",
      ">> Epoch 590 finished \tANN training loss 0.004495\n",
      ">> Epoch 591 finished \tANN training loss 0.004287\n",
      ">> Epoch 592 finished \tANN training loss 0.005265\n",
      ">> Epoch 593 finished \tANN training loss 0.004499\n",
      ">> Epoch 594 finished \tANN training loss 0.004531\n",
      ">> Epoch 595 finished \tANN training loss 0.005825\n",
      ">> Epoch 596 finished \tANN training loss 0.007352\n",
      ">> Epoch 597 finished \tANN training loss 0.008335\n",
      ">> Epoch 598 finished \tANN training loss 0.009327\n",
      ">> Epoch 599 finished \tANN training loss 0.005111\n",
      ">> Epoch 600 finished \tANN training loss 0.006193\n",
      ">> Epoch 601 finished \tANN training loss 0.009559\n",
      ">> Epoch 602 finished \tANN training loss 0.006035\n",
      ">> Epoch 603 finished \tANN training loss 0.007925\n",
      ">> Epoch 604 finished \tANN training loss 0.011353\n",
      ">> Epoch 605 finished \tANN training loss 0.010284\n",
      ">> Epoch 606 finished \tANN training loss 0.006579\n",
      ">> Epoch 607 finished \tANN training loss 0.007589\n",
      ">> Epoch 608 finished \tANN training loss 0.011888\n",
      ">> Epoch 609 finished \tANN training loss 0.007124\n",
      ">> Epoch 610 finished \tANN training loss 0.007270\n",
      ">> Epoch 611 finished \tANN training loss 0.007345\n",
      ">> Epoch 612 finished \tANN training loss 0.007129\n",
      ">> Epoch 613 finished \tANN training loss 0.008453\n",
      ">> Epoch 614 finished \tANN training loss 0.006286\n",
      ">> Epoch 615 finished \tANN training loss 0.006517\n",
      ">> Epoch 616 finished \tANN training loss 0.007106\n",
      ">> Epoch 617 finished \tANN training loss 0.012055\n",
      ">> Epoch 618 finished \tANN training loss 0.016818\n",
      ">> Epoch 619 finished \tANN training loss 0.008111\n",
      ">> Epoch 620 finished \tANN training loss 0.009514\n",
      ">> Epoch 621 finished \tANN training loss 0.009355\n",
      ">> Epoch 622 finished \tANN training loss 0.008781\n",
      ">> Epoch 623 finished \tANN training loss 0.011471\n",
      ">> Epoch 624 finished \tANN training loss 0.007314\n",
      ">> Epoch 625 finished \tANN training loss 0.007497\n",
      ">> Epoch 626 finished \tANN training loss 0.007721\n",
      ">> Epoch 627 finished \tANN training loss 0.006507\n",
      ">> Epoch 628 finished \tANN training loss 0.008728\n",
      ">> Epoch 629 finished \tANN training loss 0.009480\n",
      ">> Epoch 630 finished \tANN training loss 0.005336\n",
      ">> Epoch 631 finished \tANN training loss 0.004941\n",
      ">> Epoch 632 finished \tANN training loss 0.005787\n",
      ">> Epoch 633 finished \tANN training loss 0.007512\n",
      ">> Epoch 634 finished \tANN training loss 0.010068\n",
      ">> Epoch 635 finished \tANN training loss 0.005048\n",
      ">> Epoch 636 finished \tANN training loss 0.005803\n",
      ">> Epoch 637 finished \tANN training loss 0.005409\n",
      ">> Epoch 638 finished \tANN training loss 0.004717\n",
      ">> Epoch 639 finished \tANN training loss 0.004739\n",
      ">> Epoch 640 finished \tANN training loss 0.007922\n",
      ">> Epoch 641 finished \tANN training loss 0.006086\n",
      ">> Epoch 642 finished \tANN training loss 0.006720\n",
      ">> Epoch 643 finished \tANN training loss 0.007070\n",
      ">> Epoch 644 finished \tANN training loss 0.006377\n",
      ">> Epoch 645 finished \tANN training loss 0.012166\n",
      ">> Epoch 646 finished \tANN training loss 0.005405\n",
      ">> Epoch 647 finished \tANN training loss 0.008106\n",
      ">> Epoch 648 finished \tANN training loss 0.005854\n",
      ">> Epoch 649 finished \tANN training loss 0.005764\n",
      ">> Epoch 650 finished \tANN training loss 0.003692\n",
      ">> Epoch 651 finished \tANN training loss 0.004686\n",
      ">> Epoch 652 finished \tANN training loss 0.005141\n",
      ">> Epoch 653 finished \tANN training loss 0.004598\n",
      ">> Epoch 654 finished \tANN training loss 0.004098\n",
      ">> Epoch 655 finished \tANN training loss 0.005936\n",
      ">> Epoch 656 finished \tANN training loss 0.007181\n",
      ">> Epoch 657 finished \tANN training loss 0.005468\n",
      ">> Epoch 658 finished \tANN training loss 0.004866\n",
      ">> Epoch 659 finished \tANN training loss 0.005033\n",
      ">> Epoch 660 finished \tANN training loss 0.005986\n",
      ">> Epoch 661 finished \tANN training loss 0.003965\n",
      ">> Epoch 662 finished \tANN training loss 0.004529\n",
      ">> Epoch 663 finished \tANN training loss 0.005633\n",
      ">> Epoch 664 finished \tANN training loss 0.005351\n",
      ">> Epoch 665 finished \tANN training loss 0.007401\n",
      ">> Epoch 666 finished \tANN training loss 0.005952\n",
      ">> Epoch 667 finished \tANN training loss 0.007427\n",
      ">> Epoch 668 finished \tANN training loss 0.005191\n",
      ">> Epoch 669 finished \tANN training loss 0.004698\n",
      ">> Epoch 670 finished \tANN training loss 0.006372\n",
      ">> Epoch 671 finished \tANN training loss 0.004061\n",
      ">> Epoch 672 finished \tANN training loss 0.005487\n",
      ">> Epoch 673 finished \tANN training loss 0.003228\n",
      ">> Epoch 674 finished \tANN training loss 0.003653\n",
      ">> Epoch 675 finished \tANN training loss 0.011265\n",
      ">> Epoch 676 finished \tANN training loss 0.008472\n",
      ">> Epoch 677 finished \tANN training loss 0.005152\n",
      ">> Epoch 678 finished \tANN training loss 0.005456\n",
      ">> Epoch 679 finished \tANN training loss 0.006481\n",
      ">> Epoch 680 finished \tANN training loss 0.004773\n",
      ">> Epoch 681 finished \tANN training loss 0.005575\n",
      ">> Epoch 682 finished \tANN training loss 0.007571\n",
      ">> Epoch 683 finished \tANN training loss 0.005058\n",
      ">> Epoch 684 finished \tANN training loss 0.005283\n",
      ">> Epoch 685 finished \tANN training loss 0.005928\n",
      ">> Epoch 686 finished \tANN training loss 0.008409\n",
      ">> Epoch 687 finished \tANN training loss 0.005784\n",
      ">> Epoch 688 finished \tANN training loss 0.005149\n",
      ">> Epoch 689 finished \tANN training loss 0.005918\n",
      ">> Epoch 690 finished \tANN training loss 0.005344\n",
      ">> Epoch 691 finished \tANN training loss 0.005202\n",
      ">> Epoch 692 finished \tANN training loss 0.006825\n",
      ">> Epoch 693 finished \tANN training loss 0.006474\n",
      ">> Epoch 694 finished \tANN training loss 0.004101\n",
      ">> Epoch 695 finished \tANN training loss 0.004452\n",
      ">> Epoch 696 finished \tANN training loss 0.003733\n",
      ">> Epoch 697 finished \tANN training loss 0.003589\n",
      ">> Epoch 698 finished \tANN training loss 0.004719\n",
      ">> Epoch 699 finished \tANN training loss 0.003342\n",
      ">> Epoch 700 finished \tANN training loss 0.005417\n",
      ">> Epoch 701 finished \tANN training loss 0.005690\n",
      ">> Epoch 702 finished \tANN training loss 0.004557\n",
      ">> Epoch 703 finished \tANN training loss 0.004891\n",
      ">> Epoch 704 finished \tANN training loss 0.003187\n",
      ">> Epoch 705 finished \tANN training loss 0.004663\n",
      ">> Epoch 706 finished \tANN training loss 0.007042\n",
      ">> Epoch 707 finished \tANN training loss 0.003944\n",
      ">> Epoch 708 finished \tANN training loss 0.002344\n",
      ">> Epoch 709 finished \tANN training loss 0.004155\n",
      ">> Epoch 710 finished \tANN training loss 0.004516\n",
      ">> Epoch 711 finished \tANN training loss 0.005087\n",
      ">> Epoch 712 finished \tANN training loss 0.007237\n",
      ">> Epoch 713 finished \tANN training loss 0.004798\n",
      ">> Epoch 714 finished \tANN training loss 0.005170\n",
      ">> Epoch 715 finished \tANN training loss 0.004651\n",
      ">> Epoch 716 finished \tANN training loss 0.005693\n",
      ">> Epoch 717 finished \tANN training loss 0.004678\n",
      ">> Epoch 718 finished \tANN training loss 0.009497\n",
      ">> Epoch 719 finished \tANN training loss 0.006926\n",
      ">> Epoch 720 finished \tANN training loss 0.005788\n",
      ">> Epoch 721 finished \tANN training loss 0.006980\n",
      ">> Epoch 722 finished \tANN training loss 0.009099\n",
      ">> Epoch 723 finished \tANN training loss 0.007570\n",
      ">> Epoch 724 finished \tANN training loss 0.007345\n",
      ">> Epoch 725 finished \tANN training loss 0.008379\n",
      ">> Epoch 726 finished \tANN training loss 0.005251\n",
      ">> Epoch 727 finished \tANN training loss 0.007948\n",
      ">> Epoch 728 finished \tANN training loss 0.006129\n",
      ">> Epoch 729 finished \tANN training loss 0.005519\n",
      ">> Epoch 730 finished \tANN training loss 0.007841\n",
      ">> Epoch 731 finished \tANN training loss 0.006108\n",
      ">> Epoch 732 finished \tANN training loss 0.005384\n",
      ">> Epoch 733 finished \tANN training loss 0.004792\n",
      ">> Epoch 734 finished \tANN training loss 0.003983\n",
      ">> Epoch 735 finished \tANN training loss 0.004680\n",
      ">> Epoch 736 finished \tANN training loss 0.005909\n",
      ">> Epoch 737 finished \tANN training loss 0.004609\n",
      ">> Epoch 738 finished \tANN training loss 0.005289\n",
      ">> Epoch 739 finished \tANN training loss 0.007698\n",
      ">> Epoch 740 finished \tANN training loss 0.005064\n",
      ">> Epoch 741 finished \tANN training loss 0.004013\n",
      ">> Epoch 742 finished \tANN training loss 0.004665\n",
      ">> Epoch 743 finished \tANN training loss 0.005223\n",
      ">> Epoch 744 finished \tANN training loss 0.004834\n",
      ">> Epoch 745 finished \tANN training loss 0.004993\n",
      ">> Epoch 746 finished \tANN training loss 0.005963\n",
      ">> Epoch 747 finished \tANN training loss 0.005073\n",
      ">> Epoch 748 finished \tANN training loss 0.005231\n",
      ">> Epoch 749 finished \tANN training loss 0.004641\n",
      ">> Epoch 750 finished \tANN training loss 0.004765\n",
      ">> Epoch 751 finished \tANN training loss 0.004864\n",
      ">> Epoch 752 finished \tANN training loss 0.005854\n",
      ">> Epoch 753 finished \tANN training loss 0.004529\n",
      ">> Epoch 754 finished \tANN training loss 0.004091\n",
      ">> Epoch 755 finished \tANN training loss 0.004716\n",
      ">> Epoch 756 finished \tANN training loss 0.004474\n",
      ">> Epoch 757 finished \tANN training loss 0.003377\n",
      ">> Epoch 758 finished \tANN training loss 0.004072\n",
      ">> Epoch 759 finished \tANN training loss 0.004820\n",
      ">> Epoch 760 finished \tANN training loss 0.007065\n",
      ">> Epoch 761 finished \tANN training loss 0.003276\n",
      ">> Epoch 762 finished \tANN training loss 0.007777\n",
      ">> Epoch 763 finished \tANN training loss 0.006018\n",
      ">> Epoch 764 finished \tANN training loss 0.005149\n",
      ">> Epoch 765 finished \tANN training loss 0.005223\n",
      ">> Epoch 766 finished \tANN training loss 0.004857\n",
      ">> Epoch 767 finished \tANN training loss 0.006401\n",
      ">> Epoch 768 finished \tANN training loss 0.006456\n",
      ">> Epoch 769 finished \tANN training loss 0.005092\n",
      ">> Epoch 770 finished \tANN training loss 0.004680\n",
      ">> Epoch 771 finished \tANN training loss 0.006139\n",
      ">> Epoch 772 finished \tANN training loss 0.005271\n",
      ">> Epoch 773 finished \tANN training loss 0.003821\n",
      ">> Epoch 774 finished \tANN training loss 0.004156\n",
      ">> Epoch 775 finished \tANN training loss 0.005374\n",
      ">> Epoch 776 finished \tANN training loss 0.004562\n",
      ">> Epoch 777 finished \tANN training loss 0.006007\n",
      ">> Epoch 778 finished \tANN training loss 0.005349\n",
      ">> Epoch 779 finished \tANN training loss 0.005546\n",
      ">> Epoch 780 finished \tANN training loss 0.004695\n",
      ">> Epoch 781 finished \tANN training loss 0.004358\n",
      ">> Epoch 782 finished \tANN training loss 0.006283\n",
      ">> Epoch 783 finished \tANN training loss 0.005230\n",
      ">> Epoch 784 finished \tANN training loss 0.003877\n",
      ">> Epoch 785 finished \tANN training loss 0.003911\n",
      ">> Epoch 786 finished \tANN training loss 0.004107\n",
      ">> Epoch 787 finished \tANN training loss 0.003921\n",
      ">> Epoch 788 finished \tANN training loss 0.004029\n",
      ">> Epoch 789 finished \tANN training loss 0.005531\n",
      ">> Epoch 790 finished \tANN training loss 0.006014\n",
      ">> Epoch 791 finished \tANN training loss 0.003845\n",
      ">> Epoch 792 finished \tANN training loss 0.003877\n",
      ">> Epoch 793 finished \tANN training loss 0.004845\n",
      ">> Epoch 794 finished \tANN training loss 0.004495\n",
      ">> Epoch 795 finished \tANN training loss 0.002968\n",
      ">> Epoch 796 finished \tANN training loss 0.002754\n",
      ">> Epoch 797 finished \tANN training loss 0.004374\n",
      ">> Epoch 798 finished \tANN training loss 0.005104\n",
      ">> Epoch 799 finished \tANN training loss 0.004404\n",
      ">> Epoch 800 finished \tANN training loss 0.003640\n",
      ">> Epoch 801 finished \tANN training loss 0.005538\n",
      ">> Epoch 802 finished \tANN training loss 0.004419\n",
      ">> Epoch 803 finished \tANN training loss 0.005472\n",
      ">> Epoch 804 finished \tANN training loss 0.009971\n",
      ">> Epoch 805 finished \tANN training loss 0.005442\n",
      ">> Epoch 806 finished \tANN training loss 0.004559\n",
      ">> Epoch 807 finished \tANN training loss 0.006769\n",
      ">> Epoch 808 finished \tANN training loss 0.006451\n",
      ">> Epoch 809 finished \tANN training loss 0.005991\n",
      ">> Epoch 810 finished \tANN training loss 0.004054\n",
      ">> Epoch 811 finished \tANN training loss 0.004554\n",
      ">> Epoch 812 finished \tANN training loss 0.004552\n",
      ">> Epoch 813 finished \tANN training loss 0.004172\n",
      ">> Epoch 814 finished \tANN training loss 0.004686\n",
      ">> Epoch 815 finished \tANN training loss 0.004259\n",
      ">> Epoch 816 finished \tANN training loss 0.003871\n",
      ">> Epoch 817 finished \tANN training loss 0.004034\n",
      ">> Epoch 818 finished \tANN training loss 0.004255\n",
      ">> Epoch 819 finished \tANN training loss 0.007004\n",
      ">> Epoch 820 finished \tANN training loss 0.005074\n",
      ">> Epoch 821 finished \tANN training loss 0.003774\n",
      ">> Epoch 822 finished \tANN training loss 0.003943\n",
      ">> Epoch 823 finished \tANN training loss 0.003611\n",
      ">> Epoch 824 finished \tANN training loss 0.002976\n",
      ">> Epoch 825 finished \tANN training loss 0.002567\n",
      ">> Epoch 826 finished \tANN training loss 0.002716\n",
      ">> Epoch 827 finished \tANN training loss 0.002682\n",
      ">> Epoch 828 finished \tANN training loss 0.002783\n",
      ">> Epoch 829 finished \tANN training loss 0.002826\n",
      ">> Epoch 830 finished \tANN training loss 0.002939\n",
      ">> Epoch 831 finished \tANN training loss 0.002659\n",
      ">> Epoch 832 finished \tANN training loss 0.004159\n",
      ">> Epoch 833 finished \tANN training loss 0.003643\n",
      ">> Epoch 834 finished \tANN training loss 0.003325\n",
      ">> Epoch 835 finished \tANN training loss 0.003408\n",
      ">> Epoch 836 finished \tANN training loss 0.002832\n",
      ">> Epoch 837 finished \tANN training loss 0.002742\n",
      ">> Epoch 838 finished \tANN training loss 0.002684\n",
      ">> Epoch 839 finished \tANN training loss 0.002504\n",
      ">> Epoch 840 finished \tANN training loss 0.002738\n",
      ">> Epoch 841 finished \tANN training loss 0.002651\n",
      ">> Epoch 842 finished \tANN training loss 0.003707\n",
      ">> Epoch 843 finished \tANN training loss 0.005747\n",
      ">> Epoch 844 finished \tANN training loss 0.003848\n",
      ">> Epoch 845 finished \tANN training loss 0.003629\n",
      ">> Epoch 846 finished \tANN training loss 0.004179\n",
      ">> Epoch 847 finished \tANN training loss 0.004357\n",
      ">> Epoch 848 finished \tANN training loss 0.006515\n",
      ">> Epoch 849 finished \tANN training loss 0.003736\n",
      ">> Epoch 850 finished \tANN training loss 0.003641\n",
      ">> Epoch 851 finished \tANN training loss 0.003065\n",
      ">> Epoch 852 finished \tANN training loss 0.002702\n",
      ">> Epoch 853 finished \tANN training loss 0.003356\n",
      ">> Epoch 854 finished \tANN training loss 0.002829\n",
      ">> Epoch 855 finished \tANN training loss 0.003490\n",
      ">> Epoch 856 finished \tANN training loss 0.005935\n",
      ">> Epoch 857 finished \tANN training loss 0.004452\n",
      ">> Epoch 858 finished \tANN training loss 0.003743\n",
      ">> Epoch 859 finished \tANN training loss 0.004008\n",
      ">> Epoch 860 finished \tANN training loss 0.003156\n",
      ">> Epoch 861 finished \tANN training loss 0.003035\n",
      ">> Epoch 862 finished \tANN training loss 0.002382\n",
      ">> Epoch 863 finished \tANN training loss 0.003646\n",
      ">> Epoch 864 finished \tANN training loss 0.002842\n",
      ">> Epoch 865 finished \tANN training loss 0.003280\n",
      ">> Epoch 866 finished \tANN training loss 0.005269\n",
      ">> Epoch 867 finished \tANN training loss 0.003434\n",
      ">> Epoch 868 finished \tANN training loss 0.003820\n",
      ">> Epoch 869 finished \tANN training loss 0.003781\n",
      ">> Epoch 870 finished \tANN training loss 0.006653\n",
      ">> Epoch 871 finished \tANN training loss 0.003286\n",
      ">> Epoch 872 finished \tANN training loss 0.003185\n",
      ">> Epoch 873 finished \tANN training loss 0.002447\n",
      ">> Epoch 874 finished \tANN training loss 0.003750\n",
      ">> Epoch 875 finished \tANN training loss 0.002829\n",
      ">> Epoch 876 finished \tANN training loss 0.002769\n",
      ">> Epoch 877 finished \tANN training loss 0.002572\n",
      ">> Epoch 878 finished \tANN training loss 0.002518\n",
      ">> Epoch 879 finished \tANN training loss 0.002924\n",
      ">> Epoch 880 finished \tANN training loss 0.004056\n",
      ">> Epoch 881 finished \tANN training loss 0.002375\n",
      ">> Epoch 882 finished \tANN training loss 0.002235\n",
      ">> Epoch 883 finished \tANN training loss 0.002167\n",
      ">> Epoch 884 finished \tANN training loss 0.004622\n",
      ">> Epoch 885 finished \tANN training loss 0.002740\n",
      ">> Epoch 886 finished \tANN training loss 0.003991\n",
      ">> Epoch 887 finished \tANN training loss 0.003936\n",
      ">> Epoch 888 finished \tANN training loss 0.005836\n",
      ">> Epoch 889 finished \tANN training loss 0.005993\n",
      ">> Epoch 890 finished \tANN training loss 0.004289\n",
      ">> Epoch 891 finished \tANN training loss 0.005004\n",
      ">> Epoch 892 finished \tANN training loss 0.005290\n",
      ">> Epoch 893 finished \tANN training loss 0.002942\n",
      ">> Epoch 894 finished \tANN training loss 0.004006\n",
      ">> Epoch 895 finished \tANN training loss 0.003128\n",
      ">> Epoch 896 finished \tANN training loss 0.003603\n",
      ">> Epoch 897 finished \tANN training loss 0.005742\n",
      ">> Epoch 898 finished \tANN training loss 0.005372\n",
      ">> Epoch 899 finished \tANN training loss 0.004150\n",
      ">> Epoch 900 finished \tANN training loss 0.005133\n",
      ">> Epoch 901 finished \tANN training loss 0.004526\n",
      ">> Epoch 902 finished \tANN training loss 0.004094\n",
      ">> Epoch 903 finished \tANN training loss 0.005116\n",
      ">> Epoch 904 finished \tANN training loss 0.004073\n",
      ">> Epoch 905 finished \tANN training loss 0.004504\n",
      ">> Epoch 906 finished \tANN training loss 0.004147\n",
      ">> Epoch 907 finished \tANN training loss 0.003500\n",
      ">> Epoch 908 finished \tANN training loss 0.004293\n",
      ">> Epoch 909 finished \tANN training loss 0.003109\n",
      ">> Epoch 910 finished \tANN training loss 0.002750\n",
      ">> Epoch 911 finished \tANN training loss 0.004707\n",
      ">> Epoch 912 finished \tANN training loss 0.002968\n",
      ">> Epoch 913 finished \tANN training loss 0.003560\n",
      ">> Epoch 914 finished \tANN training loss 0.003930\n",
      ">> Epoch 915 finished \tANN training loss 0.003522\n",
      ">> Epoch 916 finished \tANN training loss 0.003873\n",
      ">> Epoch 917 finished \tANN training loss 0.003276\n",
      ">> Epoch 918 finished \tANN training loss 0.003052\n",
      ">> Epoch 919 finished \tANN training loss 0.003173\n",
      ">> Epoch 920 finished \tANN training loss 0.002395\n",
      ">> Epoch 921 finished \tANN training loss 0.005894\n",
      ">> Epoch 922 finished \tANN training loss 0.005287\n",
      ">> Epoch 923 finished \tANN training loss 0.003335\n",
      ">> Epoch 924 finished \tANN training loss 0.002912\n",
      ">> Epoch 925 finished \tANN training loss 0.004220\n",
      ">> Epoch 926 finished \tANN training loss 0.004231\n",
      ">> Epoch 927 finished \tANN training loss 0.004057\n",
      ">> Epoch 928 finished \tANN training loss 0.005638\n",
      ">> Epoch 929 finished \tANN training loss 0.003693\n",
      ">> Epoch 930 finished \tANN training loss 0.005453\n",
      ">> Epoch 931 finished \tANN training loss 0.006616\n",
      ">> Epoch 932 finished \tANN training loss 0.005762\n",
      ">> Epoch 933 finished \tANN training loss 0.004698\n",
      ">> Epoch 934 finished \tANN training loss 0.004318\n",
      ">> Epoch 935 finished \tANN training loss 0.003590\n",
      ">> Epoch 936 finished \tANN training loss 0.003147\n",
      ">> Epoch 937 finished \tANN training loss 0.003038\n",
      ">> Epoch 938 finished \tANN training loss 0.002426\n",
      ">> Epoch 939 finished \tANN training loss 0.002968\n",
      ">> Epoch 940 finished \tANN training loss 0.002383\n",
      ">> Epoch 941 finished \tANN training loss 0.002678\n",
      ">> Epoch 942 finished \tANN training loss 0.002848\n",
      ">> Epoch 943 finished \tANN training loss 0.003910\n",
      ">> Epoch 944 finished \tANN training loss 0.005194\n",
      ">> Epoch 945 finished \tANN training loss 0.003013\n",
      ">> Epoch 946 finished \tANN training loss 0.003511\n",
      ">> Epoch 947 finished \tANN training loss 0.003430\n",
      ">> Epoch 948 finished \tANN training loss 0.002458\n",
      ">> Epoch 949 finished \tANN training loss 0.002098\n",
      ">> Epoch 950 finished \tANN training loss 0.003762\n",
      ">> Epoch 951 finished \tANN training loss 0.005224\n",
      ">> Epoch 952 finished \tANN training loss 0.004126\n",
      ">> Epoch 953 finished \tANN training loss 0.004065\n",
      ">> Epoch 954 finished \tANN training loss 0.003011\n",
      ">> Epoch 955 finished \tANN training loss 0.003277\n",
      ">> Epoch 956 finished \tANN training loss 0.003474\n",
      ">> Epoch 957 finished \tANN training loss 0.004057\n",
      ">> Epoch 958 finished \tANN training loss 0.006131\n",
      ">> Epoch 959 finished \tANN training loss 0.004579\n",
      ">> Epoch 960 finished \tANN training loss 0.004152\n",
      ">> Epoch 961 finished \tANN training loss 0.004332\n",
      ">> Epoch 962 finished \tANN training loss 0.003764\n",
      ">> Epoch 963 finished \tANN training loss 0.004169\n",
      ">> Epoch 964 finished \tANN training loss 0.004278\n",
      ">> Epoch 965 finished \tANN training loss 0.004494\n",
      ">> Epoch 966 finished \tANN training loss 0.004533\n",
      ">> Epoch 967 finished \tANN training loss 0.003707\n",
      ">> Epoch 968 finished \tANN training loss 0.004435\n",
      ">> Epoch 969 finished \tANN training loss 0.004455\n",
      ">> Epoch 970 finished \tANN training loss 0.005293\n",
      ">> Epoch 971 finished \tANN training loss 0.005047\n",
      ">> Epoch 972 finished \tANN training loss 0.004611\n",
      ">> Epoch 973 finished \tANN training loss 0.006399\n",
      ">> Epoch 974 finished \tANN training loss 0.003469\n",
      ">> Epoch 975 finished \tANN training loss 0.003975\n",
      ">> Epoch 976 finished \tANN training loss 0.004625\n",
      ">> Epoch 977 finished \tANN training loss 0.004960\n",
      ">> Epoch 978 finished \tANN training loss 0.004365\n",
      ">> Epoch 979 finished \tANN training loss 0.003504\n",
      ">> Epoch 980 finished \tANN training loss 0.003199\n",
      ">> Epoch 981 finished \tANN training loss 0.004780\n",
      ">> Epoch 982 finished \tANN training loss 0.003712\n",
      ">> Epoch 983 finished \tANN training loss 0.003700\n",
      ">> Epoch 984 finished \tANN training loss 0.003482\n",
      ">> Epoch 985 finished \tANN training loss 0.004176\n",
      ">> Epoch 986 finished \tANN training loss 0.006589\n",
      ">> Epoch 987 finished \tANN training loss 0.005924\n",
      ">> Epoch 988 finished \tANN training loss 0.004700\n",
      ">> Epoch 989 finished \tANN training loss 0.004323\n",
      ">> Epoch 990 finished \tANN training loss 0.003821\n",
      ">> Epoch 991 finished \tANN training loss 0.003974\n",
      ">> Epoch 992 finished \tANN training loss 0.003512\n",
      ">> Epoch 993 finished \tANN training loss 0.003821\n",
      ">> Epoch 994 finished \tANN training loss 0.004840\n",
      ">> Epoch 995 finished \tANN training loss 0.004682\n",
      ">> Epoch 996 finished \tANN training loss 0.012746\n",
      ">> Epoch 997 finished \tANN training loss 0.005609\n",
      ">> Epoch 998 finished \tANN training loss 0.005631\n",
      ">> Epoch 999 finished \tANN training loss 0.006061\n",
      "[END] Fine tuning step\n",
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 8.034707\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 7.757697\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 7.500552\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 6.923600\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 6.251729\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 5.453343\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 4.878526\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 4.544718\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 4.218747\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 3.852830\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 3.594609\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 3.381905\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 3.177173\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 2.996011\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 2.939123\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 2.802827\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 2.688874\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 2.597111\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 2.487993\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 2.382105\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 5.411632\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 5.142107\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 4.660827\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 3.826536\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 3.053498\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 2.598526\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 2.235403\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 1.878895\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 1.559713\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 1.301240\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 1.181418\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 1.014247\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 0.889130\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 0.842390\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 0.859517\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 0.755472\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 0.711665\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 0.678035\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 0.650855\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 0.657757\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.868040\n",
      ">> Epoch 1 finished \tANN training loss 0.739647\n",
      ">> Epoch 2 finished \tANN training loss 0.667731\n",
      ">> Epoch 3 finished \tANN training loss 0.594375\n",
      ">> Epoch 4 finished \tANN training loss 0.559635\n",
      ">> Epoch 5 finished \tANN training loss 0.522128\n",
      ">> Epoch 6 finished \tANN training loss 0.491412\n",
      ">> Epoch 7 finished \tANN training loss 0.471922\n",
      ">> Epoch 8 finished \tANN training loss 0.443758\n",
      ">> Epoch 9 finished \tANN training loss 0.421900\n",
      ">> Epoch 10 finished \tANN training loss 0.436183\n",
      ">> Epoch 11 finished \tANN training loss 0.391173\n",
      ">> Epoch 12 finished \tANN training loss 0.366727\n",
      ">> Epoch 13 finished \tANN training loss 0.371295\n",
      ">> Epoch 14 finished \tANN training loss 0.345973\n",
      ">> Epoch 15 finished \tANN training loss 0.325130\n",
      ">> Epoch 16 finished \tANN training loss 0.307273\n",
      ">> Epoch 17 finished \tANN training loss 0.327926\n",
      ">> Epoch 18 finished \tANN training loss 0.297400\n",
      ">> Epoch 19 finished \tANN training loss 0.288195\n",
      ">> Epoch 20 finished \tANN training loss 0.276959\n",
      ">> Epoch 21 finished \tANN training loss 0.266014\n",
      ">> Epoch 22 finished \tANN training loss 0.263024\n",
      ">> Epoch 23 finished \tANN training loss 0.242603\n",
      ">> Epoch 24 finished \tANN training loss 0.237616\n",
      ">> Epoch 25 finished \tANN training loss 0.236681\n",
      ">> Epoch 26 finished \tANN training loss 0.223333\n",
      ">> Epoch 27 finished \tANN training loss 0.237602\n",
      ">> Epoch 28 finished \tANN training loss 0.217038\n",
      ">> Epoch 29 finished \tANN training loss 0.209567\n",
      ">> Epoch 30 finished \tANN training loss 0.199096\n",
      ">> Epoch 31 finished \tANN training loss 0.197729\n",
      ">> Epoch 32 finished \tANN training loss 0.201126\n",
      ">> Epoch 33 finished \tANN training loss 0.191109\n",
      ">> Epoch 34 finished \tANN training loss 0.188222\n",
      ">> Epoch 35 finished \tANN training loss 0.171997\n",
      ">> Epoch 36 finished \tANN training loss 0.180330\n",
      ">> Epoch 37 finished \tANN training loss 0.186112\n",
      ">> Epoch 38 finished \tANN training loss 0.161232\n",
      ">> Epoch 39 finished \tANN training loss 0.157110\n",
      ">> Epoch 40 finished \tANN training loss 0.154349\n",
      ">> Epoch 41 finished \tANN training loss 0.146475\n",
      ">> Epoch 42 finished \tANN training loss 0.149572\n",
      ">> Epoch 43 finished \tANN training loss 0.147908\n",
      ">> Epoch 44 finished \tANN training loss 0.140923\n",
      ">> Epoch 45 finished \tANN training loss 0.156700\n",
      ">> Epoch 46 finished \tANN training loss 0.156605\n",
      ">> Epoch 47 finished \tANN training loss 0.146057\n",
      ">> Epoch 48 finished \tANN training loss 0.143269\n",
      ">> Epoch 49 finished \tANN training loss 0.120982\n",
      ">> Epoch 50 finished \tANN training loss 0.131375\n",
      ">> Epoch 51 finished \tANN training loss 0.143359\n",
      ">> Epoch 52 finished \tANN training loss 0.124684\n",
      ">> Epoch 53 finished \tANN training loss 0.116465\n",
      ">> Epoch 54 finished \tANN training loss 0.112622\n",
      ">> Epoch 55 finished \tANN training loss 0.113329\n",
      ">> Epoch 56 finished \tANN training loss 0.112779\n",
      ">> Epoch 57 finished \tANN training loss 0.118929\n",
      ">> Epoch 58 finished \tANN training loss 0.108284\n",
      ">> Epoch 59 finished \tANN training loss 0.127499\n",
      ">> Epoch 60 finished \tANN training loss 0.111992\n",
      ">> Epoch 61 finished \tANN training loss 0.100013\n",
      ">> Epoch 62 finished \tANN training loss 0.104605\n",
      ">> Epoch 63 finished \tANN training loss 0.096669\n",
      ">> Epoch 64 finished \tANN training loss 0.117072\n",
      ">> Epoch 65 finished \tANN training loss 0.087431\n",
      ">> Epoch 66 finished \tANN training loss 0.087742\n",
      ">> Epoch 67 finished \tANN training loss 0.099443\n",
      ">> Epoch 68 finished \tANN training loss 0.090154\n",
      ">> Epoch 69 finished \tANN training loss 0.099057\n",
      ">> Epoch 70 finished \tANN training loss 0.090347\n",
      ">> Epoch 71 finished \tANN training loss 0.096650\n",
      ">> Epoch 72 finished \tANN training loss 0.086776\n",
      ">> Epoch 73 finished \tANN training loss 0.102207\n",
      ">> Epoch 74 finished \tANN training loss 0.093259\n",
      ">> Epoch 75 finished \tANN training loss 0.082909\n",
      ">> Epoch 76 finished \tANN training loss 0.072957\n",
      ">> Epoch 77 finished \tANN training loss 0.084840\n",
      ">> Epoch 78 finished \tANN training loss 0.125031\n",
      ">> Epoch 79 finished \tANN training loss 0.078332\n",
      ">> Epoch 80 finished \tANN training loss 0.078219\n",
      ">> Epoch 81 finished \tANN training loss 0.082365\n",
      ">> Epoch 82 finished \tANN training loss 0.070416\n",
      ">> Epoch 83 finished \tANN training loss 0.069312\n",
      ">> Epoch 84 finished \tANN training loss 0.062472\n",
      ">> Epoch 85 finished \tANN training loss 0.059548\n",
      ">> Epoch 86 finished \tANN training loss 0.062699\n",
      ">> Epoch 87 finished \tANN training loss 0.065616\n",
      ">> Epoch 88 finished \tANN training loss 0.064328\n",
      ">> Epoch 89 finished \tANN training loss 0.077803\n",
      ">> Epoch 90 finished \tANN training loss 0.064773\n",
      ">> Epoch 91 finished \tANN training loss 0.062758\n",
      ">> Epoch 92 finished \tANN training loss 0.070808\n",
      ">> Epoch 93 finished \tANN training loss 0.067825\n",
      ">> Epoch 94 finished \tANN training loss 0.060696\n",
      ">> Epoch 95 finished \tANN training loss 0.054531\n",
      ">> Epoch 96 finished \tANN training loss 0.061266\n",
      ">> Epoch 97 finished \tANN training loss 0.055639\n",
      ">> Epoch 98 finished \tANN training loss 0.053449\n",
      ">> Epoch 99 finished \tANN training loss 0.054865\n",
      ">> Epoch 100 finished \tANN training loss 0.059920\n",
      ">> Epoch 101 finished \tANN training loss 0.047094\n",
      ">> Epoch 102 finished \tANN training loss 0.056061\n",
      ">> Epoch 103 finished \tANN training loss 0.041659\n",
      ">> Epoch 104 finished \tANN training loss 0.039601\n",
      ">> Epoch 105 finished \tANN training loss 0.041015\n",
      ">> Epoch 106 finished \tANN training loss 0.041679\n",
      ">> Epoch 107 finished \tANN training loss 0.052957\n",
      ">> Epoch 108 finished \tANN training loss 0.042886\n",
      ">> Epoch 109 finished \tANN training loss 0.055746\n",
      ">> Epoch 110 finished \tANN training loss 0.049758\n",
      ">> Epoch 111 finished \tANN training loss 0.078162\n",
      ">> Epoch 112 finished \tANN training loss 0.042956\n",
      ">> Epoch 113 finished \tANN training loss 0.044172\n",
      ">> Epoch 114 finished \tANN training loss 0.040429\n",
      ">> Epoch 115 finished \tANN training loss 0.038773\n",
      ">> Epoch 116 finished \tANN training loss 0.047436\n",
      ">> Epoch 117 finished \tANN training loss 0.039982\n",
      ">> Epoch 118 finished \tANN training loss 0.037526\n",
      ">> Epoch 119 finished \tANN training loss 0.043815\n",
      ">> Epoch 120 finished \tANN training loss 0.047208\n",
      ">> Epoch 121 finished \tANN training loss 0.047420\n",
      ">> Epoch 122 finished \tANN training loss 0.037782\n",
      ">> Epoch 123 finished \tANN training loss 0.038597\n",
      ">> Epoch 124 finished \tANN training loss 0.040984\n",
      ">> Epoch 125 finished \tANN training loss 0.038203\n",
      ">> Epoch 126 finished \tANN training loss 0.042413\n",
      ">> Epoch 127 finished \tANN training loss 0.058002\n",
      ">> Epoch 128 finished \tANN training loss 0.069099\n",
      ">> Epoch 129 finished \tANN training loss 0.038195\n",
      ">> Epoch 130 finished \tANN training loss 0.034833\n",
      ">> Epoch 131 finished \tANN training loss 0.050039\n",
      ">> Epoch 132 finished \tANN training loss 0.032198\n",
      ">> Epoch 133 finished \tANN training loss 0.032806\n",
      ">> Epoch 134 finished \tANN training loss 0.035826\n",
      ">> Epoch 135 finished \tANN training loss 0.034115\n",
      ">> Epoch 136 finished \tANN training loss 0.033309\n",
      ">> Epoch 137 finished \tANN training loss 0.037826\n",
      ">> Epoch 138 finished \tANN training loss 0.031967\n",
      ">> Epoch 139 finished \tANN training loss 0.050928\n",
      ">> Epoch 140 finished \tANN training loss 0.035950\n",
      ">> Epoch 141 finished \tANN training loss 0.038687\n",
      ">> Epoch 142 finished \tANN training loss 0.038423\n",
      ">> Epoch 143 finished \tANN training loss 0.026143\n",
      ">> Epoch 144 finished \tANN training loss 0.032866\n",
      ">> Epoch 145 finished \tANN training loss 0.044357\n",
      ">> Epoch 146 finished \tANN training loss 0.053065\n",
      ">> Epoch 147 finished \tANN training loss 0.034146\n",
      ">> Epoch 148 finished \tANN training loss 0.034509\n",
      ">> Epoch 149 finished \tANN training loss 0.033008\n",
      ">> Epoch 150 finished \tANN training loss 0.029078\n",
      ">> Epoch 151 finished \tANN training loss 0.030073\n",
      ">> Epoch 152 finished \tANN training loss 0.031162\n",
      ">> Epoch 153 finished \tANN training loss 0.025252\n",
      ">> Epoch 154 finished \tANN training loss 0.030818\n",
      ">> Epoch 155 finished \tANN training loss 0.028972\n",
      ">> Epoch 156 finished \tANN training loss 0.055153\n",
      ">> Epoch 157 finished \tANN training loss 0.053966\n",
      ">> Epoch 158 finished \tANN training loss 0.026701\n",
      ">> Epoch 159 finished \tANN training loss 0.021483\n",
      ">> Epoch 160 finished \tANN training loss 0.023398\n",
      ">> Epoch 161 finished \tANN training loss 0.023980\n",
      ">> Epoch 162 finished \tANN training loss 0.030006\n",
      ">> Epoch 163 finished \tANN training loss 0.025130\n",
      ">> Epoch 164 finished \tANN training loss 0.020221\n",
      ">> Epoch 165 finished \tANN training loss 0.023857\n",
      ">> Epoch 166 finished \tANN training loss 0.025396\n",
      ">> Epoch 167 finished \tANN training loss 0.023095\n",
      ">> Epoch 168 finished \tANN training loss 0.023504\n",
      ">> Epoch 169 finished \tANN training loss 0.033240\n",
      ">> Epoch 170 finished \tANN training loss 0.023914\n",
      ">> Epoch 171 finished \tANN training loss 0.030865\n",
      ">> Epoch 172 finished \tANN training loss 0.026780\n",
      ">> Epoch 173 finished \tANN training loss 0.029102\n",
      ">> Epoch 174 finished \tANN training loss 0.023204\n",
      ">> Epoch 175 finished \tANN training loss 0.020781\n",
      ">> Epoch 176 finished \tANN training loss 0.021201\n",
      ">> Epoch 177 finished \tANN training loss 0.024203\n",
      ">> Epoch 178 finished \tANN training loss 0.040209\n",
      ">> Epoch 179 finished \tANN training loss 0.022699\n",
      ">> Epoch 180 finished \tANN training loss 0.018327\n",
      ">> Epoch 181 finished \tANN training loss 0.016870\n",
      ">> Epoch 182 finished \tANN training loss 0.017271\n",
      ">> Epoch 183 finished \tANN training loss 0.021667\n",
      ">> Epoch 184 finished \tANN training loss 0.018431\n",
      ">> Epoch 185 finished \tANN training loss 0.018398\n",
      ">> Epoch 186 finished \tANN training loss 0.028745\n",
      ">> Epoch 187 finished \tANN training loss 0.024168\n",
      ">> Epoch 188 finished \tANN training loss 0.023506\n",
      ">> Epoch 189 finished \tANN training loss 0.022588\n",
      ">> Epoch 190 finished \tANN training loss 0.045077\n",
      ">> Epoch 191 finished \tANN training loss 0.024989\n",
      ">> Epoch 192 finished \tANN training loss 0.021045\n",
      ">> Epoch 193 finished \tANN training loss 0.019543\n",
      ">> Epoch 194 finished \tANN training loss 0.024277\n",
      ">> Epoch 195 finished \tANN training loss 0.027647\n",
      ">> Epoch 196 finished \tANN training loss 0.022406\n",
      ">> Epoch 197 finished \tANN training loss 0.022308\n",
      ">> Epoch 198 finished \tANN training loss 0.026424\n",
      ">> Epoch 199 finished \tANN training loss 0.022857\n",
      ">> Epoch 200 finished \tANN training loss 0.027303\n",
      ">> Epoch 201 finished \tANN training loss 0.018701\n",
      ">> Epoch 202 finished \tANN training loss 0.028689\n",
      ">> Epoch 203 finished \tANN training loss 0.028328\n",
      ">> Epoch 204 finished \tANN training loss 0.019557\n",
      ">> Epoch 205 finished \tANN training loss 0.021757\n",
      ">> Epoch 206 finished \tANN training loss 0.016091\n",
      ">> Epoch 207 finished \tANN training loss 0.016683\n",
      ">> Epoch 208 finished \tANN training loss 0.019424\n",
      ">> Epoch 209 finished \tANN training loss 0.021815\n",
      ">> Epoch 210 finished \tANN training loss 0.020305\n",
      ">> Epoch 211 finished \tANN training loss 0.022607\n",
      ">> Epoch 212 finished \tANN training loss 0.019716\n",
      ">> Epoch 213 finished \tANN training loss 0.023524\n",
      ">> Epoch 214 finished \tANN training loss 0.020986\n",
      ">> Epoch 215 finished \tANN training loss 0.021223\n",
      ">> Epoch 216 finished \tANN training loss 0.017916\n",
      ">> Epoch 217 finished \tANN training loss 0.020056\n",
      ">> Epoch 218 finished \tANN training loss 0.019728\n",
      ">> Epoch 219 finished \tANN training loss 0.020029\n",
      ">> Epoch 220 finished \tANN training loss 0.019671\n",
      ">> Epoch 221 finished \tANN training loss 0.014052\n",
      ">> Epoch 222 finished \tANN training loss 0.017501\n",
      ">> Epoch 223 finished \tANN training loss 0.018405\n",
      ">> Epoch 224 finished \tANN training loss 0.017138\n",
      ">> Epoch 225 finished \tANN training loss 0.021518\n",
      ">> Epoch 226 finished \tANN training loss 0.021363\n",
      ">> Epoch 227 finished \tANN training loss 0.019361\n",
      ">> Epoch 228 finished \tANN training loss 0.017228\n",
      ">> Epoch 229 finished \tANN training loss 0.019706\n",
      ">> Epoch 230 finished \tANN training loss 0.015981\n",
      ">> Epoch 231 finished \tANN training loss 0.022923\n",
      ">> Epoch 232 finished \tANN training loss 0.021466\n",
      ">> Epoch 233 finished \tANN training loss 0.021735\n",
      ">> Epoch 234 finished \tANN training loss 0.020668\n",
      ">> Epoch 235 finished \tANN training loss 0.023041\n",
      ">> Epoch 236 finished \tANN training loss 0.018899\n",
      ">> Epoch 237 finished \tANN training loss 0.018941\n",
      ">> Epoch 238 finished \tANN training loss 0.016777\n",
      ">> Epoch 239 finished \tANN training loss 0.017476\n",
      ">> Epoch 240 finished \tANN training loss 0.019910\n",
      ">> Epoch 241 finished \tANN training loss 0.020367\n",
      ">> Epoch 242 finished \tANN training loss 0.019003\n",
      ">> Epoch 243 finished \tANN training loss 0.018875\n",
      ">> Epoch 244 finished \tANN training loss 0.015891\n",
      ">> Epoch 245 finished \tANN training loss 0.012413\n",
      ">> Epoch 246 finished \tANN training loss 0.017008\n",
      ">> Epoch 247 finished \tANN training loss 0.019979\n",
      ">> Epoch 248 finished \tANN training loss 0.016364\n",
      ">> Epoch 249 finished \tANN training loss 0.014399\n",
      ">> Epoch 250 finished \tANN training loss 0.017345\n",
      ">> Epoch 251 finished \tANN training loss 0.016175\n",
      ">> Epoch 252 finished \tANN training loss 0.016963\n",
      ">> Epoch 253 finished \tANN training loss 0.017822\n",
      ">> Epoch 254 finished \tANN training loss 0.017813\n",
      ">> Epoch 255 finished \tANN training loss 0.016382\n",
      ">> Epoch 256 finished \tANN training loss 0.015935\n",
      ">> Epoch 257 finished \tANN training loss 0.013910\n",
      ">> Epoch 258 finished \tANN training loss 0.013476\n",
      ">> Epoch 259 finished \tANN training loss 0.017688\n",
      ">> Epoch 260 finished \tANN training loss 0.018152\n",
      ">> Epoch 261 finished \tANN training loss 0.021964\n",
      ">> Epoch 262 finished \tANN training loss 0.022527\n",
      ">> Epoch 263 finished \tANN training loss 0.016392\n",
      ">> Epoch 264 finished \tANN training loss 0.018548\n",
      ">> Epoch 265 finished \tANN training loss 0.023973\n",
      ">> Epoch 266 finished \tANN training loss 0.019632\n",
      ">> Epoch 267 finished \tANN training loss 0.019827\n",
      ">> Epoch 268 finished \tANN training loss 0.018812\n",
      ">> Epoch 269 finished \tANN training loss 0.019704\n",
      ">> Epoch 270 finished \tANN training loss 0.021578\n",
      ">> Epoch 271 finished \tANN training loss 0.024641\n",
      ">> Epoch 272 finished \tANN training loss 0.019674\n",
      ">> Epoch 273 finished \tANN training loss 0.016647\n",
      ">> Epoch 274 finished \tANN training loss 0.012705\n",
      ">> Epoch 275 finished \tANN training loss 0.014338\n",
      ">> Epoch 276 finished \tANN training loss 0.015227\n",
      ">> Epoch 277 finished \tANN training loss 0.012098\n",
      ">> Epoch 278 finished \tANN training loss 0.012266\n",
      ">> Epoch 279 finished \tANN training loss 0.011002\n",
      ">> Epoch 280 finished \tANN training loss 0.016273\n",
      ">> Epoch 281 finished \tANN training loss 0.013241\n",
      ">> Epoch 282 finished \tANN training loss 0.010531\n",
      ">> Epoch 283 finished \tANN training loss 0.015310\n",
      ">> Epoch 284 finished \tANN training loss 0.015304\n",
      ">> Epoch 285 finished \tANN training loss 0.009700\n",
      ">> Epoch 286 finished \tANN training loss 0.019987\n",
      ">> Epoch 287 finished \tANN training loss 0.018676\n",
      ">> Epoch 288 finished \tANN training loss 0.014521\n",
      ">> Epoch 289 finished \tANN training loss 0.012655\n",
      ">> Epoch 290 finished \tANN training loss 0.014325\n",
      ">> Epoch 291 finished \tANN training loss 0.011099\n",
      ">> Epoch 292 finished \tANN training loss 0.014441\n",
      ">> Epoch 293 finished \tANN training loss 0.023429\n",
      ">> Epoch 294 finished \tANN training loss 0.010254\n",
      ">> Epoch 295 finished \tANN training loss 0.012044\n",
      ">> Epoch 296 finished \tANN training loss 0.009145\n",
      ">> Epoch 297 finished \tANN training loss 0.008774\n",
      ">> Epoch 298 finished \tANN training loss 0.011442\n",
      ">> Epoch 299 finished \tANN training loss 0.007990\n",
      ">> Epoch 300 finished \tANN training loss 0.008897\n",
      ">> Epoch 301 finished \tANN training loss 0.015099\n",
      ">> Epoch 302 finished \tANN training loss 0.013298\n",
      ">> Epoch 303 finished \tANN training loss 0.010682\n",
      ">> Epoch 304 finished \tANN training loss 0.010541\n",
      ">> Epoch 305 finished \tANN training loss 0.010169\n",
      ">> Epoch 306 finished \tANN training loss 0.008929\n",
      ">> Epoch 307 finished \tANN training loss 0.008799\n",
      ">> Epoch 308 finished \tANN training loss 0.011876\n",
      ">> Epoch 309 finished \tANN training loss 0.011623\n",
      ">> Epoch 310 finished \tANN training loss 0.010765\n",
      ">> Epoch 311 finished \tANN training loss 0.008856\n",
      ">> Epoch 312 finished \tANN training loss 0.013432\n",
      ">> Epoch 313 finished \tANN training loss 0.010720\n",
      ">> Epoch 314 finished \tANN training loss 0.012897\n",
      ">> Epoch 315 finished \tANN training loss 0.014614\n",
      ">> Epoch 316 finished \tANN training loss 0.012745\n",
      ">> Epoch 317 finished \tANN training loss 0.012866\n",
      ">> Epoch 318 finished \tANN training loss 0.012643\n",
      ">> Epoch 319 finished \tANN training loss 0.010065\n",
      ">> Epoch 320 finished \tANN training loss 0.012132\n",
      ">> Epoch 321 finished \tANN training loss 0.029444\n",
      ">> Epoch 322 finished \tANN training loss 0.016058\n",
      ">> Epoch 323 finished \tANN training loss 0.012900\n",
      ">> Epoch 324 finished \tANN training loss 0.010389\n",
      ">> Epoch 325 finished \tANN training loss 0.009501\n",
      ">> Epoch 326 finished \tANN training loss 0.008137\n",
      ">> Epoch 327 finished \tANN training loss 0.011081\n",
      ">> Epoch 328 finished \tANN training loss 0.011366\n",
      ">> Epoch 329 finished \tANN training loss 0.008524\n",
      ">> Epoch 330 finished \tANN training loss 0.016767\n",
      ">> Epoch 331 finished \tANN training loss 0.013058\n",
      ">> Epoch 332 finished \tANN training loss 0.011548\n",
      ">> Epoch 333 finished \tANN training loss 0.014603\n",
      ">> Epoch 334 finished \tANN training loss 0.014445\n",
      ">> Epoch 335 finished \tANN training loss 0.014612\n",
      ">> Epoch 336 finished \tANN training loss 0.010259\n",
      ">> Epoch 337 finished \tANN training loss 0.011111\n",
      ">> Epoch 338 finished \tANN training loss 0.010186\n",
      ">> Epoch 339 finished \tANN training loss 0.008048\n",
      ">> Epoch 340 finished \tANN training loss 0.012225\n",
      ">> Epoch 341 finished \tANN training loss 0.013521\n",
      ">> Epoch 342 finished \tANN training loss 0.011592\n",
      ">> Epoch 343 finished \tANN training loss 0.009633\n",
      ">> Epoch 344 finished \tANN training loss 0.007531\n",
      ">> Epoch 345 finished \tANN training loss 0.007744\n",
      ">> Epoch 346 finished \tANN training loss 0.006834\n",
      ">> Epoch 347 finished \tANN training loss 0.010164\n",
      ">> Epoch 348 finished \tANN training loss 0.009408\n",
      ">> Epoch 349 finished \tANN training loss 0.006196\n",
      ">> Epoch 350 finished \tANN training loss 0.006256\n",
      ">> Epoch 351 finished \tANN training loss 0.006543\n",
      ">> Epoch 352 finished \tANN training loss 0.010702\n",
      ">> Epoch 353 finished \tANN training loss 0.007308\n",
      ">> Epoch 354 finished \tANN training loss 0.007392\n",
      ">> Epoch 355 finished \tANN training loss 0.006816\n",
      ">> Epoch 356 finished \tANN training loss 0.007575\n",
      ">> Epoch 357 finished \tANN training loss 0.012014\n",
      ">> Epoch 358 finished \tANN training loss 0.008639\n",
      ">> Epoch 359 finished \tANN training loss 0.009257\n",
      ">> Epoch 360 finished \tANN training loss 0.012237\n",
      ">> Epoch 361 finished \tANN training loss 0.010170\n",
      ">> Epoch 362 finished \tANN training loss 0.009908\n",
      ">> Epoch 363 finished \tANN training loss 0.009929\n",
      ">> Epoch 364 finished \tANN training loss 0.008499\n",
      ">> Epoch 365 finished \tANN training loss 0.011269\n",
      ">> Epoch 366 finished \tANN training loss 0.011088\n",
      ">> Epoch 367 finished \tANN training loss 0.009839\n",
      ">> Epoch 368 finished \tANN training loss 0.010640\n",
      ">> Epoch 369 finished \tANN training loss 0.013631\n",
      ">> Epoch 370 finished \tANN training loss 0.011684\n",
      ">> Epoch 371 finished \tANN training loss 0.012627\n",
      ">> Epoch 372 finished \tANN training loss 0.011136\n",
      ">> Epoch 373 finished \tANN training loss 0.010912\n",
      ">> Epoch 374 finished \tANN training loss 0.008364\n",
      ">> Epoch 375 finished \tANN training loss 0.008500\n",
      ">> Epoch 376 finished \tANN training loss 0.008107\n",
      ">> Epoch 377 finished \tANN training loss 0.009121\n",
      ">> Epoch 378 finished \tANN training loss 0.011688\n",
      ">> Epoch 379 finished \tANN training loss 0.011118\n",
      ">> Epoch 380 finished \tANN training loss 0.010963\n",
      ">> Epoch 381 finished \tANN training loss 0.008216\n",
      ">> Epoch 382 finished \tANN training loss 0.010312\n",
      ">> Epoch 383 finished \tANN training loss 0.009576\n",
      ">> Epoch 384 finished \tANN training loss 0.008415\n",
      ">> Epoch 385 finished \tANN training loss 0.008454\n",
      ">> Epoch 386 finished \tANN training loss 0.010471\n",
      ">> Epoch 387 finished \tANN training loss 0.009107\n",
      ">> Epoch 388 finished \tANN training loss 0.008069\n",
      ">> Epoch 389 finished \tANN training loss 0.008500\n",
      ">> Epoch 390 finished \tANN training loss 0.009133\n",
      ">> Epoch 391 finished \tANN training loss 0.009341\n",
      ">> Epoch 392 finished \tANN training loss 0.009371\n",
      ">> Epoch 393 finished \tANN training loss 0.008370\n",
      ">> Epoch 394 finished \tANN training loss 0.009258\n",
      ">> Epoch 395 finished \tANN training loss 0.011279\n",
      ">> Epoch 396 finished \tANN training loss 0.008843\n",
      ">> Epoch 397 finished \tANN training loss 0.009096\n",
      ">> Epoch 398 finished \tANN training loss 0.009931\n",
      ">> Epoch 399 finished \tANN training loss 0.011944\n",
      ">> Epoch 400 finished \tANN training loss 0.009016\n",
      ">> Epoch 401 finished \tANN training loss 0.011544\n",
      ">> Epoch 402 finished \tANN training loss 0.011550\n",
      ">> Epoch 403 finished \tANN training loss 0.012188\n",
      ">> Epoch 404 finished \tANN training loss 0.010255\n",
      ">> Epoch 405 finished \tANN training loss 0.014805\n",
      ">> Epoch 406 finished \tANN training loss 0.009373\n",
      ">> Epoch 407 finished \tANN training loss 0.007577\n",
      ">> Epoch 408 finished \tANN training loss 0.009326\n",
      ">> Epoch 409 finished \tANN training loss 0.011880\n",
      ">> Epoch 410 finished \tANN training loss 0.010931\n",
      ">> Epoch 411 finished \tANN training loss 0.007425\n",
      ">> Epoch 412 finished \tANN training loss 0.008320\n",
      ">> Epoch 413 finished \tANN training loss 0.008261\n",
      ">> Epoch 414 finished \tANN training loss 0.007194\n",
      ">> Epoch 415 finished \tANN training loss 0.006003\n",
      ">> Epoch 416 finished \tANN training loss 0.008572\n",
      ">> Epoch 417 finished \tANN training loss 0.008107\n",
      ">> Epoch 418 finished \tANN training loss 0.007812\n",
      ">> Epoch 419 finished \tANN training loss 0.007538\n",
      ">> Epoch 420 finished \tANN training loss 0.008091\n",
      ">> Epoch 421 finished \tANN training loss 0.008243\n",
      ">> Epoch 422 finished \tANN training loss 0.006490\n",
      ">> Epoch 423 finished \tANN training loss 0.012545\n",
      ">> Epoch 424 finished \tANN training loss 0.009048\n",
      ">> Epoch 425 finished \tANN training loss 0.008635\n",
      ">> Epoch 426 finished \tANN training loss 0.008657\n",
      ">> Epoch 427 finished \tANN training loss 0.009899\n",
      ">> Epoch 428 finished \tANN training loss 0.010566\n",
      ">> Epoch 429 finished \tANN training loss 0.010759\n",
      ">> Epoch 430 finished \tANN training loss 0.010446\n",
      ">> Epoch 431 finished \tANN training loss 0.008428\n",
      ">> Epoch 432 finished \tANN training loss 0.007181\n",
      ">> Epoch 433 finished \tANN training loss 0.006820\n",
      ">> Epoch 434 finished \tANN training loss 0.006789\n",
      ">> Epoch 435 finished \tANN training loss 0.004779\n",
      ">> Epoch 436 finished \tANN training loss 0.005489\n",
      ">> Epoch 437 finished \tANN training loss 0.005199\n",
      ">> Epoch 438 finished \tANN training loss 0.006283\n",
      ">> Epoch 439 finished \tANN training loss 0.008756\n",
      ">> Epoch 440 finished \tANN training loss 0.017806\n",
      ">> Epoch 441 finished \tANN training loss 0.006086\n",
      ">> Epoch 442 finished \tANN training loss 0.006707\n",
      ">> Epoch 443 finished \tANN training loss 0.008503\n",
      ">> Epoch 444 finished \tANN training loss 0.009773\n",
      ">> Epoch 445 finished \tANN training loss 0.008839\n",
      ">> Epoch 446 finished \tANN training loss 0.012126\n",
      ">> Epoch 447 finished \tANN training loss 0.007847\n",
      ">> Epoch 448 finished \tANN training loss 0.006980\n",
      ">> Epoch 449 finished \tANN training loss 0.007260\n",
      ">> Epoch 450 finished \tANN training loss 0.009407\n",
      ">> Epoch 451 finished \tANN training loss 0.008967\n",
      ">> Epoch 452 finished \tANN training loss 0.008786\n",
      ">> Epoch 453 finished \tANN training loss 0.007700\n",
      ">> Epoch 454 finished \tANN training loss 0.009581\n",
      ">> Epoch 455 finished \tANN training loss 0.007866\n",
      ">> Epoch 456 finished \tANN training loss 0.006044\n",
      ">> Epoch 457 finished \tANN training loss 0.005889\n",
      ">> Epoch 458 finished \tANN training loss 0.005508\n",
      ">> Epoch 459 finished \tANN training loss 0.005903\n",
      ">> Epoch 460 finished \tANN training loss 0.005778\n",
      ">> Epoch 461 finished \tANN training loss 0.005889\n",
      ">> Epoch 462 finished \tANN training loss 0.004949\n",
      ">> Epoch 463 finished \tANN training loss 0.012668\n",
      ">> Epoch 464 finished \tANN training loss 0.005641\n",
      ">> Epoch 465 finished \tANN training loss 0.007261\n",
      ">> Epoch 466 finished \tANN training loss 0.008702\n",
      ">> Epoch 467 finished \tANN training loss 0.008660\n",
      ">> Epoch 468 finished \tANN training loss 0.007664\n",
      ">> Epoch 469 finished \tANN training loss 0.005736\n",
      ">> Epoch 470 finished \tANN training loss 0.007367\n",
      ">> Epoch 471 finished \tANN training loss 0.005389\n",
      ">> Epoch 472 finished \tANN training loss 0.008002\n",
      ">> Epoch 473 finished \tANN training loss 0.007290\n",
      ">> Epoch 474 finished \tANN training loss 0.008080\n",
      ">> Epoch 475 finished \tANN training loss 0.008355\n",
      ">> Epoch 476 finished \tANN training loss 0.006855\n",
      ">> Epoch 477 finished \tANN training loss 0.007027\n",
      ">> Epoch 478 finished \tANN training loss 0.010195\n",
      ">> Epoch 479 finished \tANN training loss 0.006565\n",
      ">> Epoch 480 finished \tANN training loss 0.010571\n",
      ">> Epoch 481 finished \tANN training loss 0.007607\n",
      ">> Epoch 482 finished \tANN training loss 0.009402\n",
      ">> Epoch 483 finished \tANN training loss 0.009864\n",
      ">> Epoch 484 finished \tANN training loss 0.008872\n",
      ">> Epoch 485 finished \tANN training loss 0.005806\n",
      ">> Epoch 486 finished \tANN training loss 0.005491\n",
      ">> Epoch 487 finished \tANN training loss 0.009062\n",
      ">> Epoch 488 finished \tANN training loss 0.007344\n",
      ">> Epoch 489 finished \tANN training loss 0.008567\n",
      ">> Epoch 490 finished \tANN training loss 0.008704\n",
      ">> Epoch 491 finished \tANN training loss 0.006657\n",
      ">> Epoch 492 finished \tANN training loss 0.006249\n",
      ">> Epoch 493 finished \tANN training loss 0.009010\n",
      ">> Epoch 494 finished \tANN training loss 0.008418\n",
      ">> Epoch 495 finished \tANN training loss 0.008370\n",
      ">> Epoch 496 finished \tANN training loss 0.007665\n",
      ">> Epoch 497 finished \tANN training loss 0.008747\n",
      ">> Epoch 498 finished \tANN training loss 0.006989\n",
      ">> Epoch 499 finished \tANN training loss 0.007900\n",
      ">> Epoch 500 finished \tANN training loss 0.005570\n",
      ">> Epoch 501 finished \tANN training loss 0.007178\n",
      ">> Epoch 502 finished \tANN training loss 0.006298\n",
      ">> Epoch 503 finished \tANN training loss 0.007087\n",
      ">> Epoch 504 finished \tANN training loss 0.006224\n",
      ">> Epoch 505 finished \tANN training loss 0.005879\n",
      ">> Epoch 506 finished \tANN training loss 0.005334\n",
      ">> Epoch 507 finished \tANN training loss 0.007600\n",
      ">> Epoch 508 finished \tANN training loss 0.005311\n",
      ">> Epoch 509 finished \tANN training loss 0.008170\n",
      ">> Epoch 510 finished \tANN training loss 0.008288\n",
      ">> Epoch 511 finished \tANN training loss 0.008251\n",
      ">> Epoch 512 finished \tANN training loss 0.007033\n",
      ">> Epoch 513 finished \tANN training loss 0.006629\n",
      ">> Epoch 514 finished \tANN training loss 0.006701\n",
      ">> Epoch 515 finished \tANN training loss 0.009174\n",
      ">> Epoch 516 finished \tANN training loss 0.007954\n",
      ">> Epoch 517 finished \tANN training loss 0.008438\n",
      ">> Epoch 518 finished \tANN training loss 0.006458\n",
      ">> Epoch 519 finished \tANN training loss 0.006109\n",
      ">> Epoch 520 finished \tANN training loss 0.007412\n",
      ">> Epoch 521 finished \tANN training loss 0.006003\n",
      ">> Epoch 522 finished \tANN training loss 0.007997\n",
      ">> Epoch 523 finished \tANN training loss 0.005125\n",
      ">> Epoch 524 finished \tANN training loss 0.004908\n",
      ">> Epoch 525 finished \tANN training loss 0.007590\n",
      ">> Epoch 526 finished \tANN training loss 0.008011\n",
      ">> Epoch 527 finished \tANN training loss 0.005882\n",
      ">> Epoch 528 finished \tANN training loss 0.008307\n",
      ">> Epoch 529 finished \tANN training loss 0.006306\n",
      ">> Epoch 530 finished \tANN training loss 0.007936\n",
      ">> Epoch 531 finished \tANN training loss 0.006711\n",
      ">> Epoch 532 finished \tANN training loss 0.007564\n",
      ">> Epoch 533 finished \tANN training loss 0.007964\n",
      ">> Epoch 534 finished \tANN training loss 0.006343\n",
      ">> Epoch 535 finished \tANN training loss 0.006443\n",
      ">> Epoch 536 finished \tANN training loss 0.007619\n",
      ">> Epoch 537 finished \tANN training loss 0.005136\n",
      ">> Epoch 538 finished \tANN training loss 0.005253\n",
      ">> Epoch 539 finished \tANN training loss 0.006061\n",
      ">> Epoch 540 finished \tANN training loss 0.004871\n",
      ">> Epoch 541 finished \tANN training loss 0.004714\n",
      ">> Epoch 542 finished \tANN training loss 0.011886\n",
      ">> Epoch 543 finished \tANN training loss 0.008428\n",
      ">> Epoch 544 finished \tANN training loss 0.008015\n",
      ">> Epoch 545 finished \tANN training loss 0.009616\n",
      ">> Epoch 546 finished \tANN training loss 0.006324\n",
      ">> Epoch 547 finished \tANN training loss 0.006844\n",
      ">> Epoch 548 finished \tANN training loss 0.006045\n",
      ">> Epoch 549 finished \tANN training loss 0.006155\n",
      ">> Epoch 550 finished \tANN training loss 0.005156\n",
      ">> Epoch 551 finished \tANN training loss 0.005824\n",
      ">> Epoch 552 finished \tANN training loss 0.004802\n",
      ">> Epoch 553 finished \tANN training loss 0.004649\n",
      ">> Epoch 554 finished \tANN training loss 0.004485\n",
      ">> Epoch 555 finished \tANN training loss 0.006095\n",
      ">> Epoch 556 finished \tANN training loss 0.007314\n",
      ">> Epoch 557 finished \tANN training loss 0.006849\n",
      ">> Epoch 558 finished \tANN training loss 0.006414\n",
      ">> Epoch 559 finished \tANN training loss 0.008340\n",
      ">> Epoch 560 finished \tANN training loss 0.006063\n",
      ">> Epoch 561 finished \tANN training loss 0.005634\n",
      ">> Epoch 562 finished \tANN training loss 0.005053\n",
      ">> Epoch 563 finished \tANN training loss 0.003795\n",
      ">> Epoch 564 finished \tANN training loss 0.004133\n",
      ">> Epoch 565 finished \tANN training loss 0.006163\n",
      ">> Epoch 566 finished \tANN training loss 0.005472\n",
      ">> Epoch 567 finished \tANN training loss 0.005883\n",
      ">> Epoch 568 finished \tANN training loss 0.004855\n",
      ">> Epoch 569 finished \tANN training loss 0.005344\n",
      ">> Epoch 570 finished \tANN training loss 0.005575\n",
      ">> Epoch 571 finished \tANN training loss 0.004798\n",
      ">> Epoch 572 finished \tANN training loss 0.007082\n",
      ">> Epoch 573 finished \tANN training loss 0.004736\n",
      ">> Epoch 574 finished \tANN training loss 0.005118\n",
      ">> Epoch 575 finished \tANN training loss 0.006320\n",
      ">> Epoch 576 finished \tANN training loss 0.007901\n",
      ">> Epoch 577 finished \tANN training loss 0.005234\n",
      ">> Epoch 578 finished \tANN training loss 0.005596\n",
      ">> Epoch 579 finished \tANN training loss 0.006118\n",
      ">> Epoch 580 finished \tANN training loss 0.006995\n",
      ">> Epoch 581 finished \tANN training loss 0.006819\n",
      ">> Epoch 582 finished \tANN training loss 0.005273\n",
      ">> Epoch 583 finished \tANN training loss 0.005842\n",
      ">> Epoch 584 finished \tANN training loss 0.007106\n",
      ">> Epoch 585 finished \tANN training loss 0.006465\n",
      ">> Epoch 586 finished \tANN training loss 0.006985\n",
      ">> Epoch 587 finished \tANN training loss 0.006624\n",
      ">> Epoch 588 finished \tANN training loss 0.006259\n",
      ">> Epoch 589 finished \tANN training loss 0.007664\n",
      ">> Epoch 590 finished \tANN training loss 0.008611\n",
      ">> Epoch 591 finished \tANN training loss 0.005726\n",
      ">> Epoch 592 finished \tANN training loss 0.006526\n",
      ">> Epoch 593 finished \tANN training loss 0.005760\n",
      ">> Epoch 594 finished \tANN training loss 0.006561\n",
      ">> Epoch 595 finished \tANN training loss 0.005077\n",
      ">> Epoch 596 finished \tANN training loss 0.004842\n",
      ">> Epoch 597 finished \tANN training loss 0.005098\n",
      ">> Epoch 598 finished \tANN training loss 0.009212\n",
      ">> Epoch 599 finished \tANN training loss 0.005122\n",
      ">> Epoch 600 finished \tANN training loss 0.004428\n",
      ">> Epoch 601 finished \tANN training loss 0.005459\n",
      ">> Epoch 602 finished \tANN training loss 0.004980\n",
      ">> Epoch 603 finished \tANN training loss 0.004413\n",
      ">> Epoch 604 finished \tANN training loss 0.004810\n",
      ">> Epoch 605 finished \tANN training loss 0.004884\n",
      ">> Epoch 606 finished \tANN training loss 0.005357\n",
      ">> Epoch 607 finished \tANN training loss 0.005229\n",
      ">> Epoch 608 finished \tANN training loss 0.003690\n",
      ">> Epoch 609 finished \tANN training loss 0.005295\n",
      ">> Epoch 610 finished \tANN training loss 0.005915\n",
      ">> Epoch 611 finished \tANN training loss 0.005552\n",
      ">> Epoch 612 finished \tANN training loss 0.004578\n",
      ">> Epoch 613 finished \tANN training loss 0.006187\n",
      ">> Epoch 614 finished \tANN training loss 0.005343\n",
      ">> Epoch 615 finished \tANN training loss 0.004085\n",
      ">> Epoch 616 finished \tANN training loss 0.005373\n",
      ">> Epoch 617 finished \tANN training loss 0.004354\n",
      ">> Epoch 618 finished \tANN training loss 0.005143\n",
      ">> Epoch 619 finished \tANN training loss 0.004096\n",
      ">> Epoch 620 finished \tANN training loss 0.004622\n",
      ">> Epoch 621 finished \tANN training loss 0.004291\n",
      ">> Epoch 622 finished \tANN training loss 0.004704\n",
      ">> Epoch 623 finished \tANN training loss 0.003472\n",
      ">> Epoch 624 finished \tANN training loss 0.003601\n",
      ">> Epoch 625 finished \tANN training loss 0.004323\n",
      ">> Epoch 626 finished \tANN training loss 0.003568\n",
      ">> Epoch 627 finished \tANN training loss 0.003574\n",
      ">> Epoch 628 finished \tANN training loss 0.005010\n",
      ">> Epoch 629 finished \tANN training loss 0.004645\n",
      ">> Epoch 630 finished \tANN training loss 0.005048\n",
      ">> Epoch 631 finished \tANN training loss 0.006924\n",
      ">> Epoch 632 finished \tANN training loss 0.004702\n",
      ">> Epoch 633 finished \tANN training loss 0.004371\n",
      ">> Epoch 634 finished \tANN training loss 0.005458\n",
      ">> Epoch 635 finished \tANN training loss 0.005437\n",
      ">> Epoch 636 finished \tANN training loss 0.004185\n",
      ">> Epoch 637 finished \tANN training loss 0.004602\n",
      ">> Epoch 638 finished \tANN training loss 0.003502\n",
      ">> Epoch 639 finished \tANN training loss 0.005068\n",
      ">> Epoch 640 finished \tANN training loss 0.005060\n",
      ">> Epoch 641 finished \tANN training loss 0.006914\n",
      ">> Epoch 642 finished \tANN training loss 0.008151\n",
      ">> Epoch 643 finished \tANN training loss 0.007495\n",
      ">> Epoch 644 finished \tANN training loss 0.006527\n",
      ">> Epoch 645 finished \tANN training loss 0.006065\n",
      ">> Epoch 646 finished \tANN training loss 0.005890\n",
      ">> Epoch 647 finished \tANN training loss 0.005638\n",
      ">> Epoch 648 finished \tANN training loss 0.005144\n",
      ">> Epoch 649 finished \tANN training loss 0.005571\n",
      ">> Epoch 650 finished \tANN training loss 0.006787\n",
      ">> Epoch 651 finished \tANN training loss 0.006005\n",
      ">> Epoch 652 finished \tANN training loss 0.004919\n",
      ">> Epoch 653 finished \tANN training loss 0.004533\n",
      ">> Epoch 654 finished \tANN training loss 0.004197\n",
      ">> Epoch 655 finished \tANN training loss 0.005321\n",
      ">> Epoch 656 finished \tANN training loss 0.004414\n",
      ">> Epoch 657 finished \tANN training loss 0.004492\n",
      ">> Epoch 658 finished \tANN training loss 0.004390\n",
      ">> Epoch 659 finished \tANN training loss 0.004679\n",
      ">> Epoch 660 finished \tANN training loss 0.006076\n",
      ">> Epoch 661 finished \tANN training loss 0.005487\n",
      ">> Epoch 662 finished \tANN training loss 0.005472\n",
      ">> Epoch 663 finished \tANN training loss 0.007264\n",
      ">> Epoch 664 finished \tANN training loss 0.006429\n",
      ">> Epoch 665 finished \tANN training loss 0.006499\n",
      ">> Epoch 666 finished \tANN training loss 0.006651\n",
      ">> Epoch 667 finished \tANN training loss 0.004592\n",
      ">> Epoch 668 finished \tANN training loss 0.004340\n",
      ">> Epoch 669 finished \tANN training loss 0.006198\n",
      ">> Epoch 670 finished \tANN training loss 0.006162\n",
      ">> Epoch 671 finished \tANN training loss 0.004746\n",
      ">> Epoch 672 finished \tANN training loss 0.007342\n",
      ">> Epoch 673 finished \tANN training loss 0.009322\n",
      ">> Epoch 674 finished \tANN training loss 0.006189\n",
      ">> Epoch 675 finished \tANN training loss 0.003947\n",
      ">> Epoch 676 finished \tANN training loss 0.005210\n",
      ">> Epoch 677 finished \tANN training loss 0.006396\n",
      ">> Epoch 678 finished \tANN training loss 0.006074\n",
      ">> Epoch 679 finished \tANN training loss 0.005854\n",
      ">> Epoch 680 finished \tANN training loss 0.005053\n",
      ">> Epoch 681 finished \tANN training loss 0.004890\n",
      ">> Epoch 682 finished \tANN training loss 0.006061\n",
      ">> Epoch 683 finished \tANN training loss 0.008675\n",
      ">> Epoch 684 finished \tANN training loss 0.004055\n",
      ">> Epoch 685 finished \tANN training loss 0.004236\n",
      ">> Epoch 686 finished \tANN training loss 0.004320\n",
      ">> Epoch 687 finished \tANN training loss 0.005254\n",
      ">> Epoch 688 finished \tANN training loss 0.004249\n",
      ">> Epoch 689 finished \tANN training loss 0.011422\n",
      ">> Epoch 690 finished \tANN training loss 0.004285\n",
      ">> Epoch 691 finished \tANN training loss 0.004599\n",
      ">> Epoch 692 finished \tANN training loss 0.004691\n",
      ">> Epoch 693 finished \tANN training loss 0.006321\n",
      ">> Epoch 694 finished \tANN training loss 0.005343\n",
      ">> Epoch 695 finished \tANN training loss 0.004388\n",
      ">> Epoch 696 finished \tANN training loss 0.004971\n",
      ">> Epoch 697 finished \tANN training loss 0.003525\n",
      ">> Epoch 698 finished \tANN training loss 0.004300\n",
      ">> Epoch 699 finished \tANN training loss 0.004473\n",
      ">> Epoch 700 finished \tANN training loss 0.004512\n",
      ">> Epoch 701 finished \tANN training loss 0.004129\n",
      ">> Epoch 702 finished \tANN training loss 0.005237\n",
      ">> Epoch 703 finished \tANN training loss 0.003878\n",
      ">> Epoch 704 finished \tANN training loss 0.003996\n",
      ">> Epoch 705 finished \tANN training loss 0.003693\n",
      ">> Epoch 706 finished \tANN training loss 0.004421\n",
      ">> Epoch 707 finished \tANN training loss 0.004740\n",
      ">> Epoch 708 finished \tANN training loss 0.004350\n",
      ">> Epoch 709 finished \tANN training loss 0.004434\n",
      ">> Epoch 710 finished \tANN training loss 0.007777\n",
      ">> Epoch 711 finished \tANN training loss 0.004871\n",
      ">> Epoch 712 finished \tANN training loss 0.004981\n",
      ">> Epoch 713 finished \tANN training loss 0.004517\n",
      ">> Epoch 714 finished \tANN training loss 0.006069\n",
      ">> Epoch 715 finished \tANN training loss 0.005330\n",
      ">> Epoch 716 finished \tANN training loss 0.005518\n",
      ">> Epoch 717 finished \tANN training loss 0.004424\n",
      ">> Epoch 718 finished \tANN training loss 0.004488\n",
      ">> Epoch 719 finished \tANN training loss 0.003219\n",
      ">> Epoch 720 finished \tANN training loss 0.003889\n",
      ">> Epoch 721 finished \tANN training loss 0.003650\n",
      ">> Epoch 722 finished \tANN training loss 0.003753\n",
      ">> Epoch 723 finished \tANN training loss 0.004228\n",
      ">> Epoch 724 finished \tANN training loss 0.004136\n",
      ">> Epoch 725 finished \tANN training loss 0.004599\n",
      ">> Epoch 726 finished \tANN training loss 0.003447\n",
      ">> Epoch 727 finished \tANN training loss 0.003487\n",
      ">> Epoch 728 finished \tANN training loss 0.002860\n",
      ">> Epoch 729 finished \tANN training loss 0.002489\n",
      ">> Epoch 730 finished \tANN training loss 0.002836\n",
      ">> Epoch 731 finished \tANN training loss 0.003523\n",
      ">> Epoch 732 finished \tANN training loss 0.004593\n",
      ">> Epoch 733 finished \tANN training loss 0.006178\n",
      ">> Epoch 734 finished \tANN training loss 0.004931\n",
      ">> Epoch 735 finished \tANN training loss 0.006033\n",
      ">> Epoch 736 finished \tANN training loss 0.006251\n",
      ">> Epoch 737 finished \tANN training loss 0.005093\n",
      ">> Epoch 738 finished \tANN training loss 0.006211\n",
      ">> Epoch 739 finished \tANN training loss 0.006039\n",
      ">> Epoch 740 finished \tANN training loss 0.004145\n",
      ">> Epoch 741 finished \tANN training loss 0.005080\n",
      ">> Epoch 742 finished \tANN training loss 0.004319\n",
      ">> Epoch 743 finished \tANN training loss 0.006911\n",
      ">> Epoch 744 finished \tANN training loss 0.005316\n",
      ">> Epoch 745 finished \tANN training loss 0.004383\n",
      ">> Epoch 746 finished \tANN training loss 0.005132\n",
      ">> Epoch 747 finished \tANN training loss 0.004180\n",
      ">> Epoch 748 finished \tANN training loss 0.004722\n",
      ">> Epoch 749 finished \tANN training loss 0.005764\n",
      ">> Epoch 750 finished \tANN training loss 0.004443\n",
      ">> Epoch 751 finished \tANN training loss 0.005684\n",
      ">> Epoch 752 finished \tANN training loss 0.004146\n",
      ">> Epoch 753 finished \tANN training loss 0.003657\n",
      ">> Epoch 754 finished \tANN training loss 0.005754\n",
      ">> Epoch 755 finished \tANN training loss 0.006369\n",
      ">> Epoch 756 finished \tANN training loss 0.004711\n",
      ">> Epoch 757 finished \tANN training loss 0.008495\n",
      ">> Epoch 758 finished \tANN training loss 0.005670\n",
      ">> Epoch 759 finished \tANN training loss 0.005026\n",
      ">> Epoch 760 finished \tANN training loss 0.005373\n",
      ">> Epoch 761 finished \tANN training loss 0.005539\n",
      ">> Epoch 762 finished \tANN training loss 0.006734\n",
      ">> Epoch 763 finished \tANN training loss 0.006210\n",
      ">> Epoch 764 finished \tANN training loss 0.008440\n",
      ">> Epoch 765 finished \tANN training loss 0.006213\n",
      ">> Epoch 766 finished \tANN training loss 0.004226\n",
      ">> Epoch 767 finished \tANN training loss 0.003831\n",
      ">> Epoch 768 finished \tANN training loss 0.004423\n",
      ">> Epoch 769 finished \tANN training loss 0.005272\n",
      ">> Epoch 770 finished \tANN training loss 0.004543\n",
      ">> Epoch 771 finished \tANN training loss 0.004285\n",
      ">> Epoch 772 finished \tANN training loss 0.010359\n",
      ">> Epoch 773 finished \tANN training loss 0.005780\n",
      ">> Epoch 774 finished \tANN training loss 0.006104\n",
      ">> Epoch 775 finished \tANN training loss 0.005106\n",
      ">> Epoch 776 finished \tANN training loss 0.003360\n",
      ">> Epoch 777 finished \tANN training loss 0.003371\n",
      ">> Epoch 778 finished \tANN training loss 0.005106\n",
      ">> Epoch 779 finished \tANN training loss 0.004290\n",
      ">> Epoch 780 finished \tANN training loss 0.003464\n",
      ">> Epoch 781 finished \tANN training loss 0.003758\n",
      ">> Epoch 782 finished \tANN training loss 0.004473\n",
      ">> Epoch 783 finished \tANN training loss 0.003417\n",
      ">> Epoch 784 finished \tANN training loss 0.003720\n",
      ">> Epoch 785 finished \tANN training loss 0.005481\n",
      ">> Epoch 786 finished \tANN training loss 0.004493\n",
      ">> Epoch 787 finished \tANN training loss 0.004871\n",
      ">> Epoch 788 finished \tANN training loss 0.004998\n",
      ">> Epoch 789 finished \tANN training loss 0.005092\n",
      ">> Epoch 790 finished \tANN training loss 0.006035\n",
      ">> Epoch 791 finished \tANN training loss 0.004161\n",
      ">> Epoch 792 finished \tANN training loss 0.003978\n",
      ">> Epoch 793 finished \tANN training loss 0.003586\n",
      ">> Epoch 794 finished \tANN training loss 0.003725\n",
      ">> Epoch 795 finished \tANN training loss 0.004373\n",
      ">> Epoch 796 finished \tANN training loss 0.003321\n",
      ">> Epoch 797 finished \tANN training loss 0.003765\n",
      ">> Epoch 798 finished \tANN training loss 0.003285\n",
      ">> Epoch 799 finished \tANN training loss 0.003852\n",
      ">> Epoch 800 finished \tANN training loss 0.003420\n",
      ">> Epoch 801 finished \tANN training loss 0.003464\n",
      ">> Epoch 802 finished \tANN training loss 0.003346\n",
      ">> Epoch 803 finished \tANN training loss 0.003050\n",
      ">> Epoch 804 finished \tANN training loss 0.003211\n",
      ">> Epoch 805 finished \tANN training loss 0.004137\n",
      ">> Epoch 806 finished \tANN training loss 0.004000\n",
      ">> Epoch 807 finished \tANN training loss 0.004525\n",
      ">> Epoch 808 finished \tANN training loss 0.005360\n",
      ">> Epoch 809 finished \tANN training loss 0.004199\n",
      ">> Epoch 810 finished \tANN training loss 0.003340\n",
      ">> Epoch 811 finished \tANN training loss 0.003711\n",
      ">> Epoch 812 finished \tANN training loss 0.004031\n",
      ">> Epoch 813 finished \tANN training loss 0.005308\n",
      ">> Epoch 814 finished \tANN training loss 0.004770\n",
      ">> Epoch 815 finished \tANN training loss 0.008561\n",
      ">> Epoch 816 finished \tANN training loss 0.005172\n",
      ">> Epoch 817 finished \tANN training loss 0.005959\n",
      ">> Epoch 818 finished \tANN training loss 0.004999\n",
      ">> Epoch 819 finished \tANN training loss 0.004320\n",
      ">> Epoch 820 finished \tANN training loss 0.004503\n",
      ">> Epoch 821 finished \tANN training loss 0.004104\n",
      ">> Epoch 822 finished \tANN training loss 0.005630\n",
      ">> Epoch 823 finished \tANN training loss 0.005957\n",
      ">> Epoch 824 finished \tANN training loss 0.005524\n",
      ">> Epoch 825 finished \tANN training loss 0.005897\n",
      ">> Epoch 826 finished \tANN training loss 0.004415\n",
      ">> Epoch 827 finished \tANN training loss 0.005026\n",
      ">> Epoch 828 finished \tANN training loss 0.006257\n",
      ">> Epoch 829 finished \tANN training loss 0.006416\n",
      ">> Epoch 830 finished \tANN training loss 0.006499\n",
      ">> Epoch 831 finished \tANN training loss 0.004827\n",
      ">> Epoch 832 finished \tANN training loss 0.004317\n",
      ">> Epoch 833 finished \tANN training loss 0.005328\n",
      ">> Epoch 834 finished \tANN training loss 0.005190\n",
      ">> Epoch 835 finished \tANN training loss 0.004242\n",
      ">> Epoch 836 finished \tANN training loss 0.003773\n",
      ">> Epoch 837 finished \tANN training loss 0.003224\n",
      ">> Epoch 838 finished \tANN training loss 0.003339\n",
      ">> Epoch 839 finished \tANN training loss 0.003216\n",
      ">> Epoch 840 finished \tANN training loss 0.003567\n",
      ">> Epoch 841 finished \tANN training loss 0.004818\n",
      ">> Epoch 842 finished \tANN training loss 0.003079\n",
      ">> Epoch 843 finished \tANN training loss 0.002980\n",
      ">> Epoch 844 finished \tANN training loss 0.004500\n",
      ">> Epoch 845 finished \tANN training loss 0.005380\n",
      ">> Epoch 846 finished \tANN training loss 0.004091\n",
      ">> Epoch 847 finished \tANN training loss 0.003253\n",
      ">> Epoch 848 finished \tANN training loss 0.004327\n",
      ">> Epoch 849 finished \tANN training loss 0.003753\n",
      ">> Epoch 850 finished \tANN training loss 0.003696\n",
      ">> Epoch 851 finished \tANN training loss 0.003906\n",
      ">> Epoch 852 finished \tANN training loss 0.004080\n",
      ">> Epoch 853 finished \tANN training loss 0.004327\n",
      ">> Epoch 854 finished \tANN training loss 0.004184\n",
      ">> Epoch 855 finished \tANN training loss 0.003258\n",
      ">> Epoch 856 finished \tANN training loss 0.004240\n",
      ">> Epoch 857 finished \tANN training loss 0.003606\n",
      ">> Epoch 858 finished \tANN training loss 0.004930\n",
      ">> Epoch 859 finished \tANN training loss 0.003632\n",
      ">> Epoch 860 finished \tANN training loss 0.002968\n",
      ">> Epoch 861 finished \tANN training loss 0.003381\n",
      ">> Epoch 862 finished \tANN training loss 0.003473\n",
      ">> Epoch 863 finished \tANN training loss 0.003462\n",
      ">> Epoch 864 finished \tANN training loss 0.003163\n",
      ">> Epoch 865 finished \tANN training loss 0.003863\n",
      ">> Epoch 866 finished \tANN training loss 0.003662\n",
      ">> Epoch 867 finished \tANN training loss 0.003581\n",
      ">> Epoch 868 finished \tANN training loss 0.003277\n",
      ">> Epoch 869 finished \tANN training loss 0.003130\n",
      ">> Epoch 870 finished \tANN training loss 0.004725\n",
      ">> Epoch 871 finished \tANN training loss 0.005051\n",
      ">> Epoch 872 finished \tANN training loss 0.005595\n",
      ">> Epoch 873 finished \tANN training loss 0.004922\n",
      ">> Epoch 874 finished \tANN training loss 0.005065\n",
      ">> Epoch 875 finished \tANN training loss 0.003676\n",
      ">> Epoch 876 finished \tANN training loss 0.003407\n",
      ">> Epoch 877 finished \tANN training loss 0.003643\n",
      ">> Epoch 878 finished \tANN training loss 0.004561\n",
      ">> Epoch 879 finished \tANN training loss 0.004223\n",
      ">> Epoch 880 finished \tANN training loss 0.003261\n",
      ">> Epoch 881 finished \tANN training loss 0.003156\n",
      ">> Epoch 882 finished \tANN training loss 0.003415\n",
      ">> Epoch 883 finished \tANN training loss 0.003168\n",
      ">> Epoch 884 finished \tANN training loss 0.002367\n",
      ">> Epoch 885 finished \tANN training loss 0.002998\n",
      ">> Epoch 886 finished \tANN training loss 0.003226\n",
      ">> Epoch 887 finished \tANN training loss 0.005226\n",
      ">> Epoch 888 finished \tANN training loss 0.003556\n",
      ">> Epoch 889 finished \tANN training loss 0.004270\n",
      ">> Epoch 890 finished \tANN training loss 0.004756\n",
      ">> Epoch 891 finished \tANN training loss 0.003980\n",
      ">> Epoch 892 finished \tANN training loss 0.004080\n",
      ">> Epoch 893 finished \tANN training loss 0.003211\n",
      ">> Epoch 894 finished \tANN training loss 0.003127\n",
      ">> Epoch 895 finished \tANN training loss 0.003223\n",
      ">> Epoch 896 finished \tANN training loss 0.003842\n",
      ">> Epoch 897 finished \tANN training loss 0.002591\n",
      ">> Epoch 898 finished \tANN training loss 0.002720\n",
      ">> Epoch 899 finished \tANN training loss 0.003070\n",
      ">> Epoch 900 finished \tANN training loss 0.002591\n",
      ">> Epoch 901 finished \tANN training loss 0.002373\n",
      ">> Epoch 902 finished \tANN training loss 0.002521\n",
      ">> Epoch 903 finished \tANN training loss 0.003974\n",
      ">> Epoch 904 finished \tANN training loss 0.003001\n",
      ">> Epoch 905 finished \tANN training loss 0.002401\n",
      ">> Epoch 906 finished \tANN training loss 0.002544\n",
      ">> Epoch 907 finished \tANN training loss 0.002329\n",
      ">> Epoch 908 finished \tANN training loss 0.002673\n",
      ">> Epoch 909 finished \tANN training loss 0.002437\n",
      ">> Epoch 910 finished \tANN training loss 0.004912\n",
      ">> Epoch 911 finished \tANN training loss 0.002437\n",
      ">> Epoch 912 finished \tANN training loss 0.002334\n",
      ">> Epoch 913 finished \tANN training loss 0.002432\n",
      ">> Epoch 914 finished \tANN training loss 0.002849\n",
      ">> Epoch 915 finished \tANN training loss 0.002909\n",
      ">> Epoch 916 finished \tANN training loss 0.004887\n",
      ">> Epoch 917 finished \tANN training loss 0.003457\n",
      ">> Epoch 918 finished \tANN training loss 0.003014\n",
      ">> Epoch 919 finished \tANN training loss 0.003206\n",
      ">> Epoch 920 finished \tANN training loss 0.003975\n",
      ">> Epoch 921 finished \tANN training loss 0.003854\n",
      ">> Epoch 922 finished \tANN training loss 0.003286\n",
      ">> Epoch 923 finished \tANN training loss 0.005062\n",
      ">> Epoch 924 finished \tANN training loss 0.003323\n",
      ">> Epoch 925 finished \tANN training loss 0.003827\n",
      ">> Epoch 926 finished \tANN training loss 0.003666\n",
      ">> Epoch 927 finished \tANN training loss 0.004936\n",
      ">> Epoch 928 finished \tANN training loss 0.003577\n",
      ">> Epoch 929 finished \tANN training loss 0.004438\n",
      ">> Epoch 930 finished \tANN training loss 0.004303\n",
      ">> Epoch 931 finished \tANN training loss 0.002455\n",
      ">> Epoch 932 finished \tANN training loss 0.003995\n",
      ">> Epoch 933 finished \tANN training loss 0.002716\n",
      ">> Epoch 934 finished \tANN training loss 0.003207\n",
      ">> Epoch 935 finished \tANN training loss 0.002725\n",
      ">> Epoch 936 finished \tANN training loss 0.003582\n",
      ">> Epoch 937 finished \tANN training loss 0.003303\n",
      ">> Epoch 938 finished \tANN training loss 0.002832\n",
      ">> Epoch 939 finished \tANN training loss 0.002854\n",
      ">> Epoch 940 finished \tANN training loss 0.003706\n",
      ">> Epoch 941 finished \tANN training loss 0.002621\n",
      ">> Epoch 942 finished \tANN training loss 0.002371\n",
      ">> Epoch 943 finished \tANN training loss 0.002211\n",
      ">> Epoch 944 finished \tANN training loss 0.003678\n",
      ">> Epoch 945 finished \tANN training loss 0.003716\n",
      ">> Epoch 946 finished \tANN training loss 0.004655\n",
      ">> Epoch 947 finished \tANN training loss 0.004151\n",
      ">> Epoch 948 finished \tANN training loss 0.003790\n",
      ">> Epoch 949 finished \tANN training loss 0.003485\n",
      ">> Epoch 950 finished \tANN training loss 0.005356\n",
      ">> Epoch 951 finished \tANN training loss 0.003616\n",
      ">> Epoch 952 finished \tANN training loss 0.003356\n",
      ">> Epoch 953 finished \tANN training loss 0.004084\n",
      ">> Epoch 954 finished \tANN training loss 0.003780\n",
      ">> Epoch 955 finished \tANN training loss 0.002557\n",
      ">> Epoch 956 finished \tANN training loss 0.003680\n",
      ">> Epoch 957 finished \tANN training loss 0.003389\n",
      ">> Epoch 958 finished \tANN training loss 0.004426\n",
      ">> Epoch 959 finished \tANN training loss 0.003800\n",
      ">> Epoch 960 finished \tANN training loss 0.003337\n",
      ">> Epoch 961 finished \tANN training loss 0.003495\n",
      ">> Epoch 962 finished \tANN training loss 0.005668\n",
      ">> Epoch 963 finished \tANN training loss 0.004514\n",
      ">> Epoch 964 finished \tANN training loss 0.003378\n",
      ">> Epoch 965 finished \tANN training loss 0.002895\n",
      ">> Epoch 966 finished \tANN training loss 0.003287\n",
      ">> Epoch 967 finished \tANN training loss 0.003013\n",
      ">> Epoch 968 finished \tANN training loss 0.002942\n",
      ">> Epoch 969 finished \tANN training loss 0.002940\n",
      ">> Epoch 970 finished \tANN training loss 0.002264\n",
      ">> Epoch 971 finished \tANN training loss 0.002491\n",
      ">> Epoch 972 finished \tANN training loss 0.002544\n",
      ">> Epoch 973 finished \tANN training loss 0.002089\n",
      ">> Epoch 974 finished \tANN training loss 0.002827\n",
      ">> Epoch 975 finished \tANN training loss 0.003070\n",
      ">> Epoch 976 finished \tANN training loss 0.003261\n",
      ">> Epoch 977 finished \tANN training loss 0.003876\n",
      ">> Epoch 978 finished \tANN training loss 0.003549\n",
      ">> Epoch 979 finished \tANN training loss 0.003903\n",
      ">> Epoch 980 finished \tANN training loss 0.004170\n",
      ">> Epoch 981 finished \tANN training loss 0.003678\n",
      ">> Epoch 982 finished \tANN training loss 0.003945\n",
      ">> Epoch 983 finished \tANN training loss 0.003579\n",
      ">> Epoch 984 finished \tANN training loss 0.003223\n",
      ">> Epoch 985 finished \tANN training loss 0.004633\n",
      ">> Epoch 986 finished \tANN training loss 0.003889\n",
      ">> Epoch 987 finished \tANN training loss 0.003866\n",
      ">> Epoch 988 finished \tANN training loss 0.003395\n",
      ">> Epoch 989 finished \tANN training loss 0.003616\n",
      ">> Epoch 990 finished \tANN training loss 0.003287\n",
      ">> Epoch 991 finished \tANN training loss 0.002603\n",
      ">> Epoch 992 finished \tANN training loss 0.005185\n",
      ">> Epoch 993 finished \tANN training loss 0.003459\n",
      ">> Epoch 994 finished \tANN training loss 0.004364\n",
      ">> Epoch 995 finished \tANN training loss 0.003572\n",
      ">> Epoch 996 finished \tANN training loss 0.002892\n",
      ">> Epoch 997 finished \tANN training loss 0.002884\n",
      ">> Epoch 998 finished \tANN training loss 0.002368\n",
      ">> Epoch 999 finished \tANN training loss 0.002859\n",
      "[END] Fine tuning step\n"
     ]
    }
   ],
   "source": [
    "oof_train2, oof_test2 = get_oof(dbn, x_train, y_train, x_test)\n",
    "train2 = Predict(oof_train2)\n",
    "test2 = Predict(oof_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    }
   ],
   "source": [
    "oof_train3 = np.zeros((x_train.shape[0], _N_CLASS))  # 1000 * _N_CLASS\n",
    "oof_test3 = np.empty((x_test.shape[0], _N_CLASS))  # 500 * _N_CLASS\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kf.split(x_train,y_train)):\n",
    "    kf_X_train = x_train[train_index]  # 800 * 10 交叉验证划分此时的训练集和验证集\n",
    "    kf_X_train = kf_X_train.reshape(kf_X_train.shape[0],1,kf_X_train.shape[1])\n",
    "    kf_y_train = y_train[train_index]  # 1 * 800\n",
    "    kf_y_train = lb.transform(kf_y_train)\n",
    "    kf_X_test = x_train[test_index]  # 200 * 10 验证集\n",
    "    kf_X_test = kf_X_test.reshape(kf_X_test.shape[0],1,kf_X_test.shape[1])\n",
    "\n",
    "    lstm.fit(kf_X_train, kf_y_train,epochs=1000,verbose=0,batch_size=20)  # 当前模型进行训练\n",
    "\n",
    "    oof_train3[test_index] = lstm.predict(kf_X_test)  # 当前验证集进行概率预测， 200 * _N_CLASS\n",
    "    oof_test3 += lstm.predict(x_test_lstm)  # 对测试集概率预测 oof_test_skf[i, :] ，  500 * _N_CLASS\n",
    "\n",
    "oof_test3 /= _N_FOLDS  # 对每一则交叉验证的结果取平均"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_train = []\n",
    "new_train.append(oof_train1)\n",
    "new_train.append(oof_train2)\n",
    "new_train.append(oof_train3)\n",
    "len(new_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_test = []\n",
    "new_test.append(oof_test1)\n",
    "new_test.append(oof_test2)\n",
    "new_test.append(oof_test3)\n",
    "len(new_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train = np.concatenate(new_train, axis=1)\n",
    "new_test = np.concatenate(new_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n"
     ]
    }
   ],
   "source": [
    "new_test = np.array(new_test)\n",
    "count1 = 0\n",
    "count2 = 0\n",
    "for i in range(162):\n",
    "    for j in range(9):\n",
    "        if new_test[i,j] == np.inf:\n",
    "            count1 += 1\n",
    "            print(i,j)\n",
    "        if new_test[i,j] == np.NaN:\n",
    "            count2 += 1\n",
    "            print(i,j)       \n",
    "print(count1,count2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test1 = np.nan_to_num(new_test.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stacking training ones score: 1.0000\n",
      "stacking testing ones score: 0.9574\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators = 13, criterion=\"gini\",random_state=30)\n",
    "clf.fit(new_train, y_train_labels)\n",
    "RFR_pred_train = clf.predict(new_train)\n",
    "RFR_pred_train_label = Predict(RFR_pred_train)\n",
    "stock_train_acc = accuracy_score(y_train, RFR_pred_train_label) \n",
    "new_test1 = np.nan_to_num(new_test.astype(np.float32))\n",
    "RFR1_pred = clf.predict(new_test1)\n",
    "RFR1_pred_label = Predict(RFR1_pred)\n",
    "stock_test_acc = accuracy_score(y_test, RFR1_pred_label) \n",
    "print(\"stacking training ones score: {:.4f}\".format(stock_train_acc))\n",
    "print(\"stacking testing ones score: {:.4f}\".format(stock_test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:0.9574\n",
      "macro-PRE:0.9403\n",
      "macro-SEN:0.9475\n",
      "macroF1-score:0.9435\n"
     ]
    }
   ],
   "source": [
    "print('ACC:{:.4f}'.format( metrics.accuracy_score(y_test, RFR1_pred_label)))\n",
    " \n",
    "print('macro-PRE:{:.4f}'.format( metrics.precision_score(y_test, RFR1_pred_label,average='macro')) )\n",
    " \n",
    "print('macro-SEN:{:.4f}'.format( metrics.recall_score(y_test, RFR1_pred_label,average='macro')))\n",
    " \n",
    "print('macroF1-score:{:.4f}'.format( metrics.f1_score(y_test, RFR1_pred_label,labels=[0,1,2],average='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from scipy import interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes=3\n",
    "y_test_label = label_binarize(y_test, classes=[0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFR2_pred = clf.predict_proba(new_test1)\n",
    "RFR2_pred_1 = np.array(RFR2_pred)\n",
    "RFR3_pred = RFR2_pred_1[:,:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_7692\\4214130932.py:19: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "  mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 计算每一类的ROC\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes): # 遍历三个类别\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_label[:, i], RFR3_pred[i, :])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area（方法二）\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test_label.ravel(), RFR3_pred.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# Compute macro-average ROC curve and ROC area（方法一）\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "lw=2\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.4f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='ROC curve of class {0} (area = {1:0.4f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Some extension of Receiver operating characteristic to multi-class')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lw=2\n",
    "# plt.figure()\n",
    "plt.figure(plt.figure(figsize=(8, 6)))\n",
    "\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='Macro-average (AUC = {0:0.4f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='ROC curve of class {0} (area = {1:0.4f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "\n",
    "plt.yticks(fontproperties='Times New Roman', size=16,weight='bold')#设置大小及加粗\n",
    "plt.xticks(fontproperties='Times New Roman', size=16,weight='bold')\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate',fontproperties='Times New Roman', size=20,weight='bold')\n",
    "plt.ylabel('True Positive Rate',fontproperties='Times New Roman', size=20,weight='bold')\n",
    "\n",
    "font1 = {'family' : 'Times New Roman',\n",
    "'weight' : 'bold',\n",
    "'size'   : 15}\n",
    "\n",
    "plt.legend(loc=\"lower right\",prop=font1)\n",
    "\n",
    "plt.savefig(\"ODEL_RF_ROC.svg\", dpi=300,format=\"svg\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#0099ff  size=10 face=\"黑体\">方法2：DNN </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(438, 9) (438,) (438, 3) (188, 9) (188,) (188, 3)\n"
     ]
    }
   ],
   "source": [
    "print(new_train.shape,y_train.shape,y_train_labels.shape,new_test1.shape,y_test.shape,y_test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_13 (Dense)            (None, 10)                100       \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 3)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 243\n",
      "Trainable params: 243\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def buildDNN(layer1,layer2,n_class):\n",
    "    init = K.initializers.glorot_uniform(seed=1)\n",
    "    simple_adam = tf.keras.optimizers.Adam()\n",
    "    model = K.models.Sequential()\n",
    "    model.add(K.layers.Dense(units=layer1, input_dim=9, kernel_initializer=init, activation='relu'))\n",
    "    model.add(K.layers.Dense(units=layer2, kernel_initializer=init, activation='relu'))\n",
    "    model.add(K.layers.Dense(units=n_class, kernel_initializer=init, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=simple_adam, metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "dnn = buildDNN(layer1=10,layer2=10,n_class=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training \n",
      "Train on 438 samples\n",
      "Epoch 1/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0162 - acc: 0.9909\n",
      "Epoch 2/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0155 - acc: 0.9932\n",
      "Epoch 3/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0153 - acc: 0.9932\n",
      "Epoch 4/500\n",
      "438/438 [==============================] - 0s 23us/sample - loss: 0.0149 - acc: 0.9932\n",
      "Epoch 5/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0154 - acc: 0.9932\n",
      "Epoch 6/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0158 - acc: 0.9932\n",
      "Epoch 7/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0159 - acc: 0.9932\n",
      "Epoch 8/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0151 - acc: 0.9932\n",
      "Epoch 9/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0159 - acc: 0.9932\n",
      "Epoch 10/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0153 - acc: 0.9954\n",
      "Epoch 11/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0151 - acc: 0.9932\n",
      "Epoch 12/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0150 - acc: 0.9954\n",
      "Epoch 13/500\n",
      "438/438 [==============================] - 0s 23us/sample - loss: 0.0153 - acc: 0.9932\n",
      "Epoch 14/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0151 - acc: 0.9932\n",
      "Epoch 15/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0148 - acc: 0.9932\n",
      "Epoch 16/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0150 - acc: 0.9932\n",
      "Epoch 17/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0154 - acc: 0.9909\n",
      "Epoch 18/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0150 - acc: 0.9909\n",
      "Epoch 19/500\n",
      "438/438 [==============================] - 0s 19us/sample - loss: 0.0151 - acc: 0.9932\n",
      "Epoch 20/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0147 - acc: 0.9932\n",
      "Epoch 21/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0151 - acc: 0.9954\n",
      "Epoch 22/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0148 - acc: 0.9932\n",
      "Epoch 23/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0148 - acc: 0.9932\n",
      "Epoch 24/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0147 - acc: 0.9932\n",
      "Epoch 25/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0146 - acc: 0.9932\n",
      "Epoch 26/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0149 - acc: 0.9954\n",
      "Epoch 27/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0151 - acc: 0.9954\n",
      "Epoch 28/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0148 - acc: 0.9954\n",
      "Epoch 29/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0152 - acc: 0.9954\n",
      "Epoch 30/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0144 - acc: 0.9954\n",
      "Epoch 31/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0147 - acc: 0.9954\n",
      "Epoch 32/500\n",
      "438/438 [==============================] - 0s 23us/sample - loss: 0.0146 - acc: 0.9954\n",
      "Epoch 33/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0148 - acc: 0.9954\n",
      "Epoch 34/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0155 - acc: 0.9954\n",
      "Epoch 35/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0150 - acc: 0.9954\n",
      "Epoch 36/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0147 - acc: 0.9954\n",
      "Epoch 37/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0146 - acc: 0.9954\n",
      "Epoch 38/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0150 - acc: 0.9932\n",
      "Epoch 39/500\n",
      "438/438 [==============================] - 0s 23us/sample - loss: 0.0144 - acc: 0.9954\n",
      "Epoch 40/500\n",
      "438/438 [==============================] - 0s 23us/sample - loss: 0.0145 - acc: 0.9932\n",
      "Epoch 41/500\n",
      "438/438 [==============================] - 0s 23us/sample - loss: 0.0145 - acc: 0.9954\n",
      "Epoch 42/500\n",
      "438/438 [==============================] - 0s 23us/sample - loss: 0.0143 - acc: 0.9954\n",
      "Epoch 43/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0145 - acc: 0.9954\n",
      "Epoch 44/500\n",
      "438/438 [==============================] - 0s 23us/sample - loss: 0.0143 - acc: 0.9954\n",
      "Epoch 45/500\n",
      "438/438 [==============================] - 0s 23us/sample - loss: 0.0143 - acc: 0.9954\n",
      "Epoch 46/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0146 - acc: 0.9954\n",
      "Epoch 47/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0143 - acc: 0.9954\n",
      "Epoch 48/500\n",
      "438/438 [==============================] - 0s 23us/sample - loss: 0.0146 - acc: 0.9954\n",
      "Epoch 49/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0141 - acc: 0.9954\n",
      "Epoch 50/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0142 - acc: 0.9954\n",
      "Epoch 51/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0143 - acc: 0.9954\n",
      "Epoch 52/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0142 - acc: 0.9954\n",
      "Epoch 53/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0140 - acc: 0.9954\n",
      "Epoch 54/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0143 - acc: 0.9954\n",
      "Epoch 55/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0141 - acc: 0.9954\n",
      "Epoch 56/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0148 - acc: 0.9932\n",
      "Epoch 57/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0144 - acc: 0.9954\n",
      "Epoch 58/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0141 - acc: 0.9954\n",
      "Epoch 59/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0140 - acc: 0.9954\n",
      "Epoch 60/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0141 - acc: 0.9954\n",
      "Epoch 61/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0143 - acc: 0.9954\n",
      "Epoch 62/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0144 - acc: 0.9954\n",
      "Epoch 63/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0137 - acc: 0.9954\n",
      "Epoch 64/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0144 - acc: 0.9954\n",
      "Epoch 65/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0142 - acc: 0.9954\n",
      "Epoch 66/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0137 - acc: 0.9954\n",
      "Epoch 67/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0142 - acc: 0.9954\n",
      "Epoch 68/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0141 - acc: 0.9954\n",
      "Epoch 69/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0138 - acc: 0.9954\n",
      "Epoch 70/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0145 - acc: 0.9954\n",
      "Epoch 71/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0147 - acc: 0.9954\n",
      "Epoch 72/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0138 - acc: 0.9954\n",
      "Epoch 73/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0137 - acc: 0.9954\n",
      "Epoch 74/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0139 - acc: 0.9954\n",
      "Epoch 75/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0138 - acc: 0.9954\n",
      "Epoch 76/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0141 - acc: 0.9954\n",
      "Epoch 77/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0137 - acc: 0.9954\n",
      "Epoch 78/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0137 - acc: 0.9954\n",
      "Epoch 79/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0136 - acc: 0.9954\n",
      "Epoch 80/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0136 - acc: 0.9954\n",
      "Epoch 81/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0135 - acc: 0.9954\n",
      "Epoch 82/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0135 - acc: 0.9954\n",
      "Epoch 83/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0138 - acc: 0.9954\n",
      "Epoch 84/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0137 - acc: 0.9954\n",
      "Epoch 85/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0136 - acc: 0.9954\n",
      "Epoch 86/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0139 - acc: 0.9954\n",
      "Epoch 87/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0134 - acc: 0.9954\n",
      "Epoch 88/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0134 - acc: 0.9954\n",
      "Epoch 89/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0133 - acc: 0.9954\n",
      "Epoch 90/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0133 - acc: 0.9954\n",
      "Epoch 91/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0136 - acc: 0.9954\n",
      "Epoch 92/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0136 - acc: 0.9954\n",
      "Epoch 93/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0133 - acc: 0.9954\n",
      "Epoch 94/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0133 - acc: 0.9954\n",
      "Epoch 95/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0133 - acc: 0.9954\n",
      "Epoch 96/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0134 - acc: 0.9954\n",
      "Epoch 97/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0133 - acc: 0.9954\n",
      "Epoch 98/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0135 - acc: 0.9954\n",
      "Epoch 99/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0134 - acc: 0.9954\n",
      "Epoch 100/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0132 - acc: 0.9954\n",
      "Epoch 101/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0133 - acc: 0.9954\n",
      "Epoch 102/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0133 - acc: 0.9954\n",
      "Epoch 103/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0137 - acc: 0.9954\n",
      "Epoch 104/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0133 - acc: 0.9954\n",
      "Epoch 105/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0135 - acc: 0.9954\n",
      "Epoch 106/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0133 - acc: 0.9954\n",
      "Epoch 107/500\n",
      "438/438 [==============================] - 0s 52us/sample - loss: 0.0130 - acc: 0.9954\n",
      "Epoch 108/500\n",
      "438/438 [==============================] - 0s 52us/sample - loss: 0.0130 - acc: 0.9954\n",
      "Epoch 109/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0136 - acc: 0.9954\n",
      "Epoch 110/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0134 - acc: 0.9954\n",
      "Epoch 111/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0135 - acc: 0.9954\n",
      "Epoch 112/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0134 - acc: 0.9954\n",
      "Epoch 113/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0131 - acc: 0.9954\n",
      "Epoch 114/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0138 - acc: 0.9954\n",
      "Epoch 115/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0134 - acc: 0.9954\n",
      "Epoch 116/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0132 - acc: 0.9954\n",
      "Epoch 117/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0131 - acc: 0.9954\n",
      "Epoch 118/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0135 - acc: 0.9954\n",
      "Epoch 119/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0128 - acc: 0.9954\n",
      "Epoch 120/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0128 - acc: 0.9954\n",
      "Epoch 121/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0128 - acc: 0.9954\n",
      "Epoch 122/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0127 - acc: 0.9954\n",
      "Epoch 123/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0127 - acc: 0.9954\n",
      "Epoch 124/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0128 - acc: 0.9954\n",
      "Epoch 125/500\n",
      "438/438 [==============================] - 0s 23us/sample - loss: 0.0128 - acc: 0.9954\n",
      "Epoch 126/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0127 - acc: 0.9954\n",
      "Epoch 127/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0127 - acc: 0.9954\n",
      "Epoch 128/500\n",
      "438/438 [==============================] - 0s 23us/sample - loss: 0.0126 - acc: 0.9954\n",
      "Epoch 129/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0127 - acc: 0.9954\n",
      "Epoch 130/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0131 - acc: 0.9954\n",
      "Epoch 131/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0126 - acc: 0.9954\n",
      "Epoch 132/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0126 - acc: 0.9954\n",
      "Epoch 133/500\n",
      "438/438 [==============================] - 0s 23us/sample - loss: 0.0128 - acc: 0.9954\n",
      "Epoch 134/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0127 - acc: 0.9954\n",
      "Epoch 135/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0128 - acc: 0.9954\n",
      "Epoch 136/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0124 - acc: 0.9954\n",
      "Epoch 137/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0127 - acc: 0.9954\n",
      "Epoch 138/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0124 - acc: 0.9954\n",
      "Epoch 139/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0126 - acc: 0.9954\n",
      "Epoch 140/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0129 - acc: 0.9954\n",
      "Epoch 141/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0128 - acc: 0.9954\n",
      "Epoch 142/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0124 - acc: 0.9954\n",
      "Epoch 143/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0134 - acc: 0.9954\n",
      "Epoch 144/500\n",
      "438/438 [==============================] - 0s 23us/sample - loss: 0.0125 - acc: 0.9954\n",
      "Epoch 145/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0126 - acc: 0.9954\n",
      "Epoch 146/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0125 - acc: 0.9954\n",
      "Epoch 147/500\n",
      "438/438 [==============================] - 0s 23us/sample - loss: 0.0122 - acc: 0.9954\n",
      "Epoch 148/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0124 - acc: 0.9954\n",
      "Epoch 149/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0125 - acc: 0.9954\n",
      "Epoch 150/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0122 - acc: 0.9954\n",
      "Epoch 151/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0122 - acc: 0.9954\n",
      "Epoch 152/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0123 - acc: 0.9954\n",
      "Epoch 153/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0125 - acc: 0.9954\n",
      "Epoch 154/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0121 - acc: 0.9954\n",
      "Epoch 155/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0121 - acc: 0.9954\n",
      "Epoch 156/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0124 - acc: 0.9954\n",
      "Epoch 157/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0124 - acc: 0.9954\n",
      "Epoch 158/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0121 - acc: 0.9954\n",
      "Epoch 159/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0119 - acc: 0.9954\n",
      "Epoch 160/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0122 - acc: 0.9954\n",
      "Epoch 161/500\n",
      "438/438 [==============================] - 0s 23us/sample - loss: 0.0120 - acc: 0.9954\n",
      "Epoch 162/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0120 - acc: 0.9954\n",
      "Epoch 163/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0118 - acc: 0.9954\n",
      "Epoch 164/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0119 - acc: 0.9954\n",
      "Epoch 165/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0120 - acc: 0.9954\n",
      "Epoch 166/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0118 - acc: 0.9954\n",
      "Epoch 167/500\n",
      "438/438 [==============================] - 0s 23us/sample - loss: 0.0119 - acc: 0.9954\n",
      "Epoch 168/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0119 - acc: 0.9954\n",
      "Epoch 169/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0118 - acc: 0.9954\n",
      "Epoch 170/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0119 - acc: 0.9954\n",
      "Epoch 171/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0122 - acc: 0.9954\n",
      "Epoch 172/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0126 - acc: 0.9954\n",
      "Epoch 173/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0119 - acc: 0.9954\n",
      "Epoch 174/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0117 - acc: 0.9954\n",
      "Epoch 175/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0119 - acc: 0.9954\n",
      "Epoch 176/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0117 - acc: 0.9954\n",
      "Epoch 177/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0115 - acc: 0.9954\n",
      "Epoch 178/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0119 - acc: 0.9954\n",
      "Epoch 179/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0117 - acc: 0.9954\n",
      "Epoch 180/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0117 - acc: 0.9954\n",
      "Epoch 181/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0128 - acc: 0.9954\n",
      "Epoch 182/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0118 - acc: 0.9954\n",
      "Epoch 183/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0117 - acc: 0.9954\n",
      "Epoch 184/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0119 - acc: 0.9954\n",
      "Epoch 185/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0117 - acc: 0.9954\n",
      "Epoch 186/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0115 - acc: 0.9954\n",
      "Epoch 187/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0114 - acc: 0.9954\n",
      "Epoch 188/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0115 - acc: 0.9954\n",
      "Epoch 189/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0114 - acc: 0.9954\n",
      "Epoch 190/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0116 - acc: 0.9954\n",
      "Epoch 191/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0114 - acc: 0.9954\n",
      "Epoch 192/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0115 - acc: 0.9954\n",
      "Epoch 193/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0113 - acc: 0.9954\n",
      "Epoch 194/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0114 - acc: 0.9954\n",
      "Epoch 195/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0119 - acc: 0.9954\n",
      "Epoch 196/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0113 - acc: 0.9954\n",
      "Epoch 197/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0115 - acc: 0.9954\n",
      "Epoch 198/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0115 - acc: 0.9954\n",
      "Epoch 199/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0111 - acc: 0.9954\n",
      "Epoch 200/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0111 - acc: 0.9954\n",
      "Epoch 201/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0111 - acc: 0.9954\n",
      "Epoch 202/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0112 - acc: 0.9954\n",
      "Epoch 203/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0113 - acc: 0.9954\n",
      "Epoch 204/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0114 - acc: 0.9954\n",
      "Epoch 205/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0110 - acc: 0.9954\n",
      "Epoch 206/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0111 - acc: 0.9954\n",
      "Epoch 207/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0110 - acc: 0.9954\n",
      "Epoch 208/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0111 - acc: 0.9954\n",
      "Epoch 209/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0112 - acc: 0.9954\n",
      "Epoch 210/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0110 - acc: 0.9954\n",
      "Epoch 211/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0110 - acc: 0.9954\n",
      "Epoch 212/500\n",
      "438/438 [==============================] - 0s 23us/sample - loss: 0.0109 - acc: 0.9954\n",
      "Epoch 213/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0108 - acc: 0.9954\n",
      "Epoch 214/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0108 - acc: 0.9954\n",
      "Epoch 215/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0108 - acc: 0.9954\n",
      "Epoch 216/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0111 - acc: 0.9954\n",
      "Epoch 217/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0116 - acc: 0.9954\n",
      "Epoch 218/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0109 - acc: 0.9954\n",
      "Epoch 219/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0108 - acc: 0.9954\n",
      "Epoch 220/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0108 - acc: 0.9954\n",
      "Epoch 221/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0110 - acc: 0.9954\n",
      "Epoch 222/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0107 - acc: 0.9954\n",
      "Epoch 223/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0110 - acc: 0.9954\n",
      "Epoch 224/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0107 - acc: 0.9954\n",
      "Epoch 225/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0113 - acc: 0.9954\n",
      "Epoch 226/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0111 - acc: 0.9954\n",
      "Epoch 227/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0106 - acc: 0.9954\n",
      "Epoch 228/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0116 - acc: 0.9954\n",
      "Epoch 229/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0105 - acc: 0.9954\n",
      "Epoch 230/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0106 - acc: 0.9954\n",
      "Epoch 231/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0108 - acc: 0.9954\n",
      "Epoch 232/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0110 - acc: 0.9954\n",
      "Epoch 233/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0107 - acc: 0.9954\n",
      "Epoch 234/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0105 - acc: 0.9954\n",
      "Epoch 235/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0106 - acc: 0.9954\n",
      "Epoch 236/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0104 - acc: 0.9954\n",
      "Epoch 237/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0106 - acc: 0.9954\n",
      "Epoch 238/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0104 - acc: 0.9954\n",
      "Epoch 239/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0107 - acc: 0.9954\n",
      "Epoch 240/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0104 - acc: 0.9954\n",
      "Epoch 241/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0102 - acc: 0.9954\n",
      "Epoch 242/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0105 - acc: 0.9954\n",
      "Epoch 243/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0108 - acc: 0.9954\n",
      "Epoch 244/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0107 - acc: 0.9954\n",
      "Epoch 245/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0104 - acc: 0.9954\n",
      "Epoch 246/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0102 - acc: 0.9954\n",
      "Epoch 247/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0104 - acc: 0.9954\n",
      "Epoch 248/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0102 - acc: 0.9954\n",
      "Epoch 249/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0105 - acc: 0.9954\n",
      "Epoch 250/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0105 - acc: 0.9954\n",
      "Epoch 251/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0104 - acc: 0.9954\n",
      "Epoch 252/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0101 - acc: 0.9954\n",
      "Epoch 253/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0100 - acc: 0.9954\n",
      "Epoch 254/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0104 - acc: 0.9954\n",
      "Epoch 255/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0100 - acc: 0.9954\n",
      "Epoch 256/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0100 - acc: 0.9954\n",
      "Epoch 257/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0100 - acc: 0.9954\n",
      "Epoch 258/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0101 - acc: 0.9954\n",
      "Epoch 259/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0101 - acc: 0.9954\n",
      "Epoch 260/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0099 - acc: 0.9954\n",
      "Epoch 261/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0102 - acc: 0.9954\n",
      "Epoch 262/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0105 - acc: 0.9954\n",
      "Epoch 263/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0102 - acc: 0.9954\n",
      "Epoch 264/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0099 - acc: 0.9954\n",
      "Epoch 265/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0103 - acc: 0.9954\n",
      "Epoch 266/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0098 - acc: 0.9954\n",
      "Epoch 267/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0103 - acc: 0.9954\n",
      "Epoch 268/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0098 - acc: 0.9954\n",
      "Epoch 269/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0107 - acc: 0.9954\n",
      "Epoch 270/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0104 - acc: 0.9954\n",
      "Epoch 271/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0098 - acc: 0.9954\n",
      "Epoch 272/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0096 - acc: 0.9954\n",
      "Epoch 273/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0095 - acc: 0.9954\n",
      "Epoch 274/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0096 - acc: 0.9954\n",
      "Epoch 275/500\n",
      "438/438 [==============================] - 0s 52us/sample - loss: 0.0096 - acc: 0.9954\n",
      "Epoch 276/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0097 - acc: 0.9954\n",
      "Epoch 277/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0096 - acc: 0.9954\n",
      "Epoch 278/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0095 - acc: 0.9954\n",
      "Epoch 279/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0096 - acc: 0.9954\n",
      "Epoch 280/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0098 - acc: 0.9954\n",
      "Epoch 281/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0097 - acc: 0.9954\n",
      "Epoch 282/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0097 - acc: 0.9954\n",
      "Epoch 283/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0096 - acc: 0.9954\n",
      "Epoch 284/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0096 - acc: 0.9954\n",
      "Epoch 285/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0095 - acc: 0.9954\n",
      "Epoch 286/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0094 - acc: 0.9954\n",
      "Epoch 287/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0094 - acc: 0.9954\n",
      "Epoch 288/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0094 - acc: 0.9954\n",
      "Epoch 289/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0096 - acc: 0.9954\n",
      "Epoch 290/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0097 - acc: 0.9954\n",
      "Epoch 291/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0093 - acc: 0.9954\n",
      "Epoch 292/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0093 - acc: 0.9954\n",
      "Epoch 293/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0092 - acc: 0.9954\n",
      "Epoch 294/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0091 - acc: 0.9954\n",
      "Epoch 295/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0093 - acc: 0.9954\n",
      "Epoch 296/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0096 - acc: 0.9954\n",
      "Epoch 297/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0098 - acc: 0.9954\n",
      "Epoch 298/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0094 - acc: 0.9954\n",
      "Epoch 299/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0092 - acc: 0.9954\n",
      "Epoch 300/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0091 - acc: 0.9954\n",
      "Epoch 301/500\n",
      "438/438 [==============================] - 0s 23us/sample - loss: 0.0096 - acc: 0.9954\n",
      "Epoch 302/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0093 - acc: 0.9954\n",
      "Epoch 303/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0091 - acc: 0.9954\n",
      "Epoch 304/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0090 - acc: 0.9954\n",
      "Epoch 305/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0096 - acc: 0.9954\n",
      "Epoch 306/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0097 - acc: 0.9954\n",
      "Epoch 307/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0093 - acc: 0.9954\n",
      "Epoch 308/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0091 - acc: 0.9954\n",
      "Epoch 309/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0091 - acc: 0.9954\n",
      "Epoch 310/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0090 - acc: 0.9954\n",
      "Epoch 311/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0095 - acc: 0.9977\n",
      "Epoch 312/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0093 - acc: 0.9954\n",
      "Epoch 313/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0090 - acc: 0.9954\n",
      "Epoch 314/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0087 - acc: 0.9954\n",
      "Epoch 315/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0091 - acc: 0.9954\n",
      "Epoch 316/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0089 - acc: 0.9954\n",
      "Epoch 317/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0092 - acc: 0.9954\n",
      "Epoch 318/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0091 - acc: 0.9954\n",
      "Epoch 319/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0087 - acc: 0.9954\n",
      "Epoch 320/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0096 - acc: 0.9977\n",
      "Epoch 321/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0089 - acc: 0.9977\n",
      "Epoch 322/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0090 - acc: 0.9954\n",
      "Epoch 323/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0089 - acc: 0.9954\n",
      "Epoch 324/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0087 - acc: 0.9954\n",
      "Epoch 325/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0085 - acc: 0.9954\n",
      "Epoch 326/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0086 - acc: 0.9954\n",
      "Epoch 327/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0086 - acc: 0.9954\n",
      "Epoch 328/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0088 - acc: 0.9954\n",
      "Epoch 329/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0084 - acc: 0.9954\n",
      "Epoch 330/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0089 - acc: 0.9954\n",
      "Epoch 331/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0087 - acc: 0.9954\n",
      "Epoch 332/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0090 - acc: 0.9977\n",
      "Epoch 333/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0084 - acc: 0.9954\n",
      "Epoch 334/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0084 - acc: 0.9954\n",
      "Epoch 335/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0084 - acc: 0.9954\n",
      "Epoch 336/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0088 - acc: 0.9954\n",
      "Epoch 337/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0085 - acc: 0.9954\n",
      "Epoch 338/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0086 - acc: 0.9977\n",
      "Epoch 339/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0087 - acc: 0.9954\n",
      "Epoch 340/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0084 - acc: 0.9954\n",
      "Epoch 341/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0083 - acc: 0.9954\n",
      "Epoch 342/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0086 - acc: 0.9977\n",
      "Epoch 343/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0089 - acc: 0.9954\n",
      "Epoch 344/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0082 - acc: 0.9954\n",
      "Epoch 345/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0083 - acc: 0.9954\n",
      "Epoch 346/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0085 - acc: 0.9954\n",
      "Epoch 347/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0081 - acc: 0.9954\n",
      "Epoch 348/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0082 - acc: 0.9954\n",
      "Epoch 349/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0083 - acc: 0.9954\n",
      "Epoch 350/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0083 - acc: 0.9954\n",
      "Epoch 351/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0080 - acc: 0.9954\n",
      "Epoch 352/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0084 - acc: 0.9954\n",
      "Epoch 353/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0085 - acc: 0.9977\n",
      "Epoch 354/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0080 - acc: 0.9977\n",
      "Epoch 355/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0083 - acc: 0.9954\n",
      "Epoch 356/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0082 - acc: 0.9954\n",
      "Epoch 357/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0083 - acc: 0.9954\n",
      "Epoch 358/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0081 - acc: 0.9954\n",
      "Epoch 359/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0081 - acc: 0.9954\n",
      "Epoch 360/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0082 - acc: 0.9954\n",
      "Epoch 361/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0079 - acc: 0.9954\n",
      "Epoch 362/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0079 - acc: 0.9954\n",
      "Epoch 363/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0080 - acc: 0.9954\n",
      "Epoch 364/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0082 - acc: 0.9954\n",
      "Epoch 365/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0078 - acc: 0.9954\n",
      "Epoch 366/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0080 - acc: 0.9954\n",
      "Epoch 367/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0083 - acc: 0.9954\n",
      "Epoch 368/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0078 - acc: 0.9977\n",
      "Epoch 369/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0080 - acc: 0.9954\n",
      "Epoch 370/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0083 - acc: 0.9954\n",
      "Epoch 371/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0080 - acc: 0.9954\n",
      "Epoch 372/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0081 - acc: 0.9954\n",
      "Epoch 373/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0078 - acc: 0.9954\n",
      "Epoch 374/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0078 - acc: 0.9954\n",
      "Epoch 375/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0076 - acc: 0.9954\n",
      "Epoch 376/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0079 - acc: 0.9954\n",
      "Epoch 377/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0077 - acc: 0.9954\n",
      "Epoch 378/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0076 - acc: 0.9954\n",
      "Epoch 379/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0076 - acc: 0.9977\n",
      "Epoch 380/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0080 - acc: 0.9977\n",
      "Epoch 381/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0079 - acc: 0.9977\n",
      "Epoch 382/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0076 - acc: 0.9954\n",
      "Epoch 383/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0078 - acc: 0.9954\n",
      "Epoch 384/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0075 - acc: 0.9954\n",
      "Epoch 385/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0075 - acc: 0.9977\n",
      "Epoch 386/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0073 - acc: 0.9977\n",
      "Epoch 387/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0074 - acc: 0.9977\n",
      "Epoch 388/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0075 - acc: 0.9977\n",
      "Epoch 389/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0078 - acc: 0.9954\n",
      "Epoch 390/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0082 - acc: 0.9954\n",
      "Epoch 391/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0076 - acc: 0.9954\n",
      "Epoch 392/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0075 - acc: 0.9954\n",
      "Epoch 393/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0073 - acc: 0.9954\n",
      "Epoch 394/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0076 - acc: 0.9977\n",
      "Epoch 395/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0086 - acc: 0.9954\n",
      "Epoch 396/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0077 - acc: 0.9954\n",
      "Epoch 397/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0074 - acc: 0.9954\n",
      "Epoch 398/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0078 - acc: 0.9977\n",
      "Epoch 399/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0073 - acc: 0.9977\n",
      "Epoch 400/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0072 - acc: 0.9977\n",
      "Epoch 401/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0072 - acc: 0.9954\n",
      "Epoch 402/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0075 - acc: 0.9954\n",
      "Epoch 403/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0069 - acc: 0.9954\n",
      "Epoch 404/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0071 - acc: 0.9977\n",
      "Epoch 405/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0076 - acc: 0.9977\n",
      "Epoch 406/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0071 - acc: 0.9977\n",
      "Epoch 407/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0074 - acc: 0.9977\n",
      "Epoch 408/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0073 - acc: 0.9954\n",
      "Epoch 409/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0072 - acc: 0.9977\n",
      "Epoch 410/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0071 - acc: 0.9954\n",
      "Epoch 411/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0071 - acc: 0.9977\n",
      "Epoch 412/500\n",
      "438/438 [==============================] - 0s 23us/sample - loss: 0.0070 - acc: 0.9977\n",
      "Epoch 413/500\n",
      "438/438 [==============================] - 0s 23us/sample - loss: 0.0073 - acc: 0.9977\n",
      "Epoch 414/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0071 - acc: 0.9977\n",
      "Epoch 415/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0076 - acc: 0.9977\n",
      "Epoch 416/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0070 - acc: 0.9977\n",
      "Epoch 417/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0070 - acc: 0.9954\n",
      "Epoch 418/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0071 - acc: 0.9977\n",
      "Epoch 419/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0071 - acc: 0.9977\n",
      "Epoch 420/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0069 - acc: 0.9977\n",
      "Epoch 421/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0074 - acc: 0.9954\n",
      "Epoch 422/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0070 - acc: 0.9977\n",
      "Epoch 423/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0073 - acc: 0.9977\n",
      "Epoch 424/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0068 - acc: 0.9977\n",
      "Epoch 425/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0068 - acc: 0.9977\n",
      "Epoch 426/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0068 - acc: 0.9977\n",
      "Epoch 427/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0068 - acc: 0.9977\n",
      "Epoch 428/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0067 - acc: 0.9977\n",
      "Epoch 429/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0070 - acc: 0.9977\n",
      "Epoch 430/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0074 - acc: 0.9954\n",
      "Epoch 431/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0066 - acc: 0.9977\n",
      "Epoch 432/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0069 - acc: 0.9977\n",
      "Epoch 433/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0071 - acc: 0.9954\n",
      "Epoch 434/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0065 - acc: 0.9977\n",
      "Epoch 435/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0068 - acc: 0.9977\n",
      "Epoch 436/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0067 - acc: 0.9977\n",
      "Epoch 437/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0066 - acc: 0.9977\n",
      "Epoch 438/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0067 - acc: 0.9977\n",
      "Epoch 439/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0067 - acc: 0.9954\n",
      "Epoch 440/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0066 - acc: 0.9977\n",
      "Epoch 441/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0066 - acc: 0.9977\n",
      "Epoch 442/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0065 - acc: 0.9977\n",
      "Epoch 443/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0066 - acc: 0.9977\n",
      "Epoch 444/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0063 - acc: 0.9977\n",
      "Epoch 445/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0067 - acc: 0.9954\n",
      "Epoch 446/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0064 - acc: 0.9977\n",
      "Epoch 447/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0065 - acc: 0.9977\n",
      "Epoch 448/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0063 - acc: 0.9977\n",
      "Epoch 449/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0064 - acc: 0.9977\n",
      "Epoch 450/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0066 - acc: 0.9977\n",
      "Epoch 451/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0061 - acc: 0.9977\n",
      "Epoch 452/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0068 - acc: 0.9977\n",
      "Epoch 453/500\n",
      "438/438 [==============================] - 0s 23us/sample - loss: 0.0068 - acc: 0.9977\n",
      "Epoch 454/500\n",
      "438/438 [==============================] - 0s 52us/sample - loss: 0.0066 - acc: 0.9977\n",
      "Epoch 455/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0062 - acc: 0.9977\n",
      "Epoch 456/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0061 - acc: 0.9977\n",
      "Epoch 457/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0062 - acc: 0.9977\n",
      "Epoch 458/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0063 - acc: 0.9977\n",
      "Epoch 459/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0061 - acc: 0.9977\n",
      "Epoch 460/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0061 - acc: 0.9977\n",
      "Epoch 461/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0064 - acc: 0.9954\n",
      "Epoch 462/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0063 - acc: 0.9977\n",
      "Epoch 463/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0060 - acc: 0.9977\n",
      "Epoch 464/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0062 - acc: 0.9977\n",
      "Epoch 465/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0060 - acc: 0.9977\n",
      "Epoch 466/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0064 - acc: 0.9977\n",
      "Epoch 467/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0060 - acc: 0.9977\n",
      "Epoch 468/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0060 - acc: 0.9977\n",
      "Epoch 469/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0062 - acc: 0.9977\n",
      "Epoch 470/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0061 - acc: 0.9977\n",
      "Epoch 471/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0063 - acc: 0.9977\n",
      "Epoch 472/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0060 - acc: 0.9977\n",
      "Epoch 473/500\n",
      "438/438 [==============================] - 0s 23us/sample - loss: 0.0059 - acc: 0.9977\n",
      "Epoch 474/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0058 - acc: 0.9977\n",
      "Epoch 475/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0059 - acc: 0.9977\n",
      "Epoch 476/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0061 - acc: 0.9977\n",
      "Epoch 477/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0058 - acc: 0.9977\n",
      "Epoch 478/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0061 - acc: 0.9977\n",
      "Epoch 479/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0057 - acc: 1.0000\n",
      "Epoch 480/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0056 - acc: 0.9977\n",
      "Epoch 481/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0068 - acc: 0.9977\n",
      "Epoch 482/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0066 - acc: 0.9977\n",
      "Epoch 483/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0057 - acc: 0.9977\n",
      "Epoch 484/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0057 - acc: 0.9977\n",
      "Epoch 485/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0057 - acc: 0.9977\n",
      "Epoch 486/500\n",
      "438/438 [==============================] - 0s 23us/sample - loss: 0.0056 - acc: 0.9977\n",
      "Epoch 487/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0056 - acc: 0.9977\n",
      "Epoch 488/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0058 - acc: 0.9977\n",
      "Epoch 489/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0054 - acc: 0.9977\n",
      "Epoch 490/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0057 - acc: 0.9977\n",
      "Epoch 491/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0055 - acc: 0.9977\n",
      "Epoch 492/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0060 - acc: 0.9977\n",
      "Epoch 493/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0057 - acc: 0.9977\n",
      "Epoch 494/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0060 - acc: 0.9977\n",
      "Epoch 495/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0054 - acc: 0.9977\n",
      "Epoch 496/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0054 - acc: 0.9977\n",
      "Epoch 497/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0055 - acc: 0.9977\n",
      "Epoch 498/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0057 - acc: 0.9977\n",
      "Epoch 499/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0061 - acc: 0.9977\n",
      "Epoch 500/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0063 - acc: 0.9977\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18a8c582c40>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_size = 35\n",
    "max_epochs = 500\n",
    "print(\"Starting training \")\n",
    "dnn.fit(new_train, y_train_labels, batch_size=b_size, epochs=max_epochs, shuffle=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_test_proba = dnn.predict(new_test1)\n",
    "dnn_test_pred =Predict(dnn_test_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dnn training with laryer  test score: 0.9148936170212766\n"
     ]
    }
   ],
   "source": [
    "dnn_test_acc = accuracy_score(y_test, dnn_test_pred)\n",
    "print(\"Dnn training with laryer  test score: {}\".format(dnn_test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC: 0.9148936170212766\n",
      "macro-PRE: 0.8913117576779817\n",
      "macro-SEN: 0.8918109809142426\n",
      "macroF1-score: 0.8915356164931717\n"
     ]
    }
   ],
   "source": [
    "print('ACC:', metrics.accuracy_score(y_test,dnn_test_pred))\n",
    " \n",
    "print('macro-PRE:',metrics.precision_score(y_test,dnn_test_pred,average='macro')) \n",
    " \n",
    "print('macro-SEN:',metrics.recall_score(y_test, dnn_test_pred,average='macro'))\n",
    " \n",
    "print('macroF1-score:',metrics.f1_score(y_test, dnn_test_pred,labels=[0,1,2],average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9916\\1718713388.py:19: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "  mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 计算每一类的ROC\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes): # 遍历三个类别\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_label[:, i], dnn_test_proba[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area（方法二）\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test_label.ravel(), dnn_test_proba.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# Compute macro-average ROC curve and ROC area（方法一）\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "lw=2\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.4f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='ROC curve of class {0} (area = {1:0.4f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Some extension of Receiver operating characteristic to multi-class')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#0099ff  size=10 face=\"黑体\">方法3：DBN </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DBN\n",
    "dbn_cant_neuronas_capas_ocultas =  \"10,10\"\n",
    "RBM_cant_epocas_entrenamiento = 35 \n",
    "Backprop_cant_epocas_entrenamiento = 500 \n",
    "\n",
    "# cantidad de neuronas ocultas \n",
    "hidden_layers = []\n",
    "for val in dbn_cant_neuronas_capas_ocultas.split(','):\n",
    "      hidden_layers.append( int(val))\n",
    "\n",
    "dbn = SupervisedDBNClassification(hidden_layers_structure = hidden_layers,\n",
    "                                                learning_rate_rbm=0.05,\n",
    "                                                learning_rate=0.1,\n",
    "                                                n_epochs_rbm=RBM_cant_epocas_entrenamiento,\n",
    "                                                n_iter_backprop=Backprop_cant_epocas_entrenamiento,\n",
    "                                                batch_size=32,\n",
    "                                                activation_function='relu',\n",
    "                                                dropout_p=0.2,\n",
    "                                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 1.843194\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 1.583123\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 1.370053\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 0.974814\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 0.681603\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 0.552442\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 0.490036\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 0.438852\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 0.383054\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 0.338256\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 0.316691\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 0.292734\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 0.277774\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 0.263041\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 0.248388\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 0.236441\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 0.227755\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 0.216726\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 0.213645\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 0.198078\n",
      ">> Epoch 21 finished \tRBM Reconstruction error 0.189086\n",
      ">> Epoch 22 finished \tRBM Reconstruction error 0.176931\n",
      ">> Epoch 23 finished \tRBM Reconstruction error 0.161742\n",
      ">> Epoch 24 finished \tRBM Reconstruction error 0.149698\n",
      ">> Epoch 25 finished \tRBM Reconstruction error 0.138175\n",
      ">> Epoch 26 finished \tRBM Reconstruction error 0.127808\n",
      ">> Epoch 27 finished \tRBM Reconstruction error 0.122318\n",
      ">> Epoch 28 finished \tRBM Reconstruction error 0.118736\n",
      ">> Epoch 29 finished \tRBM Reconstruction error 0.115855\n",
      ">> Epoch 30 finished \tRBM Reconstruction error 0.112718\n",
      ">> Epoch 31 finished \tRBM Reconstruction error 0.109644\n",
      ">> Epoch 32 finished \tRBM Reconstruction error 0.110482\n",
      ">> Epoch 33 finished \tRBM Reconstruction error 0.102233\n",
      ">> Epoch 34 finished \tRBM Reconstruction error 0.104539\n",
      ">> Epoch 35 finished \tRBM Reconstruction error 0.108499\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 1.906158\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 1.446696\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 1.091216\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 0.738089\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 0.564933\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 0.490387\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 0.444548\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 0.398985\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 0.352681\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 0.313939\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 0.283629\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 0.252827\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 0.229629\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 0.207994\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 0.184541\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 0.167353\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 0.152756\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 0.135781\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 0.115869\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 0.099545\n",
      ">> Epoch 21 finished \tRBM Reconstruction error 0.093396\n",
      ">> Epoch 22 finished \tRBM Reconstruction error 0.082629\n",
      ">> Epoch 23 finished \tRBM Reconstruction error 0.072653\n",
      ">> Epoch 24 finished \tRBM Reconstruction error 0.075510\n",
      ">> Epoch 25 finished \tRBM Reconstruction error 0.068560\n",
      ">> Epoch 26 finished \tRBM Reconstruction error 0.065845\n",
      ">> Epoch 27 finished \tRBM Reconstruction error 0.052265\n",
      ">> Epoch 28 finished \tRBM Reconstruction error 0.059907\n",
      ">> Epoch 29 finished \tRBM Reconstruction error 0.057589\n",
      ">> Epoch 30 finished \tRBM Reconstruction error 0.059485\n",
      ">> Epoch 31 finished \tRBM Reconstruction error 0.050658\n",
      ">> Epoch 32 finished \tRBM Reconstruction error 0.048169\n",
      ">> Epoch 33 finished \tRBM Reconstruction error 0.054297\n",
      ">> Epoch 34 finished \tRBM Reconstruction error 0.050936\n",
      ">> Epoch 35 finished \tRBM Reconstruction error 0.055308\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.480655\n",
      ">> Epoch 1 finished \tANN training loss 0.271142\n",
      ">> Epoch 2 finished \tANN training loss 0.183907\n",
      ">> Epoch 3 finished \tANN training loss 0.137262\n",
      ">> Epoch 4 finished \tANN training loss 0.109742\n",
      ">> Epoch 5 finished \tANN training loss 0.092996\n",
      ">> Epoch 6 finished \tANN training loss 0.084818\n",
      ">> Epoch 7 finished \tANN training loss 0.079367\n",
      ">> Epoch 8 finished \tANN training loss 0.076419\n",
      ">> Epoch 9 finished \tANN training loss 0.072141\n",
      ">> Epoch 10 finished \tANN training loss 0.070782\n",
      ">> Epoch 11 finished \tANN training loss 0.069632\n",
      ">> Epoch 12 finished \tANN training loss 0.069119\n",
      ">> Epoch 13 finished \tANN training loss 0.066871\n",
      ">> Epoch 14 finished \tANN training loss 0.067095\n",
      ">> Epoch 15 finished \tANN training loss 0.067284\n",
      ">> Epoch 16 finished \tANN training loss 0.065504\n",
      ">> Epoch 17 finished \tANN training loss 0.065390\n",
      ">> Epoch 18 finished \tANN training loss 0.065157\n",
      ">> Epoch 19 finished \tANN training loss 0.065663\n",
      ">> Epoch 20 finished \tANN training loss 0.065671\n",
      ">> Epoch 21 finished \tANN training loss 0.065754\n",
      ">> Epoch 22 finished \tANN training loss 0.066906\n",
      ">> Epoch 23 finished \tANN training loss 0.066304\n",
      ">> Epoch 24 finished \tANN training loss 0.065600\n",
      ">> Epoch 25 finished \tANN training loss 0.064324\n",
      ">> Epoch 26 finished \tANN training loss 0.064275\n",
      ">> Epoch 27 finished \tANN training loss 0.064219\n",
      ">> Epoch 28 finished \tANN training loss 0.063984\n",
      ">> Epoch 29 finished \tANN training loss 0.064072\n",
      ">> Epoch 30 finished \tANN training loss 0.063351\n",
      ">> Epoch 31 finished \tANN training loss 0.063441\n",
      ">> Epoch 32 finished \tANN training loss 0.062646\n",
      ">> Epoch 33 finished \tANN training loss 0.062410\n",
      ">> Epoch 34 finished \tANN training loss 0.062541\n",
      ">> Epoch 35 finished \tANN training loss 0.062169\n",
      ">> Epoch 36 finished \tANN training loss 0.063183\n",
      ">> Epoch 37 finished \tANN training loss 0.062206\n",
      ">> Epoch 38 finished \tANN training loss 0.062844\n",
      ">> Epoch 39 finished \tANN training loss 0.063527\n",
      ">> Epoch 40 finished \tANN training loss 0.063587\n",
      ">> Epoch 41 finished \tANN training loss 0.062926\n",
      ">> Epoch 42 finished \tANN training loss 0.062592\n",
      ">> Epoch 43 finished \tANN training loss 0.061702\n",
      ">> Epoch 44 finished \tANN training loss 0.062532\n",
      ">> Epoch 45 finished \tANN training loss 0.064203\n",
      ">> Epoch 46 finished \tANN training loss 0.062897\n",
      ">> Epoch 47 finished \tANN training loss 0.061615\n",
      ">> Epoch 48 finished \tANN training loss 0.061289\n",
      ">> Epoch 49 finished \tANN training loss 0.060651\n",
      ">> Epoch 50 finished \tANN training loss 0.061044\n",
      ">> Epoch 51 finished \tANN training loss 0.061224\n",
      ">> Epoch 52 finished \tANN training loss 0.062273\n",
      ">> Epoch 53 finished \tANN training loss 0.060772\n",
      ">> Epoch 54 finished \tANN training loss 0.060594\n",
      ">> Epoch 55 finished \tANN training loss 0.060184\n",
      ">> Epoch 56 finished \tANN training loss 0.060871\n",
      ">> Epoch 57 finished \tANN training loss 0.060586\n",
      ">> Epoch 58 finished \tANN training loss 0.061761\n",
      ">> Epoch 59 finished \tANN training loss 0.061928\n",
      ">> Epoch 60 finished \tANN training loss 0.062139\n",
      ">> Epoch 61 finished \tANN training loss 0.060646\n",
      ">> Epoch 62 finished \tANN training loss 0.060343\n",
      ">> Epoch 63 finished \tANN training loss 0.059763\n",
      ">> Epoch 64 finished \tANN training loss 0.060087\n",
      ">> Epoch 65 finished \tANN training loss 0.060795\n",
      ">> Epoch 66 finished \tANN training loss 0.062146\n",
      ">> Epoch 67 finished \tANN training loss 0.062127\n",
      ">> Epoch 68 finished \tANN training loss 0.061674\n",
      ">> Epoch 69 finished \tANN training loss 0.060950\n",
      ">> Epoch 70 finished \tANN training loss 0.060753\n",
      ">> Epoch 71 finished \tANN training loss 0.062218\n",
      ">> Epoch 72 finished \tANN training loss 0.061860\n",
      ">> Epoch 73 finished \tANN training loss 0.061481\n",
      ">> Epoch 74 finished \tANN training loss 0.061823\n",
      ">> Epoch 75 finished \tANN training loss 0.062036\n",
      ">> Epoch 76 finished \tANN training loss 0.061927\n",
      ">> Epoch 77 finished \tANN training loss 0.062162\n",
      ">> Epoch 78 finished \tANN training loss 0.061251\n",
      ">> Epoch 79 finished \tANN training loss 0.061907\n",
      ">> Epoch 80 finished \tANN training loss 0.061037\n",
      ">> Epoch 81 finished \tANN training loss 0.061558\n",
      ">> Epoch 82 finished \tANN training loss 0.061000\n",
      ">> Epoch 83 finished \tANN training loss 0.061267\n",
      ">> Epoch 84 finished \tANN training loss 0.060852\n",
      ">> Epoch 85 finished \tANN training loss 0.061526\n",
      ">> Epoch 86 finished \tANN training loss 0.061134\n",
      ">> Epoch 87 finished \tANN training loss 0.061409\n",
      ">> Epoch 88 finished \tANN training loss 0.060524\n",
      ">> Epoch 89 finished \tANN training loss 0.059971\n",
      ">> Epoch 90 finished \tANN training loss 0.059509\n",
      ">> Epoch 91 finished \tANN training loss 0.060170\n",
      ">> Epoch 92 finished \tANN training loss 0.061318\n",
      ">> Epoch 93 finished \tANN training loss 0.059876\n",
      ">> Epoch 94 finished \tANN training loss 0.061314\n",
      ">> Epoch 95 finished \tANN training loss 0.060538\n",
      ">> Epoch 96 finished \tANN training loss 0.060196\n",
      ">> Epoch 97 finished \tANN training loss 0.059954\n",
      ">> Epoch 98 finished \tANN training loss 0.059850\n",
      ">> Epoch 99 finished \tANN training loss 0.060402\n",
      ">> Epoch 100 finished \tANN training loss 0.060212\n",
      ">> Epoch 101 finished \tANN training loss 0.060147\n",
      ">> Epoch 102 finished \tANN training loss 0.059393\n",
      ">> Epoch 103 finished \tANN training loss 0.059069\n",
      ">> Epoch 104 finished \tANN training loss 0.059604\n",
      ">> Epoch 105 finished \tANN training loss 0.059869\n",
      ">> Epoch 106 finished \tANN training loss 0.059755\n",
      ">> Epoch 107 finished \tANN training loss 0.060162\n",
      ">> Epoch 108 finished \tANN training loss 0.060184\n",
      ">> Epoch 109 finished \tANN training loss 0.059860\n",
      ">> Epoch 110 finished \tANN training loss 0.059198\n",
      ">> Epoch 111 finished \tANN training loss 0.058936\n",
      ">> Epoch 112 finished \tANN training loss 0.058765\n",
      ">> Epoch 113 finished \tANN training loss 0.059147\n",
      ">> Epoch 114 finished \tANN training loss 0.059748\n",
      ">> Epoch 115 finished \tANN training loss 0.059823\n",
      ">> Epoch 116 finished \tANN training loss 0.060307\n",
      ">> Epoch 117 finished \tANN training loss 0.060477\n",
      ">> Epoch 118 finished \tANN training loss 0.060888\n",
      ">> Epoch 119 finished \tANN training loss 0.060105\n",
      ">> Epoch 120 finished \tANN training loss 0.058571\n",
      ">> Epoch 121 finished \tANN training loss 0.059059\n",
      ">> Epoch 122 finished \tANN training loss 0.059605\n",
      ">> Epoch 123 finished \tANN training loss 0.059121\n",
      ">> Epoch 124 finished \tANN training loss 0.059435\n",
      ">> Epoch 125 finished \tANN training loss 0.058959\n",
      ">> Epoch 126 finished \tANN training loss 0.058945\n",
      ">> Epoch 127 finished \tANN training loss 0.058781\n",
      ">> Epoch 128 finished \tANN training loss 0.058362\n",
      ">> Epoch 129 finished \tANN training loss 0.058646\n",
      ">> Epoch 130 finished \tANN training loss 0.059569\n",
      ">> Epoch 131 finished \tANN training loss 0.059268\n",
      ">> Epoch 132 finished \tANN training loss 0.057969\n",
      ">> Epoch 133 finished \tANN training loss 0.058569\n",
      ">> Epoch 134 finished \tANN training loss 0.059906\n",
      ">> Epoch 135 finished \tANN training loss 0.059823\n",
      ">> Epoch 136 finished \tANN training loss 0.059420\n",
      ">> Epoch 137 finished \tANN training loss 0.059595\n",
      ">> Epoch 138 finished \tANN training loss 0.059871\n",
      ">> Epoch 139 finished \tANN training loss 0.059077\n",
      ">> Epoch 140 finished \tANN training loss 0.057832\n",
      ">> Epoch 141 finished \tANN training loss 0.057825\n",
      ">> Epoch 142 finished \tANN training loss 0.058316\n",
      ">> Epoch 143 finished \tANN training loss 0.059295\n",
      ">> Epoch 144 finished \tANN training loss 0.059610\n",
      ">> Epoch 145 finished \tANN training loss 0.058754\n",
      ">> Epoch 146 finished \tANN training loss 0.058328\n",
      ">> Epoch 147 finished \tANN training loss 0.058729\n",
      ">> Epoch 148 finished \tANN training loss 0.060199\n",
      ">> Epoch 149 finished \tANN training loss 0.058486\n",
      ">> Epoch 150 finished \tANN training loss 0.058605\n",
      ">> Epoch 151 finished \tANN training loss 0.058874\n",
      ">> Epoch 152 finished \tANN training loss 0.059655\n",
      ">> Epoch 153 finished \tANN training loss 0.060063\n",
      ">> Epoch 154 finished \tANN training loss 0.059407\n",
      ">> Epoch 155 finished \tANN training loss 0.058144\n",
      ">> Epoch 156 finished \tANN training loss 0.058506\n",
      ">> Epoch 157 finished \tANN training loss 0.058208\n",
      ">> Epoch 158 finished \tANN training loss 0.057577\n",
      ">> Epoch 159 finished \tANN training loss 0.057736\n",
      ">> Epoch 160 finished \tANN training loss 0.057987\n",
      ">> Epoch 161 finished \tANN training loss 0.058444\n",
      ">> Epoch 162 finished \tANN training loss 0.056879\n",
      ">> Epoch 163 finished \tANN training loss 0.056670\n",
      ">> Epoch 164 finished \tANN training loss 0.057360\n",
      ">> Epoch 165 finished \tANN training loss 0.056480\n",
      ">> Epoch 166 finished \tANN training loss 0.056395\n",
      ">> Epoch 167 finished \tANN training loss 0.057644\n",
      ">> Epoch 168 finished \tANN training loss 0.056779\n",
      ">> Epoch 169 finished \tANN training loss 0.057694\n",
      ">> Epoch 170 finished \tANN training loss 0.057194\n",
      ">> Epoch 171 finished \tANN training loss 0.057270\n",
      ">> Epoch 172 finished \tANN training loss 0.055398\n",
      ">> Epoch 173 finished \tANN training loss 0.055985\n",
      ">> Epoch 174 finished \tANN training loss 0.055917\n",
      ">> Epoch 175 finished \tANN training loss 0.056759\n",
      ">> Epoch 176 finished \tANN training loss 0.055714\n",
      ">> Epoch 177 finished \tANN training loss 0.056361\n",
      ">> Epoch 178 finished \tANN training loss 0.056267\n",
      ">> Epoch 179 finished \tANN training loss 0.056557\n",
      ">> Epoch 180 finished \tANN training loss 0.055940\n",
      ">> Epoch 181 finished \tANN training loss 0.055908\n",
      ">> Epoch 182 finished \tANN training loss 0.056345\n",
      ">> Epoch 183 finished \tANN training loss 0.056137\n",
      ">> Epoch 184 finished \tANN training loss 0.055656\n",
      ">> Epoch 185 finished \tANN training loss 0.056132\n",
      ">> Epoch 186 finished \tANN training loss 0.055970\n",
      ">> Epoch 187 finished \tANN training loss 0.055613\n",
      ">> Epoch 188 finished \tANN training loss 0.055453\n",
      ">> Epoch 189 finished \tANN training loss 0.055258\n",
      ">> Epoch 190 finished \tANN training loss 0.056338\n",
      ">> Epoch 191 finished \tANN training loss 0.056373\n",
      ">> Epoch 192 finished \tANN training loss 0.057059\n",
      ">> Epoch 193 finished \tANN training loss 0.057597\n",
      ">> Epoch 194 finished \tANN training loss 0.057106\n",
      ">> Epoch 195 finished \tANN training loss 0.057139\n",
      ">> Epoch 196 finished \tANN training loss 0.057903\n",
      ">> Epoch 197 finished \tANN training loss 0.057887\n",
      ">> Epoch 198 finished \tANN training loss 0.058123\n",
      ">> Epoch 199 finished \tANN training loss 0.057482\n",
      ">> Epoch 200 finished \tANN training loss 0.058273\n",
      ">> Epoch 201 finished \tANN training loss 0.057864\n",
      ">> Epoch 202 finished \tANN training loss 0.057201\n",
      ">> Epoch 203 finished \tANN training loss 0.057428\n",
      ">> Epoch 204 finished \tANN training loss 0.057732\n",
      ">> Epoch 205 finished \tANN training loss 0.057780\n",
      ">> Epoch 206 finished \tANN training loss 0.057722\n",
      ">> Epoch 207 finished \tANN training loss 0.056711\n",
      ">> Epoch 208 finished \tANN training loss 0.057065\n",
      ">> Epoch 209 finished \tANN training loss 0.056544\n",
      ">> Epoch 210 finished \tANN training loss 0.056465\n",
      ">> Epoch 211 finished \tANN training loss 0.057066\n",
      ">> Epoch 212 finished \tANN training loss 0.057659\n",
      ">> Epoch 213 finished \tANN training loss 0.056878\n",
      ">> Epoch 214 finished \tANN training loss 0.056745\n",
      ">> Epoch 215 finished \tANN training loss 0.056781\n",
      ">> Epoch 216 finished \tANN training loss 0.057980\n",
      ">> Epoch 217 finished \tANN training loss 0.057402\n",
      ">> Epoch 218 finished \tANN training loss 0.058026\n",
      ">> Epoch 219 finished \tANN training loss 0.058326\n",
      ">> Epoch 220 finished \tANN training loss 0.056856\n",
      ">> Epoch 221 finished \tANN training loss 0.057422\n",
      ">> Epoch 222 finished \tANN training loss 0.057133\n",
      ">> Epoch 223 finished \tANN training loss 0.057583\n",
      ">> Epoch 224 finished \tANN training loss 0.056949\n",
      ">> Epoch 225 finished \tANN training loss 0.057005\n",
      ">> Epoch 226 finished \tANN training loss 0.056449\n",
      ">> Epoch 227 finished \tANN training loss 0.056367\n",
      ">> Epoch 228 finished \tANN training loss 0.056573\n",
      ">> Epoch 229 finished \tANN training loss 0.057055\n",
      ">> Epoch 230 finished \tANN training loss 0.055671\n",
      ">> Epoch 231 finished \tANN training loss 0.055322\n",
      ">> Epoch 232 finished \tANN training loss 0.055161\n",
      ">> Epoch 233 finished \tANN training loss 0.055036\n",
      ">> Epoch 234 finished \tANN training loss 0.055364\n",
      ">> Epoch 235 finished \tANN training loss 0.056392\n",
      ">> Epoch 236 finished \tANN training loss 0.054823\n",
      ">> Epoch 237 finished \tANN training loss 0.055169\n",
      ">> Epoch 238 finished \tANN training loss 0.055379\n",
      ">> Epoch 239 finished \tANN training loss 0.055743\n",
      ">> Epoch 240 finished \tANN training loss 0.054938\n",
      ">> Epoch 241 finished \tANN training loss 0.054636\n",
      ">> Epoch 242 finished \tANN training loss 0.055791\n",
      ">> Epoch 243 finished \tANN training loss 0.056348\n",
      ">> Epoch 244 finished \tANN training loss 0.055723\n",
      ">> Epoch 245 finished \tANN training loss 0.056318\n",
      ">> Epoch 246 finished \tANN training loss 0.055397\n",
      ">> Epoch 247 finished \tANN training loss 0.055325\n",
      ">> Epoch 248 finished \tANN training loss 0.055151\n",
      ">> Epoch 249 finished \tANN training loss 0.055046\n",
      ">> Epoch 250 finished \tANN training loss 0.055901\n",
      ">> Epoch 251 finished \tANN training loss 0.055516\n",
      ">> Epoch 252 finished \tANN training loss 0.055326\n",
      ">> Epoch 253 finished \tANN training loss 0.055257\n",
      ">> Epoch 254 finished \tANN training loss 0.055649\n",
      ">> Epoch 255 finished \tANN training loss 0.056169\n",
      ">> Epoch 256 finished \tANN training loss 0.055633\n",
      ">> Epoch 257 finished \tANN training loss 0.054384\n",
      ">> Epoch 258 finished \tANN training loss 0.054352\n",
      ">> Epoch 259 finished \tANN training loss 0.054494\n",
      ">> Epoch 260 finished \tANN training loss 0.053897\n",
      ">> Epoch 261 finished \tANN training loss 0.054477\n",
      ">> Epoch 262 finished \tANN training loss 0.053725\n",
      ">> Epoch 263 finished \tANN training loss 0.053535\n",
      ">> Epoch 264 finished \tANN training loss 0.053320\n",
      ">> Epoch 265 finished \tANN training loss 0.053459\n",
      ">> Epoch 266 finished \tANN training loss 0.053428\n",
      ">> Epoch 267 finished \tANN training loss 0.054045\n",
      ">> Epoch 268 finished \tANN training loss 0.053517\n",
      ">> Epoch 269 finished \tANN training loss 0.053967\n",
      ">> Epoch 270 finished \tANN training loss 0.054159\n",
      ">> Epoch 271 finished \tANN training loss 0.055447\n",
      ">> Epoch 272 finished \tANN training loss 0.056218\n",
      ">> Epoch 273 finished \tANN training loss 0.055918\n",
      ">> Epoch 274 finished \tANN training loss 0.055135\n",
      ">> Epoch 275 finished \tANN training loss 0.055057\n",
      ">> Epoch 276 finished \tANN training loss 0.053978\n",
      ">> Epoch 277 finished \tANN training loss 0.054157\n",
      ">> Epoch 278 finished \tANN training loss 0.054790\n",
      ">> Epoch 279 finished \tANN training loss 0.054459\n",
      ">> Epoch 280 finished \tANN training loss 0.055038\n",
      ">> Epoch 281 finished \tANN training loss 0.054152\n",
      ">> Epoch 282 finished \tANN training loss 0.054048\n",
      ">> Epoch 283 finished \tANN training loss 0.055111\n",
      ">> Epoch 284 finished \tANN training loss 0.054894\n",
      ">> Epoch 285 finished \tANN training loss 0.054617\n",
      ">> Epoch 286 finished \tANN training loss 0.053245\n",
      ">> Epoch 287 finished \tANN training loss 0.052820\n",
      ">> Epoch 288 finished \tANN training loss 0.053376\n",
      ">> Epoch 289 finished \tANN training loss 0.054821\n",
      ">> Epoch 290 finished \tANN training loss 0.054041\n",
      ">> Epoch 291 finished \tANN training loss 0.055558\n",
      ">> Epoch 292 finished \tANN training loss 0.055045\n",
      ">> Epoch 293 finished \tANN training loss 0.054040\n",
      ">> Epoch 294 finished \tANN training loss 0.054822\n",
      ">> Epoch 295 finished \tANN training loss 0.056124\n",
      ">> Epoch 296 finished \tANN training loss 0.053647\n",
      ">> Epoch 297 finished \tANN training loss 0.053586\n",
      ">> Epoch 298 finished \tANN training loss 0.053550\n",
      ">> Epoch 299 finished \tANN training loss 0.053431\n",
      ">> Epoch 300 finished \tANN training loss 0.053504\n",
      ">> Epoch 301 finished \tANN training loss 0.053374\n",
      ">> Epoch 302 finished \tANN training loss 0.053366\n",
      ">> Epoch 303 finished \tANN training loss 0.053326\n",
      ">> Epoch 304 finished \tANN training loss 0.053235\n",
      ">> Epoch 305 finished \tANN training loss 0.053798\n",
      ">> Epoch 306 finished \tANN training loss 0.052996\n",
      ">> Epoch 307 finished \tANN training loss 0.053667\n",
      ">> Epoch 308 finished \tANN training loss 0.053668\n",
      ">> Epoch 309 finished \tANN training loss 0.053416\n",
      ">> Epoch 310 finished \tANN training loss 0.053066\n",
      ">> Epoch 311 finished \tANN training loss 0.052649\n",
      ">> Epoch 312 finished \tANN training loss 0.053054\n",
      ">> Epoch 313 finished \tANN training loss 0.053455\n",
      ">> Epoch 314 finished \tANN training loss 0.054220\n",
      ">> Epoch 315 finished \tANN training loss 0.053516\n",
      ">> Epoch 316 finished \tANN training loss 0.053197\n",
      ">> Epoch 317 finished \tANN training loss 0.053416\n",
      ">> Epoch 318 finished \tANN training loss 0.052525\n",
      ">> Epoch 319 finished \tANN training loss 0.052632\n",
      ">> Epoch 320 finished \tANN training loss 0.051855\n",
      ">> Epoch 321 finished \tANN training loss 0.051363\n",
      ">> Epoch 322 finished \tANN training loss 0.051767\n",
      ">> Epoch 323 finished \tANN training loss 0.052678\n",
      ">> Epoch 324 finished \tANN training loss 0.051877\n",
      ">> Epoch 325 finished \tANN training loss 0.051219\n",
      ">> Epoch 326 finished \tANN training loss 0.051604\n",
      ">> Epoch 327 finished \tANN training loss 0.052334\n",
      ">> Epoch 328 finished \tANN training loss 0.052685\n",
      ">> Epoch 329 finished \tANN training loss 0.052347\n",
      ">> Epoch 330 finished \tANN training loss 0.053082\n",
      ">> Epoch 331 finished \tANN training loss 0.053957\n",
      ">> Epoch 332 finished \tANN training loss 0.053406\n",
      ">> Epoch 333 finished \tANN training loss 0.053899\n",
      ">> Epoch 334 finished \tANN training loss 0.053997\n",
      ">> Epoch 335 finished \tANN training loss 0.052440\n",
      ">> Epoch 336 finished \tANN training loss 0.053239\n",
      ">> Epoch 337 finished \tANN training loss 0.052754\n",
      ">> Epoch 338 finished \tANN training loss 0.053076\n",
      ">> Epoch 339 finished \tANN training loss 0.053761\n",
      ">> Epoch 340 finished \tANN training loss 0.055205\n",
      ">> Epoch 341 finished \tANN training loss 0.054000\n",
      ">> Epoch 342 finished \tANN training loss 0.052977\n",
      ">> Epoch 343 finished \tANN training loss 0.053533\n",
      ">> Epoch 344 finished \tANN training loss 0.053161\n",
      ">> Epoch 345 finished \tANN training loss 0.052426\n",
      ">> Epoch 346 finished \tANN training loss 0.051691\n",
      ">> Epoch 347 finished \tANN training loss 0.051885\n",
      ">> Epoch 348 finished \tANN training loss 0.051725\n",
      ">> Epoch 349 finished \tANN training loss 0.051980\n",
      ">> Epoch 350 finished \tANN training loss 0.051933\n",
      ">> Epoch 351 finished \tANN training loss 0.052305\n",
      ">> Epoch 352 finished \tANN training loss 0.052494\n",
      ">> Epoch 353 finished \tANN training loss 0.052440\n",
      ">> Epoch 354 finished \tANN training loss 0.052767\n",
      ">> Epoch 355 finished \tANN training loss 0.053122\n",
      ">> Epoch 356 finished \tANN training loss 0.052129\n",
      ">> Epoch 357 finished \tANN training loss 0.051849\n",
      ">> Epoch 358 finished \tANN training loss 0.051645\n",
      ">> Epoch 359 finished \tANN training loss 0.051005\n",
      ">> Epoch 360 finished \tANN training loss 0.051334\n",
      ">> Epoch 361 finished \tANN training loss 0.050415\n",
      ">> Epoch 362 finished \tANN training loss 0.050504\n",
      ">> Epoch 363 finished \tANN training loss 0.051183\n",
      ">> Epoch 364 finished \tANN training loss 0.050881\n",
      ">> Epoch 365 finished \tANN training loss 0.051099\n",
      ">> Epoch 366 finished \tANN training loss 0.051733\n",
      ">> Epoch 367 finished \tANN training loss 0.051435\n",
      ">> Epoch 368 finished \tANN training loss 0.051425\n",
      ">> Epoch 369 finished \tANN training loss 0.051762\n",
      ">> Epoch 370 finished \tANN training loss 0.051239\n",
      ">> Epoch 371 finished \tANN training loss 0.052065\n",
      ">> Epoch 372 finished \tANN training loss 0.052438\n",
      ">> Epoch 373 finished \tANN training loss 0.051398\n",
      ">> Epoch 374 finished \tANN training loss 0.052251\n",
      ">> Epoch 375 finished \tANN training loss 0.051991\n",
      ">> Epoch 376 finished \tANN training loss 0.052001\n",
      ">> Epoch 377 finished \tANN training loss 0.051667\n",
      ">> Epoch 378 finished \tANN training loss 0.051977\n",
      ">> Epoch 379 finished \tANN training loss 0.052016\n",
      ">> Epoch 380 finished \tANN training loss 0.052308\n",
      ">> Epoch 381 finished \tANN training loss 0.052396\n",
      ">> Epoch 382 finished \tANN training loss 0.051538\n",
      ">> Epoch 383 finished \tANN training loss 0.051027\n",
      ">> Epoch 384 finished \tANN training loss 0.051428\n",
      ">> Epoch 385 finished \tANN training loss 0.050721\n",
      ">> Epoch 386 finished \tANN training loss 0.051349\n",
      ">> Epoch 387 finished \tANN training loss 0.051578\n",
      ">> Epoch 388 finished \tANN training loss 0.052422\n",
      ">> Epoch 389 finished \tANN training loss 0.051101\n",
      ">> Epoch 390 finished \tANN training loss 0.050744\n",
      ">> Epoch 391 finished \tANN training loss 0.050411\n",
      ">> Epoch 392 finished \tANN training loss 0.051318\n",
      ">> Epoch 393 finished \tANN training loss 0.051658\n",
      ">> Epoch 394 finished \tANN training loss 0.050356\n",
      ">> Epoch 395 finished \tANN training loss 0.049678\n",
      ">> Epoch 396 finished \tANN training loss 0.049169\n",
      ">> Epoch 397 finished \tANN training loss 0.049155\n",
      ">> Epoch 398 finished \tANN training loss 0.049902\n",
      ">> Epoch 399 finished \tANN training loss 0.050834\n",
      ">> Epoch 400 finished \tANN training loss 0.052730\n",
      ">> Epoch 401 finished \tANN training loss 0.050397\n",
      ">> Epoch 402 finished \tANN training loss 0.050576\n",
      ">> Epoch 403 finished \tANN training loss 0.049104\n",
      ">> Epoch 404 finished \tANN training loss 0.049758\n",
      ">> Epoch 405 finished \tANN training loss 0.049676\n",
      ">> Epoch 406 finished \tANN training loss 0.051236\n",
      ">> Epoch 407 finished \tANN training loss 0.050912\n",
      ">> Epoch 408 finished \tANN training loss 0.050111\n",
      ">> Epoch 409 finished \tANN training loss 0.048369\n",
      ">> Epoch 410 finished \tANN training loss 0.048902\n",
      ">> Epoch 411 finished \tANN training loss 0.048767\n",
      ">> Epoch 412 finished \tANN training loss 0.048143\n",
      ">> Epoch 413 finished \tANN training loss 0.048904\n",
      ">> Epoch 414 finished \tANN training loss 0.048963\n",
      ">> Epoch 415 finished \tANN training loss 0.048540\n",
      ">> Epoch 416 finished \tANN training loss 0.048567\n",
      ">> Epoch 417 finished \tANN training loss 0.048728\n",
      ">> Epoch 418 finished \tANN training loss 0.048963\n",
      ">> Epoch 419 finished \tANN training loss 0.049289\n",
      ">> Epoch 420 finished \tANN training loss 0.049583\n",
      ">> Epoch 421 finished \tANN training loss 0.049395\n",
      ">> Epoch 422 finished \tANN training loss 0.049260\n",
      ">> Epoch 423 finished \tANN training loss 0.050071\n",
      ">> Epoch 424 finished \tANN training loss 0.048183\n",
      ">> Epoch 425 finished \tANN training loss 0.048910\n",
      ">> Epoch 426 finished \tANN training loss 0.049366\n",
      ">> Epoch 427 finished \tANN training loss 0.048731\n",
      ">> Epoch 428 finished \tANN training loss 0.048887\n",
      ">> Epoch 429 finished \tANN training loss 0.048849\n",
      ">> Epoch 430 finished \tANN training loss 0.048915\n",
      ">> Epoch 431 finished \tANN training loss 0.050041\n",
      ">> Epoch 432 finished \tANN training loss 0.048443\n",
      ">> Epoch 433 finished \tANN training loss 0.048236\n",
      ">> Epoch 434 finished \tANN training loss 0.049004\n",
      ">> Epoch 435 finished \tANN training loss 0.049350\n",
      ">> Epoch 436 finished \tANN training loss 0.048394\n",
      ">> Epoch 437 finished \tANN training loss 0.048150\n",
      ">> Epoch 438 finished \tANN training loss 0.048850\n",
      ">> Epoch 439 finished \tANN training loss 0.048794\n",
      ">> Epoch 440 finished \tANN training loss 0.048345\n",
      ">> Epoch 441 finished \tANN training loss 0.047885\n",
      ">> Epoch 442 finished \tANN training loss 0.047568\n",
      ">> Epoch 443 finished \tANN training loss 0.048789\n",
      ">> Epoch 444 finished \tANN training loss 0.048000\n",
      ">> Epoch 445 finished \tANN training loss 0.049062\n",
      ">> Epoch 446 finished \tANN training loss 0.048357\n",
      ">> Epoch 447 finished \tANN training loss 0.048056\n",
      ">> Epoch 448 finished \tANN training loss 0.047839\n",
      ">> Epoch 449 finished \tANN training loss 0.047802\n",
      ">> Epoch 450 finished \tANN training loss 0.047603\n",
      ">> Epoch 451 finished \tANN training loss 0.047976\n",
      ">> Epoch 452 finished \tANN training loss 0.048289\n",
      ">> Epoch 453 finished \tANN training loss 0.048422\n",
      ">> Epoch 454 finished \tANN training loss 0.048594\n",
      ">> Epoch 455 finished \tANN training loss 0.048111\n",
      ">> Epoch 456 finished \tANN training loss 0.048392\n",
      ">> Epoch 457 finished \tANN training loss 0.048837\n",
      ">> Epoch 458 finished \tANN training loss 0.048189\n",
      ">> Epoch 459 finished \tANN training loss 0.047598\n",
      ">> Epoch 460 finished \tANN training loss 0.048346\n",
      ">> Epoch 461 finished \tANN training loss 0.046562\n",
      ">> Epoch 462 finished \tANN training loss 0.046500\n",
      ">> Epoch 463 finished \tANN training loss 0.047011\n",
      ">> Epoch 464 finished \tANN training loss 0.047970\n",
      ">> Epoch 465 finished \tANN training loss 0.047613\n",
      ">> Epoch 466 finished \tANN training loss 0.047820\n",
      ">> Epoch 467 finished \tANN training loss 0.048271\n",
      ">> Epoch 468 finished \tANN training loss 0.047323\n",
      ">> Epoch 469 finished \tANN training loss 0.047915\n",
      ">> Epoch 470 finished \tANN training loss 0.046471\n",
      ">> Epoch 471 finished \tANN training loss 0.047106\n",
      ">> Epoch 472 finished \tANN training loss 0.046443\n",
      ">> Epoch 473 finished \tANN training loss 0.045644\n",
      ">> Epoch 474 finished \tANN training loss 0.046418\n",
      ">> Epoch 475 finished \tANN training loss 0.048115\n",
      ">> Epoch 476 finished \tANN training loss 0.048009\n",
      ">> Epoch 477 finished \tANN training loss 0.046314\n",
      ">> Epoch 478 finished \tANN training loss 0.046472\n",
      ">> Epoch 479 finished \tANN training loss 0.046517\n",
      ">> Epoch 480 finished \tANN training loss 0.047016\n",
      ">> Epoch 481 finished \tANN training loss 0.046646\n",
      ">> Epoch 482 finished \tANN training loss 0.046608\n",
      ">> Epoch 483 finished \tANN training loss 0.047411\n",
      ">> Epoch 484 finished \tANN training loss 0.046298\n",
      ">> Epoch 485 finished \tANN training loss 0.045938\n",
      ">> Epoch 486 finished \tANN training loss 0.045952\n",
      ">> Epoch 487 finished \tANN training loss 0.045420\n",
      ">> Epoch 488 finished \tANN training loss 0.045239\n",
      ">> Epoch 489 finished \tANN training loss 0.045624\n",
      ">> Epoch 490 finished \tANN training loss 0.045815\n",
      ">> Epoch 491 finished \tANN training loss 0.045053\n",
      ">> Epoch 492 finished \tANN training loss 0.045261\n",
      ">> Epoch 493 finished \tANN training loss 0.045738\n",
      ">> Epoch 494 finished \tANN training loss 0.045429\n",
      ">> Epoch 495 finished \tANN training loss 0.045003\n",
      ">> Epoch 496 finished \tANN training loss 0.045388\n",
      ">> Epoch 497 finished \tANN training loss 0.044297\n",
      ">> Epoch 498 finished \tANN training loss 0.044395\n",
      ">> Epoch 499 finished \tANN training loss 0.044082\n",
      "[END] Fine tuning step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SupervisedDBNClassification(batch_size=32, dropout_p=0.2,\n",
       "                            idx_to_label_map={0: 0, 1: 1, 2: 2},\n",
       "                            l2_regularization=1.0,\n",
       "                            label_to_idx_map={0: 0, 1: 1, 2: 2},\n",
       "                            learning_rate=0.1, n_iter_backprop=500,\n",
       "                            verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SupervisedDBNClassification</label><div class=\"sk-toggleable__content\"><pre>SupervisedDBNClassification(batch_size=32, dropout_p=0.2,\n",
       "                            idx_to_label_map={0: 0, 1: 1, 2: 2},\n",
       "                            l2_regularization=1.0,\n",
       "                            label_to_idx_map={0: 0, 1: 1, 2: 2},\n",
       "                            learning_rate=0.1, n_iter_backprop=500,\n",
       "                            verbose=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SupervisedDBNClassification(batch_size=32, dropout_p=0.2,\n",
       "                            idx_to_label_map={0: 0, 1: 1, 2: 2},\n",
       "                            l2_regularization=1.0,\n",
       "                            label_to_idx_map={0: 0, 1: 1, 2: 2},\n",
       "                            learning_rate=0.1, n_iter_backprop=500,\n",
       "                            verbose=True)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbn.fit(new_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbn_test_pred =dbn.predict(new_test1)\n",
    "dbn_test_proba =dbn.predict_proba(new_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(np.isnan(dbn_test_pred ).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbn training test score: 0.898936170212766\n"
     ]
    }
   ],
   "source": [
    "dbn_test_acc = accuracy_score(y_test, dbn_test_pred)\n",
    "print(\"dbn training test score: {}\".format(dbn_test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC: 0.898936170212766\n",
      "macro-PRE: 0.872142449484074\n",
      "macro-SEN: 0.8733063547577035\n",
      "macroF1-score: 0.8724969448850685\n"
     ]
    }
   ],
   "source": [
    "print('ACC:', metrics.accuracy_score(y_test,dbn_test_pred)) \n",
    " \n",
    "print('macro-PRE:',metrics.precision_score(y_test,dbn_test_pred,average='macro')) \n",
    " \n",
    "print('macro-SEN:',metrics.recall_score(y_test, dbn_test_pred,average='macro'))\n",
    " \n",
    "print('macroF1-score:',metrics.f1_score(y_test, dbn_test_pred,labels=[0,1,2],average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9916\\1687146651.py:19: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "  mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaMAAAEWCAYAAADLkvgyAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAABkqklEQVR4nO2dd3gVxdeA30MCASmho/QikBCS0JsiIEiXIijyQwREIaCAgIqiImBBVIo0wfZhQUFREAsWFEFBqQZEQhUIHUIvCWnn+2M3l5vkJrmRJDdl3ue5TzK7szNnZ3fn7MycPUdUFYPBYDAYPEk+TwtgMBgMBoNRRgaDwWDwOEYZGQwGg8HjGGVkMBgMBo9jlJHBYDAYPI5RRgaDwWDwOEYZZREislJEBnig3pdEJEJETmR13a4QkZYistvTcmQHROSyiFTP4jpVRG7Nyjozi//6TOWGe1BEWovIkVT2V7bvL6//UPZBEWl3YxKmn3QrIxG5XUTWi8gFETkrIutEpHFmCJcVZFXDq2onVf0gs+txRkQqAWOBOqp6s4v9rUUk3r5pL4nIbhEZlJkyqepvqlo7M+vIjojIryLysPM2VS2iqv96SiZPkhHPnbvPVFIF/F/vQRGZKCIfp/e4rCBpe6pquH1/xXlSrvSQLmUkIsWAb4DZQEmgAjAJuJbxohkygCrAGVU9lUqeY6paBCgGjAbeEZEcpyxExDsv1u0pPNzeIiJmVie3oapu/4BGwPlU9ucDngMOAaeADwFfe19VQIFBwGHgHBACNAa2A+eBOUnKewgIs/P+AFRJpe5mwHq7nG1Aa3t7CyACqGSng+08fsBHQDwQCVwGnkqtLHvfr8CLwDrgEvAjUNreVxD4GDhjH7sJKOd03MPpaKcBQLgt+7OpnLevffxpu7zn7PLb2ecVb5/bQhfHtgaOJNl2CrjXSc6ngf32OX0GlHTKe7tTOx0GBtrbfYA3bPlPAvOBQknrtMtemqT+N4FZTuf2HnAcOAq8BHjZ+wba12AGcBZ4ycX5+QAzgWP2bybg4ywHMN5u44NAvyTHpnoOwDjgBNZ9VALrRe001v36DVDRzv8yEAdE2ddijr1dgVvt/xcCc4Fvse6rDUANJ3naA7uBC8A8YA32/eTivL3s89pvl7WF6/e/Yj13e2055wJi76sB/GJf6whgEVDcqdyD9jlvx3oB9eb6/XEJ2An0TCLLI1jPcML+Bvz35+5l+5pHAreS+Jm61W6TC7bsS+zta+1zvmLX1Yck9z1QCfjSvnZnSNIP2Xk6AtFAjF3ONnt7eWAF1j24D3gklWd1oX3tVtplrANuxrovzwG7gPpO+R33h9PxL7l4jpK1J9f7Ee9U5El2bZyuczv7/ybAH/Y1OQ7MAQrY+wTr+Ttlt/t2oK69r7Nd5iWsZ/eJNPWLO0rISfhi9sX6AOgElHChPPYB1YEi9gX+KEknOx+r026P9XAuB8pijbJOAa3s/D3ssvyxbvrngPUpyFXBlqszVgd6l50u49QZ/AIUshvssSQPWLt0lPUr1sNXyy7vV+BVe99Q4GvgJqwOoSFQzIUycqed3rHLD8Z68P1TOPcPga+Aovaxe4DBKSmblJSRfa7dsG7q+va2x4E/gYpYnfMC4FN7X2WsG60vkB8oBdSz983EekBL2nJ9DUxxUWcV4KpTG3lh3fDN7PRyu87CWPfIRmCokzKKBUZg3R+FXJzfZFv+skAZrI7uRSc5YoHp9rm1wuqwart5DrHAVPvYQvb597KvfVHgc2B5ks704STyJVVGZ7Eefm8sRbDY3lcauAjcY+8bhdUppqSMngT+BmpjdRjBQCmnOr8BitvX8DTQ0alDv8s+pzJYHfnMJM9KKFbnnaCY78XqkPNhdfRXgFuc9h3FeuEUu/wqN/DchQMBdhvkJ/Ez9SnwrH1sQeD2VDr11ly/B72wFN8MrPss0bFJ2nUi8HGSbWuwFExBoJ7dnm1TUUYRWP1CQaw+6QDwoC3HS8Dq9CqjFNqzKqkoI3evjS1rM7vNq2Ipr8ftfR2wXnSK22X4O13740BL+/8S2IouVf2SVgYXJ+FvN8oRrAdyBdff/n8GhjvlrY310CSciAIVnPafAfo4pb9wOtGV2J2qU2d5FRejI6y3tY+SbPsBGGD/n99utL+B77HfBFO4iGmV9SvwnNO+4cD39v8PYXV4QS5k/JXrD4477VTRaf9G4H4XZXphKao6TtuGAr+6umFdHN8aS/mct8uJS2h/e38YTg8WcIuTnM8Ay1yUKVgdkvNbfXPgQAoP0e/Ag/b/dwH77f/L2TIVcsrbF/thxVJG4Wncq/uBzk7pDsBBJzligcJO+z8DnnfzHKKBgqnUXQ845+r6O21LqozeddrXGdhl//8g8EeSNj6ctDyn/buB7insUxJ31J8BT6eQtwfwV5Jn5aE02jw0oW6s52ZUCvkOkv7nbnIqz9SHwNs4PTeu2jnpPWhf19OkMoJwOm4iTsoISynHAUWdtk3BxSyE0zV+xyk9AghzSgfiNPPkQu6FZJwycvvaJNn3OPZzD9yJ9fLbDMiXJF84Vl9ULK12Tfile95VVcNUdaCqVgTqYr0VzbR3l8eaKkrgEFbHVc5p20mn/yNdpIvY/1cB3hSR8yJyHuutUbDeoJJSBbg3Ia+d/3aszhNVjcG6kHWBaWq3VgqkWpaNs2XaVSeZP8K6yItF5JiIvCYi+V3U4U47pVSHM6WBAi7KctVGKXFMVYtjjXpnYd1gCVQBljm1QxjWw1cO60Hc76K8Mlijgy1Ox31vb3fFJ1hKBuB/djqh7vzAcadyFmCNchI4nMa5uWrn8k7pc6p6xcV+d87htKpGJSRE5CYRWSAih0TkItaoong6rZlSuublcTpX+/5N0ZKKlK9NqvWISFkRWSwiR+1z+BjrHnMmUZuLyIMiEurUTnWdjklLDmfcee5Su95PYfUPG0XkHxF5yM16KwGHVDXWzfzOlAfOquolp21pPX/u9n8Zhm09eNn+/WNvduvaiEgtEflGRE7Y98Qr2NdXVX/BmrabC5wUkbdtuwKwZgk6A4dEZI2INE+rrhtaBFTVXVzv5MGal6/ilKUy1tvnSdLPYawpmeJOv0Kquj6FvB8lyVtYVV8FEJEKwAvA/wHTRMTH+TTSU1ZqqGqMqk5S1TpYa1Vdsd5qk5JR7RSBNVJJWtbRdJaDql7DejsNFJEe9ubDQKckbVFQVY/a+2qkIFMkEOB0jK9aRhKu+BxoLSIVgZ5cV0aHsUZGpZ3KKaaqAc5ip3Fartr5mFO6hIgUdrHfnXNIWvdYrBFuU1UtBtxhbxc3ZU2N41hTpVaBIuKcdkFK1yYtpmDJGWSfwwNclz8Bx3mISBWs6eTHsKYBiwM7nI5JTY7/8tyl2IaqekJVH1HV8lhv5PPcNGE/DFR20yAjaf3HgJIiUtRp2396/lLgKtZLUQLJLGJTke36Dst6sIj9S3h+3L1H3sJay6pp3xPjcbonVHWWqjbEmj6thTVFjKpuUtXuWC+Py7FG4KmSXms6PxEZa3ccCabDfbHm5cGatx0tItVEpAiWFl3yH9865gPPiEiAXZeviNybQt6PgbtFpIOIeIlIQdtsuaL94C7EWggfjPVgv+h07EmstZs0y0pLYBFpIyKB9tvwRSxF4cq0MkPaSS2zzc+Al0WkqN05jLHPId2oajQwDZhgb5pvl10FQETKiEh3e98ioJ2I3Cci3iJSSkTqqWo8Vgc1Q0TK2sdVEJEOKdR5Gmu65f+wpsHC7O3HsYxDpolIMRHJJyI1RKRVOk7pU+A5W+7S9nklbZtJIlJARFpivTx8nt5zsCmKpcDOi0hJrJcfZ5LeZ+nhW+yXBLvTfJTUO6Z3gRdFpKZleCZBIlLKjXqKYi2An7df4J5MI39hrE7wNIBYnwXUddr/LvCEiDS05bg14V4iA587u+57nfKes+VKePZSa/uNWH3CqyJS2K73thTyngSqim3Jp6qHsablp9jHBWH1MYvckdkNQoH/2e3REWtdMyXSe3+ldm2cKYrVl10WET9gWMIOEWksIk3Fmv25gmUDEGc/T/1ExNeelbqI634wEekdGV0CmgIbROQKlhLagfVWCPA+1lTVWqyFuSisedF0o6rLsBaIF9vDwx1YRhOu8h4GumNp7dNYWv9JrPMbiTWt9Lw9vTEIGGR3PmC9DT4n1tTAE2mUlRY3A0uxGj8Ma3HTlWLIsHayj7sC/Iu1/vKJXf5/5X2sN8W7sSzbVgA/isglrOvdFKzvGLCG4WOxplBDsRbKwRph7QP+tK/dKqxRQ0p8gmX990mS7Q9iTUPuxOpglpJ42iYtXgI2Yxmt/A1stbclcMIu9xhWBxJij/b/yznMxDJkiMBqp++T7H8T6C0i50RkVjrOAVWNwFpwfg1rnbWOfV4pfVIxHesl5Uese/E9W7a0mIRl7XYBSwF+mYZcO7FeXv7A6gwDsSzEEvZ/jmU89AlW37EcyyAEMva5A2shfoOIXMa6Z0ep6gF730TgA7uu+5KcQxxwN9YCfjjW9GefFOr43P57RkS22v/3xVqfOQYsA15Q1Z/clDktRtmynQf6YbVfSiRqz7QKTuPaOPME1vT5JawXtCVO+4rZ285hTU+ewbJABegPHLSfnRCsUXaqJJh0Ggx5ChFpjbUY7dabd3bCfjM/gmWKvtrT8hgMGYH5cMxgyAHY01fFxVrvTJi3/zONwwyGHINRRgZDzqA5lvVTBNbUTQ9VjfSsSAZDxmGm6QwGg8HgcczIyGAwGAweJ8c5eCxdurRWrVrV02IYDAZDjmLLli0RqprSx+ceJ8cpo6pVq7J582ZPi2EwGAw5ChE5lHYuz2Gm6QwGg8HgcYwyMhgMBoPHMcrIYDAYDB7HKCODwWAweByjjAwGg8HgcYwyMhgMBoPHyTTTbhF5H8sl/ylVretiv2B5Mu6MFbdjoKpuTZrPYDAYXLFuXTgXL153XN6iRSV8fQsmy3fhQhTr11+Py1esmA+33VbZZZk7dpzi8OELjnRAQFkqV/Z1mXflyr2J0p061XSZLzz8Av/8c8qRrlTJl7p1y7rMmxXnlF3JzO+MFmJFAfwwhf2dgJr2rylWEKemmSiPwWDIIcTFxfPvv+coUqQAt9xS1GWekSO/Z+vW44705s2P0LBh+WT59u49S+fO16OTNGx4C5s3D3FZ5qxZG3jnnevvxAsWdGXIkIYu8zqXCaCaNISVxcqVewkJ+daRHjKkAQsW3O0yb+ad00aX+7ITmeqbTkSqAt+kMDJaAPyqqp/a6d1AazuoWoo0atRIc9pHr7O+ucTf4TGeFsNgMORBdqxewO71H3HuWNgWVW3kaXlSwpNrRhVIHNP+CCnEjheRISKyWUQ2nz59OkuEy0iMIjIYDJ6iZIUAzh/f7Wkx0sST7oDExTaXwzRVfRt4G6yRUWYK5YqMGtm8M9xVIMWcQcLF+q+Nr6qcPn2VuLh4YmPjiYtTqlYt7jLvsWOX2Lv3DLGxVt6KFYsREOB6jn3p0p2cPn2FuDglNjaeBx4IonTpm5LlO3jwPLNmbXCUWaWKL+PG3e6yzBkz/uCHH/Y75HzqqRYu1wOio+Pw85vjqDtfPuHw4dEuy5w9ewMjR14P/vroo42ZM6ezy7yBgW+xY8f1NYbt20MIDCyXLN/69Ye57bbrQX2bNavIH38MdlnmoEFfsXBhqCP93nvdeOih+snyxcTEUaDA9WC43t75iIl53mWZc+ZsZMSIlY708OGNmDu3i8u8wcHz2b79pCMdGjqU4ODkkdN/++0Qd9yx0JGuW7csf/89LHGmadbdOOLgt+zbd86xedasjtSsmTy6+p49Zxg16nrb16xZklmzXAaN5s03/+T77/c70iNHNklxLahTp8TRxVeu7Ocy33ff7WX27OvTZJ063crIka5XJEaM+O6Gz+nw4cN88803DBs2zD6nslxu+QZr145xWWd2wZPK6AhQySldESt0b7YjIxRRYOX8buXrAnx3w7VlHSNHruTDD7c5Ou633urCwIH1kuWLi1PKlXvDkfbyEmJjJ7gsc9myMB577HonN2xYI+bNc93JvfTSWrZtu97JtWlT1aUyOnnyMjNmXI9F17hx+RSV0Y4dp/jhh+sd0gMPBLrM5+UlHDhw3pHOl8/V+5WFt3fiSYjY2Pgbzpu+MhPLFhfnOq+XV/IyVRXL3ij1+uPiUn5VcVdW54X9MmVuokKFoinWP3u2a2WelFq1SqWoKJIyalQzRo1q5lZed8vs3LkmnTu7VmhJuZFzio2NZdasWUyYMIErV65Qt25dWrZs6TgnEaOMUmIF8JiILMYyXLiQ1nqRp8mKkU12VkSuHpOoqFguXLhu/RMTE+fyWC+vpJ2hpqOTS7mTddV5uiIzlEFS5RMfn1HnlPh4d88ptTLTc04i4LyUHB+vyWRKT5ng/jmVKFGIX38dgJ9facqVK5JieYbEbNiwgaFDh7Jt2zYAevXqRfXq1T0sVfrITNPuT4HWQGkROQK8AOQHUNX5WP1uZ2Aflmn3oIyqO1cYDMikRMm4uAku37zffnsLQ4d+40gPHlyfd9/t5rLIxo3fYfPm64PPDRsepkmT5Mt0oaEnqF9/gSMdHFyO0NAQl2W628mICPnyCfHx13u5jOjk/usoIrW3eHcVnIjg7Z0v0f64OE02CgHIn9+LAgW88PbOh7d3Pnx8Un70atcubZdj5S1UyPWoumTJQtx3XwDe3vnw8pIUpz0BunSpRblyRRxlNm1aMcW8ixbdg5dXPke5rpQrQI8efjRqVB4vL6sdSpQolGKZa9YMJF8+K5+l8FIeRbZqVTXFfYbEnDt3jvHjx7NgwQJUlapVqzJnzhy6dHE9k5CdyTRlpKp909ivwKOZUXdmKCJ3p9lc8V+m3goU8CI6+vooIzo6joIFk18uHx+vRGnnY/5r3qT5rl1Lucz0KI6bby5CfLw6OrnY2PhkHT/ALbcU5Y47qjg6uTp1Ug7B0rOnH/Xr3+zoZMuWLewyX/nyRZk2rb2j7jJlXOcDGD68MXffXctRZq1ayefsE9i9+zGHnF5e+VwqV4CBA+u5nL50xaJF97iVr2rV4ixZ0tutvF271qJr11pu5e3b1/W0ZFJKl77J5ZSoK1JSqHzZBQ5k5/mA7M2kSZOYP38+3t7ePPHEEzz//PPcdJN71yS7kePCjrtj2v3IvLNA9jEYSPkd0DWdgbVFp3D5crRj24ULT1OsmE+yvN9+u4enn/6ZAgW88PHxokOHGrzwQmuX5c6du5FDhy7g4+OFj483/fsHUaVK8WT5rlyJ5vffw/Hx8cbHx4uiRX1S/Ejv8uVoYmLiHB13gQJeLhWMweCSael9OmyqdYZ7vk07Xy4kNjYWb2/rxTQiIoLBgwfz8ssvU7dusi9oEiEi2dq02yijGyQ9o551SayfKlUqRni4a+uro0cvkj+/pWAKFPCiYEHvVKc2DIYcSYIyGpuz+iFPEBUVxdSpU1m+fDkbNmygQIEC6To+uyujHBfpNbvhriLqjPWVtI+Pl2Pa6/Dhi4SHX3DpbqRChWIZJ6TBYMjR/PzzzwwbNoy9ey0XRD/88AN33+3ai0NOxSijjMLJ4MDLS7h27bnk01U+3vTpU5f8+fPRsmVlbr+9MpUqGaVjMBhcc/LkScaOHcuiRdY3Tf7+/rz11lu0atXKw5JlPEYZuUl0dBwFClxf2E86PVeqVCHOnIkELIuqEycuuxzdfPBBj8wV1GAw5Ao+/vhjRowYwfnz5ylYsCATJkxg7Nix6Z6eyykYZeQGv/12iAceWMby5X2oX/8WILEi6gycrFqcggW9qVzZl0qVfFM1HTYYDIa0iI+P5/z583Ts2JG5c+fmuO+G0otRRmnw1Ve76NNnKdeuxdGx4yLWrXuIW2+9bhiRoHLiNz6S6hf4bmHMXA2GPMvly5f5448/uOuuuwDo378/5cuXp23btnnCeCnPKqNLl65x6tQVx++22yon+2YiLOw0PfJ7QdRzAJzCinfhihtWRGAUkSFvUs09Fzi5meXLlzNixAhOnz7Njh07uPXWWxER2rVr52nRsow8q4y6d1/M6tUHHekffniA9u1rJMrj718G/FP+4DLTHiFj5mow5AkOHTrEyJEjWbFiBQCNGjXi2rVraRyVO8mzXycm/QL/1KkrqebP5zWZt9/ZgoLjlzc/uTMYDDdKTEwMr7/+OnXq1GHFihUULVqU2bNn8+effxIQEOBp8TxCrhkZpeWPLtnHqUt6Wz+b/vYvJX79dQAtW1a5QSkNBoMBRo4cyfz58wG47777mDFjBuXLJ4/ompfINSOjpIooqS+5G1mN6QxGERkMhgzj8ccfx9/fn5UrV7JkyZI8r4ggB46MDp2Odbj7cYUrF0AxMXGQ3/pGyKzGGAyGrERV+fjjj/nuu+/45JNPEBFq167Njh07yJcv14wHbpgcp4xSI7Byfte+4vJ7ucj9HzCm1waDIR3s3r2bYcOGsXr1asAy1+7c2TJ9MoooMTlSGaXmAHVUKsfdsPVbVigiY+ZqMOR4IiMjmTJlClOnTiU6OppSpUoxbdo0OnVyHe7ckEOVkTs0bfYuGzYcdaSLFi3A22GPQkY4IDWm1waDIQVWrVpFSEgI+/dboesHDx7M1KlTKVUq5bhYhlxkwJCUDz7oQZEi1304lSlTmJMnUzffNhgMhhtl/fr17N+/n4CAAH777Tfeffddo4jcINcqo9q1S/POO5aL9cGD6/P338No0OAWD0tlMBhyG3FxcezevduRHjduHHPnzmXr1q3cfvvtHpQsZ5Frp+kA7r+/LlWrFqdZs4qeFsVgMORC/vrrL0JCQvj333/ZvXs3JUuWxMfHh+HDh3tatBxHrlZGQHJFZCziDAbDDXLp0iUmTJjArFmziI+Pp0KFCuzfv5+SJTM/unRuJddO06XIjSoiY+1mMORZVJUvvvgCf39/Zs6cCcDo0aMJCwujcePGnhUuh5PrR0YpYiziDAZDOnn88ceZNWsWAI0bN2bBggXUr1/fw1LlDnLNyCgi4irLloU50hs2HPGgNAaDITfSs2dPfH19mTt3Ln/88YdRRBlIrlFG//xzinvu+cyRfvLJnzwojcFgyA38/vvvvPjii45069atCQ8PZ/jw4Xh5ZZBnFwOQi6bpkob59vLKNXrWYDBkMWfOnGHcuHG89957ALRt25YWLVoAUKxYBnw4b0hGrlBGXYDv7qwG+oJjW/78RhkZDIb0oap8+OGHPPHEE0RERJA/f36efvppMx2XBeQKZZTUPs7r+30EBpZNvNGYdBsMhlQICwtj2LBhrFmzBoA2bdowb948/Pz8PCxZ3iBXKKMEEibqrrWpytWmFRLvdFZExjzbYDAkYfr06axZs4YyZcowffp0+vXrh4h4Wqw8Q65SRgn4+Hjj45PCqRmTboPBYHPhwgV8fX0BmDJlCoULF2bChAnm41UPYBZWDAZDnuPYsWP06dOHZs2aER0dDUDp0qWZOXOmUUQewigjg8GQZ4iLi2P27Nn4+fnx2WefER4eztatWz0tlgGjjAwGQx5hy5YtNG3alJEjR3Lp0iW6detGWFgYzZo187RoBjJZGYlIRxHZLSL7RORpF/t9ReRrEdkmIv+IyKD01rF375mMEdZgMORaJk6cSJMmTdiyZQuVKlVi+fLlfPXVV1SuXNnTohlsMk0ZiYgXMBfoBNQB+opInSTZHgV2qmow0BqYJiIFSAeLF+9w/F+v3vxEacAy6Z5mLGIMhrxM9erVERHGjh3Lzp076d69u6dFMiQhM63pmgD7VPVfABFZDHQHdjrlUaCoWPaTRYCzQGx6Ktm9+/rIaNu2k5w/H5U4gzHpNhjyHP/++y+bNm2iT58+APTv35+mTZtSu3ZtD0tmSInMnKarABx2Sh+xtzkzB/AHjgF/A6NUNT5pQSIyREQ2i8jmpPuclRFA7dophPcdq3DPt+kQ32Aw5DSio6N55ZVXCAgIYMCAAezbtw8AETGKKJuTmcrI1dxY0o98OgChQHmgHjBHRJI5flLVt1W1kao2SrqvcmXfROnatUv/R3ENBkNOZu3atdSrV49nn32WqKgoevfubfzI5SAyUxkdASo5pStijYCcGQR8qRb7gANAunxvfPHFfY7/Q0OHcsstRf6btAaDIUcSERHBoEGDaNWqFWFhYdSsWZNVq1bx8ccfU7Zs2bQLMGQLMlMZbQJqikg12yjhfmBFkjzhQFsAESkH1Ab+/a8VBgffbNx3GAx5jJCQEBYuXIiPjw+TJk1i+/bttG3b1tNiGdJJphkwqGqsiDwG/AB4Ae+r6j8iEmLvnw+8CCwUkb+xpvXGqWrEDVVsHKIaDLme+Ph48uWz3qVffvllIiMjmTlzJjVr1vSwZIb/iqjmLF9tZarU09OHQhNtSxgLKbg2467W2RgvGAy5gKtXr/Liiy8SGhrKd999Z2ZC0oGIbHG17p5dyJWOUgHjENVgyGV8++23PPbYYxw8eBARYePGjTRt2tTTYhkyCOMOyGAwZGuOHDlCr1696Nq1KwcPHiQ4OJj169cbRZTLyNHK6J13tjBmzA+O9KFD5z0njMFgyHDmzZuHv78/X375JYULF2b69Ols3rzZ+JPLheRoZfTVV7uZMeNPR3rr1uMelMZgMGQ0ERERXL58mZ49exIWFsbo0aPx9s69qwt5mRx9VU+dupIoXa5ckRswDDcYDJ7m/Pnz7Nq1yzHyGTduHE2aNKFjx44elsyQ2eTokdHJk4mVUdmyhT0kicFguBFUlcWLF+Pv70+3bt04e/YsAD4+PkYR5RFy9MjolVfu5MiRiyTEprj5ZuN9wWDIaezbt49HH32UH3/8EYAWLVpw4cIFE3E1j5GjlVG/fkEADmVUpEi6ok8YDAYPcu3aNV577TVefvllrl27RokSJXjttdd46KGHHB+0GvIObisjESmsqlfSzmkwGAxp06dPH7766isAHnzwQV5//XXjSy4Pk+brh4i0EJGdQJidDhaReZkumcFgyNU8/vjj+Pn58csvv/DBBx8YRZTHcWdkNAMr1MMKAFXdJiJ3ZKpUBoMhVxEfH8/7779PWFgY06ZNA6B169bs2LEDLy8vD0tnyA64NU2nqoeT+ICKyxxxDAZDbuPvv/8mJCSE9evXA9aUXHBwMIBRRAYH7qwSHhaRFoCKSAEReQJ7ys6TPPfcL4wf/zORkTGeFsVgMLjgypUrPPXUU9SvX5/169dz8803s3jxYoKCgjwtmiEb4s7IKAR4Eytk+BHgR2B4ZgqVFqGhJ3j11d+Ji1M+++wf2DfSk+IYDIYkfP311zz22GOEh4cjIjz66KO8/PLL+Pr6pn2wIU/ijjKqrar9nDeIyG3AuswRKW2eeeZn4uIsr9z795/zlBgGgyEFli9fTnh4OPXr12fBggU0btzY0yIZsjnuTNPNdnNblrF69QFPVm8wGJIQGxvLoUOHHOmpU6cye/ZsNm7caBSRwS1SHBmJSHOgBVBGRMY47SqGFbnVY4wZ05yjRy9x9OhFTp26wt+eFMZgyOP8+eefhISEcO3aNbZt20aBAgUoXbo0jz32mKdFM+QgUpumKwAUsfMUddp+EeidmUKlxSuvJI5vb2I9GgxZz7lz5xg/fjwLFixAValatSoHDx6kVq1anhbNkANJURmp6hpgjYgsVNVDKeUzGAx5C1Xl008/ZfTo0Zw6dQpvb2+efPJJnnvuOW666SZPi2fIobhjwHBVRF4HAoCCCRtV9c5Mk+o/8M2XXeDAd54Ww2DI9fTr149PP/0UgJYtW/LWW28REBDgYakMOR13DBgWAbuAasAk4CCwKRNl+k90cVZE1Tp7ThCDIZfTsWNHSpUqxfvvv8+vv/5qFJEhQxBVTT2DyBZVbSgi21U1yN62RlVbZYmESShTpZ6ePhSaWEZAp9krR2NTPx+DwZA+Vq1axf79+xk6dChgTdOdO3fOhHjIYdh9eSNPy5ES7kzTJbg4OC4iXYBjQMXME8lgMGQHTp48yZgxY/jkk0/w8fGhXbt21KhRAxExisiQ4bijjF4SEV9gLNb3RcWAxzNTqLSoUWMWNWqUoEaNEpZlXYlCnhTHYMhVxMfH8/bbb/P0009z4cIFChYsyIQJE6hUqZKnRTPkYtJURqr6jf3vBaANODwweIx//z3Hv/+eY9UqmDHDhCQ2GDKKbdu2MXToUDZs2ABAp06dmDNnDtWrV/ewZIbcTmofvXoB92H5pPteVXeISFdgPFAIqJ81IqZMhQrFKFgwRwerNRiyFU899RQbNmygfPnyvPnmm/Tq1YskHvsNhkwhtZ78PaASsBGYJSKHgObA06q6PAtkS5MaNUp4WgSDIUejqly9epXChQsDMGvWLObPn8+kSZMoVqyYh6Uz5CVSU0aNgCBVjReRgkAEcKuqnsga0VJm27YQ9u8/S6FC+T0tisGQYzl06BAjRozgypUrrFq1ChGhdu3azJgxw9OiGfIgqSmjaFWNB1DVKBHZkx0UEUBQUDmCgsp5WgyDIUcSExPDjBkzmDRpElevXqVo0aLs3bvXuPExeJTUlJGfiGy3/xeghp0WQBO+OTIYDDmHdevWERISwo4dOwDo06cP06dPp3z58h6WzJDXSU0Z+WeZFAaDIdMZMWIEc+bMAaB69erMnTuXjh2NNaohe5Cao1TjHNVgyEWUKVOG/PnzM27cOMaPH0+hQub7PEP2IU13QDdUuEhHrJDlXsC7qvqqizytgZlAfiAiLTdDidwBuXKOatwBGQwA7Nq1i/DwcNq3bw/AtWvXOHDgAH5+fh6WzOAJsrs7IHccpf4n7O+U5gKdgDpAXxGpkyRPcWAe0E1VA4B73Sl7587TdIqLT66IjINUg4HIyEief/55goKCeOCBBzh79iwAPj4+RhEZsi1ufTEqIoWAyqq6Ox1lNwH2qeq/dhmLge7ATqc8/wO+VNVwAFU95U7BAQHzQF+4Lt9YpTPwbTqEMxhyIz/++CPDhw9n//79AHTr1s18tGrIEaQ5MhKRu4FQ4Hs7XU9EVrhRdgXgsFP6iL3NmVpACRH5VUS2iMiDbkmdBMUoIkPe5vjx49x///106NCB/fv3ExAQwG+//ca7775LiRLm43BD9sedabqJWKOc8wCqGgpUdeM4V69jSRd0vIGGQBegA/C8iCT72EFEhojIZhHZ7Ea9BkOe45577mHJkiUUKlSIqVOn8tdff3H77bd7WiyDwW3cUUaxqnrhP5R9BMudUAIVscJPJM3zvapeUdUIYC0QnLQgVX1bVRslLL5VqeL7H8QxGHIXzsZHr776Kl27dmXnzp089dRT5M9vvJMYchbuKKMdIvI/wEtEaorIbGC9G8dtAmqKSDURKQDcDySd3vsKaCki3iJyE9AUCEur4IMHH3ejeoMhd3Lp0iVGjx7tCHYH0KpVK77++muqVq3qOcEMhhvAHWU0AggArgGfYIWSeDytg1Q1FngM+AFLwXymqv+ISIiIhNh5wrDWorZjOWR9V1V3pCnRl12uR3Y1GPIIqsoXX3yBv78/M2fO5P/+7/84ePCgp8UyGDIEd8KO11fVv7JInjQpU6Wenh657fqGap3hHmO+YMjdHDhwgMcee4zvvrM+Z2jSpAnz58+nfn2PR3Ix5BCy+3dG7ph2TxeRW4DPgcWq+k8my+Q2MlaTWUQYDLkJVeW1115j0qRJREZG4uvry5QpUxgyZAheXl6eFs9gyDDSnKZT1TZAa+A08LaI/C0iz2W2YAaDAUSEPXv2EBkZSd++fdm1axfDhg0zisiQ60iXOyARCQSeAvqoaoFMkyoVnKfpzMjIkBuJiIjgxIkT1K1b15H+66+/uOuuuzwsmSEnk92n6dz56NVfRCaKyA5gDpYlXcVMl8xgyGOoKgsXLsTPz497772X6OhoAEqXLm0UkSHX486a0f8BnwLtVTXpd0IGgyEDCAsLIyQkhLVr1wIQHBzMuXPnKFfOBJE05A3SVEaq2iwrBDEY8iJXr17l5Zdf5vXXXycmJoYyZcowffp0+vXrZ3zKGfIUKSojEflMVe8Tkb9J7MbHRHo1GDIAVeXOO+9kw4YNAAwdOpQpU6YYX3KGPElqI6NR9t+uWSGIwZDXEBGGDx/O1atXWbBgAc2bN/e0SAaDx0jRgEFVj9v/DlfVQ84/YHjWiGcw5B7i4uKYPXs206dPd2zr378/W7ZsMYrIkOdxxx2QKzOeThktiMGQm9m8eTNNmzZl5MiRjB8/nmPHLFsgETFOTQ0GUlFGIjLMXi+qLSLbnX4HsHzJGQyGNLhw4QIjRoygSZMmbNmyhUqVKrFkyRLKly/vadEMhmxFamtGnwArgSnA007bL6nq2UyVymDI4agqn3/+OY8//jjHjx/Hy8uL0aNH88ILL1CkSBFPi2cwZDtSU0aqqgdF5NGkO0SkpFFIBkPqLFiwgOPHj9OsWTPmz59PcHCyUF0Gg8EmrZFRV2ALlmm380cPClTPRLkMhhzHtWvXOH/+POXKlUNEmDdvHr/++iuPPPII+fK5szxrMORdUlRGqtrV/lst68RJH509LYDBYLNmzRpCQkIoX748q1atQkSoXbs2tWvX9rRoBkOOwB3fdLeJSGH7/wdEZLqIVM580dLGRDEyeJrTp08zcOBAWrduza5duzh8+DAnT570tFgGQ47DnbmDt4CrIhKM5bH7EPBRpkplMGRz4uPjee+99/Dz8+ODDz7Ax8eHSZMmsX37dm6++WZPi2cw5DjccZQaq6oqIt2BN1X1PREZkNmCGQzZFVWlQ4cOrFq1CoB27doxb948atas6WHJDIacizsjo0si8gzQH/hWRLwA85WeIc8iIrRs2ZJy5crxySef8OOPPxpFZDDcIGkG1xORm4H/AZtU9Td7vai1qn6YFQImxTm4HmNNaD1D1vDtt98SExNDjx49AMtyLjIykuLFi3tULoPBXXJ8cD1VPQEsAnxFpCsQ5SlFZDBkNUeOHKFXr1507dqVRx55hLNnrc/rfHx8jCIyGDIQd6zp7gM2AvcC9wEbRKR3ZgtmMHiS2NhYZsyYgb+/P19++SWFCxdm/PjxFCtWzNOiGQy5EncMGJ4FGqvqKQARKQOsApZmpmAGg6fYuHEjQ4cOJTQ0FICePXvy5ptvUqlSJc8KZjDkYtxRRvkSFJHNGdwzfDAYchzx8fEMGjSInTt3UrlyZebMmcPdd9/tabEMhlyPO8roexH5AfjUTvcBvss8kQyGrEVVuXbtGgULFiRfvnzMnTuXlStXMmHCBAoXLuxp8QyGPEGa1nQAInIPcDuWf7q1qrosswVLCWNNZ8hI9u3bx/Dhw6lUqRLvvfeep8UxGDKNHGtNJyI1ReQrEdmBZbwwTVVHe1IRGQwZxbVr15g8eTJ169blp59+Yvny5Zw5c8bTYhkMeZbU1n7eB74BemF57p6dJRIZDJnML7/8QlBQEC+88ALXrl1jwIAB7Nq1i1KlSnlaNIMhz5LamlFRVX3H/n+3iGzNCoEMhswiLi6OQYMG8dFHlmvF2rVrM3/+fFq3bu1ZwQwGQ6rKqKCI1Od6HKNCzmlVNcrJkKPw8vLC29ubggUL8txzz/HEE0/g4+PjabEMBgOpGDCIyOpUjlNVvTNzREodY8BgSA9///03UVFRNG7cGIAzZ85w/vx5atSo4WHJDIasJbsbMKQWXK9NVgpiMGQkV65cYeLEicyYMYOaNWuybds2ChQoQKlSpczakMGQDXHnOyODIUexYsUKRowYQXh4OCJCu3btiImJoUCBAp4WzWAwpECmelIQkY4isltE9onI06nkaywiccbnneFGCA8Pp0ePHnTv3p3w8HAaNGjAxo0bmT17tvl41WDI5mTayMiOezQXuAs4AmwSkRWqutNFvqnAD5kliyH3ExcXR+vWrTlw4ABFixblpZdeYvjw4Xh7m8G/wZATcMdrt4jIAyIywU5XFpEmbpTdBNinqv+qajSwGOjuIt8I4AvglIt9BkOqJBjgeHl5MXHiRHr37k1YWBgjR440ishgyEG4M003D2gO9LXTl7BGPGlRATjslD5ib3MgIhWAnsD81AoSkSEisllENrtRryEPcO7cOUJCQnjllVcc2/r378/nn39OhQoVUjnSYDBkR9xRRk1V9VEgCkBVzwHurASLi21JbbFnAuNUNS61glT1bVVtlJ3NEg1Zg6qyaNEi/Pz8WLBgAVOnTuXChQuAFQ7cYDDkTNyZx4ix13UUHPGM4t047gjgHACmInAsSZ5GwGK7EykNdBaRWFVd7kb5hjzGnj17GD58OD///DMALVu25K233sLX19fDkhkMhhvFnZHRLGAZUFZEXgZ+B15J/RAANgE1RaSaiBQA7gdWOGdQ1WqqWlVVq2IF6xtuFJEhKbGxsUycOJHAwEB+/vlnSpUqxfvvv8+aNWsICAjwtHgGgyEDSHNkpKqLRGQL0BZr6q2Hqoa5cVysiDyGZSXnBbyvqv+ISIi9P9V1IoMhAS8vL3777Teio6N56KGHmDp1KqVLl/a0WAaDIQNJM56RiFR2tV1VwzNFojQw7oDyBidPniQqKooqVaoAsHfvXo4fP84dd9zhYckMhpxJdncH5M403bdYoSS+BX4G/gVWZqZQhrxLfHw88+fPp3bt2gwePNhhul2zZk2jiAyGXIw703SBzmkRaQAMzTSJDHmW0NBQQkJC2LBhAwAFChTg8uXLFC1a1MOSGQyGzCbd7oDs0BGNM0EWQx7l0qVLjBkzhoYNG7JhwwbKly/P559/zrfffmsUkcGQR0hzZCQiY5yS+YAGwOlMk8hdqnX2tASGDCA6OpoGDRqwb98+8uXLx6hRo5g8eTLFihXztGgGgyELcec7I+dX01istaMvMkccNzGGC7mGAgUK0L9/f77++mvmz59Pw4YNPS2SwWDwAKla09kfu76qqk9mnUipU6ZKPT19KNTTYhj+IzExMcyYMYPKlStz//33A9boyMvLCy8vLw9LZzDkXrK7NV2KIyMR8ba/FWqQlQIZci/r1q0jJCSEHTt2UKZMGbp27UqRIkVMnCGDwZDqNN1GrPWhUBFZAXwOXEnYqapfZrJshlzC2bNnGTduHO+++y4A1atXZ968eRQpUsTDkhkMhuyCO2tGJYEzwJ1Y/unE/muUkSFVVJWPPvqIsWPHEhERQf78+Rk3bhzjx4+nUKFCnhbPYDBkI1JTRmVtS7odXFdCCRgLghxKTEwMR44cISoqKtPrUlUqVKjARx99hI+PD6VKlSJ//vwcPHgw0+s2GPIqBQsWpGLFiuTPn9/ToqSL1JSRF1AE90JBGHIIR44coWjRolStWjVTQi7Ex8cTHx/vCGxXqVIlrl27RqlSpUyIB4Mhk1FVzpw5w5EjR6hWrZqnxUkXqSmj46o6OcskMWQJUVFRmaaILly4QHh4uEPZARQtWtR8uGowZBEiQqlSpTh92vOfgqaX1JSReY3NpWS0IoqOjubw4cOcO3cOgHz58hEXF2dMtQ0GD5BTZyBSU0Zts0wKQ45EVTl9+jRHjx4lLi6OfPnyUb58ecqWLUu+fOn2NGUwGPIwKfYYqno2KwUx5Czi4+PZtWsX4eHhxMXF4evrS0BAADfffLNRRJnEwYMHKVSoEPXq1aNOnTo8+OCDxMTEOPb//vvvNGnSBD8/P/z8/Hj77bcTHf/hhx9St25dAgICqFOnDm+88UZWn0KaLF++nMmTs+/qwNmzZ7nrrruoWbMmd911l2M2IClvvvmmo61nzpzp2N6nTx/q1atHvXr1qFq1KvXq1QNg48aNju3BwcEsW7YMsPw2JmyvV68epUuX5vHHHwdgzpw5/N///V9mnm7Woqo56le6crAa/js7d+5MlIaJiX4psWDB5kT5HnlkhR44cEC3bdumZ8+e1fj4+MwW3W1iY2M9Vnd8fLzGxcVlStkHDhzQgIAAVbXOsU2bNvrxxx+rqurx48e1UqVKumXLFlVVPX36tDZo0EC/+eYbVVX97rvvtH79+nr06FFVVY2MjNS33347Q+WLiYm54TKaN2+up0+fztI608OTTz6pU6ZMUVXVKVOm6FNPPZUsz99//60BAQF65coVjYmJ0bZt2+qePXuS5RszZoxOmjRJVdWRV1X12LFjWqZMGZfn1qBBA12zZo3jmHr16rmUM+lzrqoKbNZs0Ien9DOvsAa3UBduoypWrEhAQAAlSpRwe5764MGD+Pn58fDDD1O3bl369evHqlWruO2226hZsyYbN24ErDfFFi1aUL9+fVq0aMHu3bsBiIuL44knniAwMJCgoCBmz54NQNWqVZk8eTK33347n3/+OZ9++imBgYHUrVuXcePGuZTl8uXLtG3blgYNGhAYGMhXX30FwLhx45g3b54j38SJE5k2bRoAr7/+Oo0bNyYoKIgXXnjBcU7+/v4MHz6cBg0acPjwYYYNG0ajRo0ICAhw5AP47rvv8PPz4/bbb2fkyJF07doVgCtXrvDQQw/RuHFj6tev75AlJby8vGjSpAlHjx4FYO7cuQwcOJAGDSyHKaVLl+a1117j1VdfBWDKlCm88cYblC9fHrDMfx955JFk5Z48eZKePXsSHBxMcHAw69ev5+DBg9StW9eR54033mDixIkAtG7dmvHjx9OqVStefvllqlatSnx8PABXr16lUqVKxMTEsH//fjp27EjDhg1p2bIlu3btSlb3nj178PHxcUTx/frrr2natCn169enXbt2nDx50nE9hgwZQvv27XnwwQc5ffo0vXr1onHjxjRu3Jh169YBKd9DN8JXX33FgAEDABgwYADLly9PlicsLIxmzZpx00034e3tTatWrRwjnQRUlc8++4y+ffsCOPKCZWTk6nnau3cvp06domXLlo5jqlat6nhmcjye1obp/ZmR0Y3xX0ZGUVFROnnyd8lGRv+FAwcOqJeXl27fvl3j4uK0QYMGOmjQII2Pj9fly5dr9+7dVVX1woULjjfDn376Se+55x5VVZ03b57ec889jn1nzpxRVdUqVaro1KlTVVX16NGjWqlSJT116pTGxMRomzZtdNmyZclkiYmJ0QsXLqiqNZKoUaOGxsfH69atW/WOO+5w5PP399dDhw7pDz/8oI888ohj9NOlSxdds2aNHjhwQEVE//jjD8cxCXLFxsZqq1atdNu2bRoZGakVK1bUf//9V1VV77//fu3SpYuqqj7zzDP60UcfqarquXPntGbNmnr58uVkbZcwMoqMjNTWrVvrtm3bVFW1Z8+eunz58kT5z58/ryVKlFBV1RIlSuj58+fTvD733XefzpgxwyH7+fPnE9Wrqvr666/rCy+8oKqqrVq10mHDhjn2devWTX/55RdVVV28eLEOHjxYVVXvvPNOx+jgzz//1DZt2iSr+/3339cxY8Y40s4j7nfeecex74UXXtAGDRro1atXVVW1b9+++ttvv6mq6qFDh9TPz09VU76HnLl48aIGBwe7/P3zzz/J8vv6+iZKFy9ePFmenTt3as2aNTUiIkKvXLmizZo108ceeyxRnjVr1mjDhg0Tbfvzzz+1Tp06WrhwYf3yyy+TlTtp0iQdO3Zsom0vvfSSvvHGGy5lSArZfGTkjgcGQx4lPj6ekydPcvz4cSIjIzOs3GrVqhEYaMVsDAgIoG3btogIgYGBjg9iL1y4wIABA9i7dy8i4lgbWbVqFSEhIY63yJIlSzrK7dOnDwCbNm2idevWlClTBoB+/fqxdu1aevTokUgOVWX8+PGsXbuWfPnycfToUU6ePEn9+vU5deoUx44d4/Tp05QoUYLKlSsza9YsfvzxR+rXrw9YI6u9e/dSuXJlqlSpQrNmzRxlf/bZZ7z99tvExsZy/Phxdu7cSXx8PNWrV3d8/9G3b1/Hus6PP/7IihUrHOs4UVFRhIeH4+/vn0jm/fv3U69ePfbu3Uvv3r0JCgpynIurt+n0Wlb98ssvfPjhh4A1+vL19U1xXSSBhHZP+H/JkiW0adOGxYsXM3z4cC5fvsz69eu59957HfmuXbuWrJzjx487rhlY38T16dOH48ePEx0dnei7mW7dujm8eKxatYqdO3c69l28eJFLly6leA85U7RoUUJDQ9NolfTh7+/PuHHjuOuuuyhSpAjBwcGO+zWBTz/91DEqSqBp06b8888/hIWFMWDAADp16kTBggUd+xcvXsxHH32U6JiyZcu6HGXmRIwyyuOovuBy+6VLlzh06JDDU8PDD9dj0qS7M+Srbh8fH8f/+fLlc6Tz5ctHbGwsAM8//zxt2rRh2bJlHDx4kNatW9vyuu50AQoXLuzI44oNGzYwdKgVpHjy5MmcPXuW06dPs2XLFvLnz0/VqlUd59u7d2+WLl3KiRMnHN7FVZVnnnnGUUYCBw8edNQNcODAAd544w02bdpEiRIlGDhwIFFRUSnKlVD2F198Qe3atVPMA1CjRg1CQ0M5fvw4rVu3ZsWKFXTr1o2AgAA2b95Mt27dHHm3bNlCnTp1AEvpb9myhTvvvDPV8l3h7e3tmHoDknnvcD73bt268cwzz3D27FlHfVeuXKF48eJpdvqFChXiwoULjvSIESMYM2YM3bp149dff3VMDSatMz4+nj/++COZi6kRI0a4vIecuXTpkmPaKymffPKJo/0SKFeuHMePH+eWW27h+PHjlC1b1uWxgwcPZvDgwQCMHz+eihUrOvbFxsby5ZdfsmXLFpfH+vv7U7hwYXbs2EGjRpaT7W3bthEbG5ssxEpUVFSuca1l1owMyYiPj2f//v1ERUXh4+NDrVq1qF69epa6F7lw4QIVKlQAYOHChY7t7du3Z/78+Q6ldfZscqPPpk2bsmbNGiIiIoiLi+PTTz+lVatWNG3alNDQUEJDQ+nWrRsXLlygbNmy5M+fn9WrV3Po0CFHGffffz+LFy9m6dKl9O7dG4AOHTrw/vvvc/nyZQCOHj3KqVOnktV/8eJFChcujK+vLydPnmTlypUA+Pn58e+//zpGf0uWLHEc06FDB2bPnu1QWH/99Veq7XPLLbfw6quvMmXKFAAeffRRFi5c6Ojwz5w5w7hx43jqqacAeOaZZ3jqqac4ceIEYI1MZs2alazctm3b8tZbbwHW+tzFixcpV64cp06d4syZM1y7do1vvvkmRbmKFClCkyZNGDVqFF27dsXLy4tixYpRrVo1Pv/8c8BSvNu2bUt2rL+/P/v27XOkne+BDz74IMU627dvz5w5cxzphDZI6R5yJmFk5OqXVBGBpWwTZPnggw/o3r27y3IT7ovw8HC+/PLLRKOgVatW4efnl0hBHThwwHFPHzp0iN27dzs+HAfXIymw1tmc1/NyMkYZGYDra4dgjVAqVarELbfcQkBAgEeirj711FM888wz3HbbbcTFxTm2P/zww1SuXJmgoCCCg4P55JNPkh17yy23MGXKFNq0aUNwcDANGjRw2Wn069ePzZs306hRIxYtWoSfn59jX0BAAJcuXaJChQrccsstgNXp/e9//6N58+YEBgbSu3dvLl26lKzc4OBg6tevT0BAAA899BC33XYbYL35z5s3j44dO3L77bdTrlw5fH19AWskGBMTQ1BQEHXr1uX5559Ps4169OjB1atX+e2337jlllv4+OOPeeSRR/Dz86NFixY89NBD3H333QB07tyZRx99lHbt2hEQEEDDhg0dnZ8zb775JqtXryYwMJCGDRvyzz//kD9/fiZMmEDTpk3p2rVronZyRZ8+ffj4448TTd8tWrSI9957j+DgYAICAlwaaNxxxx389ddfjvtw4sSJ3HvvvbRs2dJh1OCKWbNmsXnzZoKCgqhTpw7z588HUr6HboSnn36an376iZo1a/LTTz/x9NNPA3Ds2DE6d74efbpXr17UqVOHu+++m7lz51KiRAnHvsWLFydTLL///jvBwcHUq1ePnj17Mm/evETn7Gzs4My6deto165dhpybp0k1uF52xATXuzHCwsKSrUNERkZy6NAhihUr5rC2MmQOly9fpkiRIqgqjz76KDVr1mT06NGeFivbMGrUKO6+++5c08FmJn/99RfTp09Pto4Erp/z7B5cz4yM8jBxcXEcOXKEnTt3cvnyZSIiIhKtDRgynnfeeYd69eoREBDAhQsXkq0/5XXGjx/P1atXPS1GjiAiIoIXX3zR02JkGGZklMdIeGNKcGqaYNVUpkwZKlSokMzqx2Aw5Dxy4sjI9Dx5jATjhARz3UKFClGlShUTddVgMHgUo4zyGAnm0wlOTcuVK5djvfwaDIbcg1FGeYDNmzdTvHhxbr31VgCHyajz9z4Gg8HgSYwBQy7mwoULjBgxgiZNmhASEuIwmfXx8TGKyGAwZCuMMsqFqCpLlizBz8+POXPmkC9fPho0aODyuxJP4OXlRb169ahbty53330358+fd+z7559/uPPOO6lVqxY1a9bkxRdfTOS5YOXKlTRq1Ah/f3/8/Px44oknPHAG/42+ffsSFBTEjBkz3MqfWet4qsrIkSO59dZbCQoKYuvWrSnmu/POO7l48WKmyJERfPDBB9SsWZOaNWum+GHsoUOHaNu2LUFBQbRu3ZojR4449j311FMEBATg7+/PyJEjHffagQMHaNq0KTVr1qRPnz5ER0cD8Ouvv+Lr6+sI6ZAQ7iI6Opo77rgj2zxjORJPO8dL7884Sk2dffv2aYcOHRRQQJs3b+5wpqnq2oFiVlO4cGHH/w8++KC+9NJLqqp69epVrV69uv7www+qarnI79ixo86ZM0dVLdf81atX17CwMFW1HJ3OnTs3Q2XLrJAEx48f18qVK6frGOd2yki+/fZb7dixo8bHx+sff/yhTZo0cZnvm2++0ccffzxdZWdl+I4zZ85otWrV9MyZM3r27FmtVq2anj17Nlm+3r1768KFC1VV9eeff9YHHnhAVVXXrVunLVq00NjYWI2NjdVmzZrp6tWrVVX13nvv1U8//VRVVYcOHarz5s1TVdXVq1c7nNsmZeLEiY6QHp4mJzpK9bgA6f0ZZZQyFy9e1OLFiyugxYsX1wULFiSLreN8k2bWRUoL5072rbfecnh9fvfdd7V///6J8u7bt08rVqyoqqr9+/fX9957L83yL126pAMHDtS6detqYGCgLl26NFm9n3/+uQ4YMEBVVQcMGKCjR4/W1q1b6+OPP65VqlTRc+fOOfLWqFFDT5w4oadOndJ77rlHGzVqpI0aNdLff/89Wd2RkZGOuuvVq+fwYB0YGKgFCxbU4OBgXbt2baJjTpw4oT169NCgoCANCgrSdevWJZL30qVLeuedd2r9+vW1bt26Du/cly9f1s6dO2tQUJAGBATo4sWLVVV13Lhx6u/vr4GBgcm8PKuqDhkyRD/55BNHulatWnrs2LFk+fr27evonFVVu3fvrg0aNNA6deroggULHNsLFy6szz//vDZp0kR/++03/eijj7Rx48YaHBysQ4YMcSiokJAQbdiwodapU0cnTJiQrL708sknn+iQIUNSPK8E6tSpo4cPH1ZVK95U0aJFVVV1/fr1Du/fV65c0YYNG+rOnTs1Pj5eS5Uq5XgxWb9+vbZv315VU1dGoaGh2qlTpxs+r4zAKKOkhUNHYDewD3jaxf5+wHb7tx4ITqtMo4xSZ9KkSdq/f389efKky/3ZSRnFxsZq7969deXKlaqqOnr0aJ05c2ay/MWLF9cLFy5o/fr1NTQ0NM3yn3rqKR01apQjnfC2nJoy6tKli6PTHDlypL7//vuqarn1b9u2raqmHKrAmTfeeEMHDhyoqqphYWFaqVIljYyMTBaGwRlXYRuc5U0p1MXSpUv14YcfdpRz/vx5PXPmjNaqVcsResFZqSbQpUsXx3moWuEdNm3alCxf5cqV9eLFi450QliMq1evakBAgEZERKiqKqBLlixRVev+6tq1q0ZHR6uq6rBhw/SDDz5IdLxzWI2kvPbaay7DOYwYMSJZ3tdff11ffPFFR3ry5Mn6+uuvJ8vXt29fx331xRdfKOCQfezYserr66vFihXT8ePHq+r1Nk4gPDzcce1Wr16tJUuW1KCgIO3YsaPu2LHDkS82NlZLly6drH5PkBOVUaZZ04mIFzAXuAs4AmwSkRWqutMp2wGglaqeE5FOwNtA08ySKbdx+vRpnnzySdq2bUv//v0By8eZu6banvrcOTIyknr16nHw4EEaNmzIXXfdZcmjKXvkTo/5+apVq1i8eLEj7ewXLCXuvfdevLy8AMu32uTJkxk0aBCLFy92+FhLKVRB0aJFHdt+//13RowYAViOUatUqcKePXtS9e/nKmyDM6quQ10EBgbyxBNPMG7cOLp27UrLli2JjY2lYMGCPPzww3Tp0sURvC9peUlx1b5nz55NdG6zZs1yBIk7fPgwe/fupVSpUnh5edGrVy8Afv75Z7Zs2ULjxo0B61oneLZ2FVYjIQRGAk8++SRPPvlkim31X87jjTfe4LHHHmPhwoXccccdjo+79+3bR1hYmGMN6a677mLt2rXJPhZ1LrdBgwYcOnSIIkWK8N1339GjRw/27t0LWNeuQIECye4Jg3tkpgFDE2Cfqv6rqtHAYiCRt0pVXa+qCcFS/gQqYkiT+Ph43n33XWrXrs0HH3zAs88+64jVkhO+GSpUqBChoaEcOnSI6Oho5s6dC+AIg+DMv//+S5EiRShatKgjDEJapKTUnLelFgahefPm7Nu3j9OnT7N8+XLuuece4HqoggSvzkePHk3W6bjqIG+URYsWOUJdhIaGUq5cOaKioqhVqxZbtmwhMDCQZ555hsmTJ+Pt7c3GjRvp1asXy5cvp2PHjsnKq1ixIocPH3akjxw54tInoXPoiF9//ZVVq1bxxx9/sG3bNurXr+9ow4IFCzoUuaoyYMAARxvt3r2biRMnOsJq/Pzzz2zfvp0uXbokuwZgRdJNMA5w/o0cOfI/n0f58uX58ssv+euvv3j55ZcB8PX1ZdmyZTRr1owiRYpQpEgROnXqxJ9//knp0qU5f/68wxjBudxixYo5DEs6d+5MTEwMERERjrquXbuWKAaRwX0yUxlVAA47pY/Y21JiMLDS1Q4RGSIim0Vks6v9eYkdO3Zwxx138Mgjj3Du3DnatWvHzz//nKXhHTIKX19fZs2axRtvvEFMTAz9+vXj999/Z9WqVYD1Vj1y5EhHGIQnn3ySV155hT179gCWcpg+fXqycpOGFEjwNlGuXDnCwsKIj49PFgbaGRGhZ8+ejBkzBn9/f0qVKuWyXFfxee644w4WLVoEWO79w8PD04xR5CpsgzMphbo4duwYN910Ew888ABPPPEEW7du5fLly1y4cIHOnTszc+ZMlzJ269aNDz/8EFXlzz//xNfX1+GZ3JnatWvz77//OmQoUaIEN910E7t27eLPP/9M8VyWLl3qCKFw9uxZDh06lGJYjaQ8+eSTLsM5uAp30aFDB3788UfOnTvHuXPn+PHHH+nQoUOyfM4+F6dMmcJDDz0EQOXKlVmzZg2xsbHExMSwZs0a/P39ERHatGnD0qVLgcShIk6cOOF44di4cSPx8fGO++PMmTOUKVMmRz6L2YLMmv8D7gXedUr3B2ankLcNEAaUSqvcvLpmdPXqVX3qqafU29tbAS1Xrpx+8sknjrUBd8lu1nSqql27dtUPP/xQVVW3b9+urVq10lq1ammNGjV04sSJic7x66+/1gYNGqifn5/6+/vrE088kaz8S5cu6YMPPqgBAQEaFBSkX3zxhapa60TVq1fXVq1a6aOPPppozejzzz9PVMamTZsUcFhhqVprCffdd58GBgaqv7+/Dh06NFndkZGROmDAgGQGDKmtGZ04cUK7deumdevW1eDgYF2/fn2idjp9+rQ2a9ZMGzZsqIMHD1Y/Pz89cOCAfv/99xoYGKjBwcHaqFEj3bRpkx47dkwbN26sgYGBWrdu3UTyJxAfH6/Dhw/X6tWra926dV2uF6laazDvvPOOqlqh5zt27KiBgYHau3dvbdWqlcO4Ien1XLx4sQYHB2tgYKA2aNDAEY59wIAB6ufnp507d9aePXvq//3f/7msNz289957WqNGDa1Ro4ZjnU9V9fnnn9evvvpKVa3rfuutt2rNmjV18ODBGhUVparWGs+QIUMc99Lo0aMdx+/fv18bN26sNWrU0N69ezuOmT17ttapU0eDgoK0adOmDmOThHqcw6Z7kpy4ZpSZyqg58INT+hngGRf5goD9QC13ys2ryigqKkr9/PxURHT48OEuF6bdITsoI0PO4NixY9quXTtPi5Fj6Nmzp+7atcvTYqhqzlRGmekOaBNQU0SqAUeB+4H/OWcQkcrAl0B/Vd2TibLkSI4cOcJNN91EyZIl8fHxcUSrbNrU2HgYMp9bbrmFRx55hIsXL3okwGJOIjo6mh49eqQ5JWtImUxbM1LVWOAx4AesKbjPVPUfEQkRkRA72wSgFDBPRELNmpBFbGwsM2bMwN/fP5FlUdOmTY0iMmQp9913n1FEblCgQAEefPBBT4uRo8lUR6mq+h3wXZJt853+fxh4ODNlyGls2LCBoUOHsm3bNsBaOI6NjTVxhgwGQ67G+KbLJpw/f57hw4fTvHlztm3bRpUqVfj6669ZunSpUUQGgyHXY3q5bMC5c+eoU6cOJ06cwNvbm7Fjx/L8888n+vbFYDAYcjNGGWUDSpQoQadOndizZw9vvfUWgYGBnhbJYDAYshQzTecBrl27xuTJk1mzZo1j25w5c1i7dm2eUEQmhIRnQ0js2rWL5s2b4+PjwxtvvJFiPtXcHULi0KFDNGzYkHr16hEQEMD8+Y7lbAYPHkxwcDBBQUH07t2by5cvA5Y3jKCgIIKCgmjRooVjbdeEkMgAPG1bnt5fTv/O6Oeff9ZatWopoP7+/lnqcl81e3xnZEJIuEdmhZA4efKkbty4UcePH+/SsWgCuT2ExLVr1xwfs166dEmrVKmiR48eVVV1OKZVtRz4TpkyRVWtsBMJdXz33XeJwm+YEBI39jMjoyzi1KlT9O/fn7Zt27Jnzx78/PyYN2+ew6eXR5gmmfNLB82bN+fo0aMAfPLJJ9x22220b98egJtuuok5c+bw6quvAvDaa6/x7LPP4ufnB1i+04YPH56szMuXLzNo0CACAwMJCgriiy++ABKPNJYuXcrAgQMBGDhwIGPGjKFNmzY8+eSTVK1aNdFo7dZbb+XkyZOcPn2aXr160bhxYxo3bsy6deuS1R0VFeWou379+qxevRqwXAmdOnWKevXq8dtvvyU65uTJk/Ts2ZPg4GCCg4NZv359svNp27YtDRo0IDAwkK+++gqAK1eu0KVLF4KDg6lbty5LliwB4Omnn6ZOnToEBQW5HDmWLVuWxo0bp+m2ZtGiRQ43OAA9evSgYcOGBAQE8Pbbbzu2FylShAkTJtC0aVP++OMPPv74Y5o0aUK9evUYOnQocXFxAAwbNoxGjRoREBDACy+8kGrd7vDDDz9w1113UbJkSUqUKMFdd93F999/nyzfzp07adu2LQBt2rRxtF+BAgUcEY+vXbvmcBkEOMzZVZXIyEiHX8MWLVo4HO82a9YsUaC+Hj16OFxBGdKPWTPKZBKcmo4bN47z589TsGBBnnvuOZ588kkKFCjgafE8SlxcHD///DODBw8GrCm6hg0bJspTo0YNLl++zMWLF9mxYwdjx45Ns9wXX3wRX19f/v77b+C6b7rU2LNnD6tWrcLLy8vhu27QoEFs2LCBqlWrUq5cOf73v/8xevRobr/9dsLDw+nQoQNhYWGJyklw+vr333+za9cu2rdvz549e1ixYgVdu3Z16Stu5MiRtGrVimXLlhEXF+eYEkqgYMGCLFu2jGLFihEREUGzZs3o1q0b33//PeXLl+fbb78FrM8Azp49y7Jly9i1axcikkipppd169axYMECR/r999+nZMmSREZG0rhxY3r16kWpUqW4cuUKdevWZfLkyYSFhTF16lTWrVtH/vz5GT58OIsWLeLBBx/k5ZdfpmTJksTFxdG2bVu2b9+ezGv366+/7rJDv+OOO5L5pzt69CiVKlVypCtWrOh4sXEmODiYL774glGjRrFs2TIuXbrEmTNnKFWqFIcPH6ZLly7s27eP119/PZGj1UGDBvHdd99Rp04dpk2blqzc9957j06dOjnSdevWZdOmTW60rMEVRhllMhcuXODZZ5/l/PnzdOjQgblz51KjRg1Pi2Ux1jNBJEwIicRkdQgJd8ntISQAKlWqxPbt2zl27Bg9evSgd+/elCtXDoD/+7//Iy4ujhEjRrBkyRIGDRrkKHP16tW89957/P77745tJoTEjWGm6TKBK1eucO3aNcDqCOfPn8+SJUtYuXJl9lFEHsSEkEgfGR1Cwl1yewiJpHkCAgKSTaF6eXnRp08fx1QvwPbt23n44Yf56quvHB67EzAhJP47RhllMCtWrKBOnTq89tprjm29evXivvvuyxGxhrISE0LCIqtDSLhLbg8hceTIESIjIwHrHlm3bh21a9dGVdm3bx9gKdevv/7asU4ZHh7OPffcw0cffUStWrUS1WNCSNwgnragSO8vu1rTHTp0SLt3765YAVT1tttu07i4OE+LlYzsZk2nakJIZHUIiePHj2uFChW0aNGi6uvrqxUqVEhkPZZAbg8h8eOPP2pgYKAGBQVpYGCgLliwQFVV4+LitEWLFlq3bl0NCAjQ//3vf472GTx4sBYvXtwRDr1hw4aOOk0IiRv7eVyA9P6ymzKKjo7W119/XW+66SYFtGjRovrmm29mucm2u2QHZWTIGZgQEunDhJC4sZ8xYLgBIiIiHFZBYC2Cz5gxgwoVUgtoazDkDEwICfcxISRuHKOMboBSpUpRunRpqlWrxpw5c+jcubOnRTIYMpT77rvP0yLkCEwIiRvHKKN0oKosWrSIJk2aUKtWLUSEjz/+GF9fX2666SZPi2cwGAw5FmNN5ya7d++mXbt29O/fn+HDh1sLblhTGUYRGQwGw41hlFEaREVF8cILLxAUFMQvv/xCqVKleOCBBzwtlsFgMOQqzDRdKqxatYphw4Y5vjl46KGHeO2115J96GYwGAyGG8OMjFLg5MmTdO3alX379lGnTh3Wrl3Le++9ZxRRBmBCSHg2hERKYRCSopq7Q0gAjBs3jrp16yZyNAuW89xq1ao5PEAk/Xh406ZNeHl5sXTpUsCEkMgQPG1bnt5fZn5nFBcXl+gDy6lTp+qUKVP02rVrmVZnVpMdvjMyISTcI7NCSKQWBsGZ3B5C4ptvvtF27dppTEyMXr58WRs2bOj4uNXVh9AJxMbGaps2bbRTp06J8pgQEuY7owwhNDSUkJAQHn30Ufr37w/gcEOTW3lk3tlMKfed4SXdztu8eXPHd1ophZBo3bo1jz76aLpCSIwYMYLNmzcjIrzwwgv06tWLIkWKODxiL126lG+++YaFCxcycOBASpYsyV9//UW9evVYtmwZoaGhFC9eHLBCSKxbt458+fIREhJCeHg4ADNnzuS2225LVHdUVBTDhg1j8+bNeHt7M336dNq0aZMohMTs2bNp2bKl45iTJ08SEhLicL3z1ltv0aJFi0Tn0717d86dO0dMTAwvvfQS3bt358qVK9x3330cOXKEuLg4nn/+efr06cPTTz/NihUr8Pb2pn379skC6DmXnTQMgjOLFi1iyJAhjnSPHj04fPgwUVFRjBo1yrGvSJEijBkzhh9++IFp06Zx8OBBZs2aRXR0NE2bNnWEShk2bBibNm0iMjKS3r17M2nSJJf1uotzCAnAEUKib9++ifLt3LnTMRpt06YNPXr0cGxv1aoV3t7eeHt7ExwczPfff5+mOfvs2bPp1atXMg/dPXr04JlnnqFfv343dF55lTw/TXfp0iXGjBlDw4YN2bBhA9OnT080LWTIPBJCSHTr1g1wL4RE0v2ucA4hsX37du688840j0kIITFjxgy6d+/u8F3nHEJi1KhRjB49mk2bNvHFF1/w8MMPJyvHOYTEp59+yoABA4iKimLFihXUqFGD0NDQRIoIroeQ2LZtG1u3biUgICDR/oQQElu3bmX16tWMHTsWVXWEkNi2bRs7duygY8eOjhAS//zzD9u3b+e5555L9byThkFwZt26dYna+/3332fLli1s3ryZWbNmcebMGQBHCIkNGzZQqlQplixZwrp16wgNDcXLy8vhq+/ll19m8+bNbN++nTVr1jheQpxJj6PU9IaQABKFkAgODmblypVcvXqViIgIVq9encjx6rPPPktQUBCjR492OD4+evQoy5YtIyQkJFk9JoTEjZFnR0aqyvLlyxk5ciRHjhwhX758jBo1ismTJ+cZh6bpGcFkJCaERGI8FULCVRgEZ3J7CIn27duzadMmWrRoQZkyZWjevLkjtMSUKVO4+eabiY6OZsiQIUydOpUJEybw+OOPM3XqVJdBMU0IiRsjT46MIiIi6NatG/fccw9HjhyhUaNGbNq0iZkzZxq3J1mACSGRPjIjhERqYRASyAshJJ599llCQ0P56aefUFVq1qwJWN8Pigg+Pj4MGjSIjRs3ArB582buv/9+qlatytKlSxk+fDjLly931GVCSNwAnl60Su8vIwwYoqKi1M/PT4sVK6Zz5szJtk5NM4PsZsCwdetWrVSpkkZHR+vVq1e1WrVq+tNPP6mqZdDQpUsXnTVrlqqqbtu2TWvUqKG7d+9WVcvgZNq0acnKHzdunI4aNcqRTljUrlGjhu7cuVPj4uL0nnvuSdVr9xNPPKEPPPCAdurUybGtb9+++tprrznSf/31V7K6p02bpg899JCqqu7evVsrV66sUVFRqXrt7tOnj86YMUNVrcXxhEX0hHaaOXOmPvbYY6qq+ssvvyigBw4c0KNHj2pkZKSqqi5btky7d++uly5d0pMnT6qqtcBfokSJZPUdOnRIa9SooevWrXMpTwJNmzbVvXv3qqrq8uXLtWvXrqqqGhYWpj4+Pi69dv/zzz966623JpLh4MGDGhoaqkFBQRoXF6cnTpzQsmXL3rDX7jNnzmjVqlX17NmzevbsWa1ataqeOXMmWb7Tp087POiPHz9en3/+eVW12joiIkJVrXsrICDAYcBy7NgxVVWNj4/XUaNG6bhx45KVm/S+iYiIUD8/vxs6p4wiJxoweFyA9P7+qzL6/fffHTeeqmpoaKjjhstLZDdlpGpCSGR1CInUwiA4k9tDSERGRqq/v7/6+/tr06ZNE71ctGnTxhFCol+/fnrp0qVkdSe9b0wICaOMUiUiIkIffvhhBXTw4MHpOjY3kh2UkSFnYEJIpA8TQuLGfrl2zUhV+eCDD/Dz8+Pdd98lf/78lC9f3tLABoMhTZxDSBhSx4SQuHFypTXdrl27CAkJYc2aNQC0bt2at956y/F9isFgcA8TQsI9TAiJGyfXKaMjR44QHBxMdHQ0pUuXZtq0afTv3z/PmGu7g2rKJtQGgyFnk1Nnf3KdMqpYsSL9+/cnX758vPrqq46vsw0WBQsW5MyZM5QqVcooJIMhl6GqnDlzJkeal0tO06JlqtTT04dCHenjx48zevRoQkJCaN26NWB9D5IvX65dDrshYmJiOHLkiMtvPAwGQ86nYMGCVKxYkfz58yfaLiJbVLWRh8RKkxw7MoqLi+Ott97i2Wef5eLFi+zbt49NmzYhIkYRpUL+/PmpVq2ap8UwGAyGRGRqry0iHUVkt4jsE5GnXewXEZll798uIg3cKXfr1q00a9aMESNGcPHiRe6++26++OILM+1kMBgMOZRMGxmJiBcwF7gLOAJsEpEVqrrTKVsnoKb9awq8Zf9Nkctnj9K4cWPi4+OpWLEis2fPpnv37kYRGQwGQw4mM0dGTYB9qvqvqkYDi4HuSfJ0Bz60v8n6EyguIrekVui1q+cQEcaMGUNYWBg9evQwishgMBhyOJm5ZlQBOOyUPkLyUY+rPBWA486ZRGQIkBBY5Voc7Jg+fTrTp0/PWIlzHqWBCE8LkU0wbXEd0xbXMW1xnWz9RW5mKiNXw5Wkpnvu5EFV3wbeBhCRzdnZIiQrMW1xHdMW1zFtcR3TFtcRkc1p5/IcmTlNdwSo5JSuCBz7D3kMBoPBkMvJTGW0CagpItVEpABwP7AiSZ4VwIO2VV0z4IKqHk9akMFgMBhyN5k2TaeqsSLyGPAD4AW8r6r/iEiIvX8+8B3QGdgHXAUGuVH025kkck7EtMV1TFtcx7TFdUxbXCdbt0WO88BgMBgMhtyHcVVgMBgMBo9jlJHBYDAYPE62VUaZ5UooJ+JGW/Sz22C7iKwXkWBPyJkVpNUWTvkai0iciPTOSvmyEnfaQkRai0ioiPwjImuyWsaswo1nxFdEvhaRbXZbuLM+neMQkfdF5JSI7Ehhf/btNz0datbVD8vgYT9QHSgAbAPqJMnTGViJ9a1SM2CDp+X2YFu0AErY/3fKy23hlO8XLAOZ3p6W24P3RXFgJ1DZTpf1tNwebIvxwFT7/zLAWaCAp2XPhLa4A2gA7Ehhf7btN7PryChTXAnlUNJsC1Vdr6rn7OSfWN9r5UbcuS8ARgBfAKeyUrgsxp22+B/wpaqGA6hqbm0Pd9pCgaJi+Q4rgqWMYrNWzMxHVddinVtKZNt+M7sqo5TcBKU3T24gvec5GOvNJzeSZluISAWgJzA/C+XyBO7cF7WAEiLyq4hsEZHcGhfbnbaYA/hjfVT/NzBKVeOzRrxsRbbtN7NrPKMMcyWUC3D7PEWkDZYyuj1TJfIc7rTFTGCcqsblcge67rSFN9AQaAsUAv4QkT9VdU9mC5fFuNMWHYBQ4E6gBvCTiPymqhczWbbsRrbtN7OrMjKuhK7j1nmKSBDwLtBJVc9kkWxZjTtt0QhYbCui0kBnEYlV1eVZImHW4e4zEqGqV4ArIrIWCAZymzJypy0GAa+qtXCyT0QOAH7AxqwRMduQbfvN7DpNZ1wJXSfNthCRysCXQP9c+NbrTJptoarVVLWqqlYFlgLDc6EiAveeka+AliLiLSI3YXnND8tiObMCd9oiHGuEiIiUw/Jg/W+WSpk9yLb9ZrYcGWnmuRLKcbjZFhOAUsA8e0QQq7nQU7GbbZEncKctVDVMRL4HtgPxwLuq6tLkNyfj5n3xIrBQRP7Gmqoap6q5LrSEiHwKtAZKi8gR4AUgP2T/ftO4AzIYDAaDx8mu03QGg8FgyEMYZWQwGAwGj2OUkcFgMBg8jlFGBoPBYPA4RhkZDAaDweMYZWTIltget0OdflVTyXs5A+pbKCIH7Lq2ikjz/1DGuyJSx/5/fJJ9629URruchHbZYXuhLp5G/noi0jkj6jYYMhNj2m3IlojIZVUtktF5UyljIfCNqi4VkfbAG6oadAPl3bBMaZUrIh8Ae1T15VTyDwQaqepjGS2LwZCRmJGRIUcgIkVE5Gd71PK3iCTz1i0it4jIWqeRQ0t7e3sR+cM+9nMRSUtJrAVutY8dY5e1Q0Qet7cVFpFv7dg4O0Skj739VxFpJCKvAoVsORbZ+y7bf5c4j1TsEVkvEfESkddFZJNYcWaGutEsf2A7uRSRJmLFsvrL/lvb9kYwGehjy9LHlv19u56/XLWjweARPB3DwvzMz9UPiMNybBkKLMPyFlLM3lca6wvyhJH9ZfvvWOBZ+38voKiddy1Q2N4+Dpjgor6F2LGPgHuBDVhORv8GCmOFHfgHqA/0At5xOtbX/vsr1ijEIZNTngQZewIf2P8XwPKgXAgYAjxnb/cBNgPVXMh52en8Pgc62uligLf9fzvgC/v/gcAcp+NfAR6w/y+O5aeusKevt/mZX7Z0B2QwAJGqWi8hISL5gVdE5A4s1zYVgHLACadjNgHv23mXq2qoiLQC6gDrbFdJBbBGFK54XUSeA05jeT9vCyxTy9EoIvIl0BL4HnhDRKZiTe39lo7zWgnMEhEfoCOwVlUj7anBILkemdYXqAkcSHJ8IREJBaoCW4CfnPJ/ICI1sbww50+h/vZANxF5wk4XBCqTO33WGXIQRhkZcgr9sCJ0NlTVGBE5iNWROlDVtbay6gJ8JCKvA+eAn1S1rxt1PKmqSxMSItLOVSZV3SMiDbF8fE0RkR9VdbI7J6GqUSLyK1ZIgz7ApwnVASNU9Yc0iohU1Xoi4gt8AzwKzMLyvbZaVXvaxh6/pnC8AL1Udbc78hoMWYVZMzLkFHyBU7YiagNUSZpBRKrYed4B3sMKv/wncJuIJKwB3SQitdyscy3Qwz6mMNYU228iUh64qqofA2/Y9SQlxh6huWIxloPKlljOPbH/Dks4RkRq2XW6RFUvACOBJ+xjfIGj9u6BTlkvYU1XJvADMELsYaKI1E+pDoMhKzHKyJBTWAQ0EpHNWKOkXS7ytAZCReQvrHWdN1X1NFbn/KmIbMdSTn7uVKiqW7HWkjZirSG9q6p/AYHARnu67FngJReHvw1sTzBgSMKPwB3AKrXCZIMVi2onsFVEdgALSGPmwpZlG1bIhNewRmnrsNaTElgN1EkwYMAaQeW3Zdthpw0Gj2NMuw0Gg8HgcczIyGAwGAwexygjg8FgMHgco4wMBoPB4HGMMjIYDAaDxzHKyGAwGAwexygjg8FgMHgco4wMBoPB4HH+H5fu/55o+ztkAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 计算每一类的ROC\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes): # 遍历三个类别\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_label[:, i], dbn_test_proba[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area（方法二）\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test_label.ravel(), dbn_test_proba.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# Compute macro-average ROC curve and ROC area（方法一）\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "lw=2\n",
    "# plt.figure()\n",
    "# plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "#          label='micro-average ROC curve (area = {0:0.4f})'\n",
    "#                ''.format(roc_auc[\"micro\"]),\n",
    "#          color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.4f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='ROC curve of class {0} (area = {1:0.4f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Some extension of Receiver operating characteristic to multi-class')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#0099ff  size=10 face=\"黑体\">方法3：RNN </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 1, 9)]            0         \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 10)                800       \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 3)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 943\n",
      "Trainable params: 943\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def buildLSTM(timeStep,inputColNum,outStep,learnRate=1e-4):\n",
    "    '''\n",
    "    搭建LSTM网络，激活函数为tanh\n",
    "    timeStep：输入时间步\n",
    "    inputColNum：输入列数\n",
    "    outStep：输出时间步\n",
    "    learnRate：学习率    \n",
    "    '''\n",
    "    #输入层\n",
    "    inputLayer = Input(shape=(timeStep,inputColNum))\n",
    "\n",
    "    #中间层\n",
    "    middle = LSTM(10,activation='tanh')(inputLayer)\n",
    "    middle = Dense(10,activation='tanh')(middle)\n",
    "\n",
    "    #输出层 全连接\n",
    "    outputLayer = Dense(outStep)(middle)\n",
    "    \n",
    "    #建模\n",
    "    model = Model(inputs=inputLayer,outputs=outputLayer)\n",
    "    optimizer = Adam(learning_rate=learnRate)\n",
    "    model.compile(optimizer=optimizer,loss='mse') \n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "#搭建LSTM\n",
    "lstm = buildLSTM(timeStep=1,inputColNum=9,outStep=3,learnRate=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_lstm = new_train.reshape(new_train.shape[0],1,new_train.shape[1])\n",
    "new_test_lstm = new_test1.reshape(new_test1.shape[0],1,new_test1.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 438 samples\n",
      "Epoch 1/500\n",
      "438/438 [==============================] - 1s 2ms/sample - loss: 0.3411\n",
      "Epoch 2/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.3353\n",
      "Epoch 3/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.3296\n",
      "Epoch 4/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.3240\n",
      "Epoch 5/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.3186\n",
      "Epoch 6/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.3133\n",
      "Epoch 7/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.3081\n",
      "Epoch 8/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.3030\n",
      "Epoch 9/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.2980\n",
      "Epoch 10/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.2931\n",
      "Epoch 11/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.2883\n",
      "Epoch 12/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.2835\n",
      "Epoch 13/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.2788\n",
      "Epoch 14/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.2742\n",
      "Epoch 15/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.2696\n",
      "Epoch 16/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.2650\n",
      "Epoch 17/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.2605\n",
      "Epoch 18/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.2561\n",
      "Epoch 19/500\n",
      "438/438 [==============================] - 0s 41us/sample - loss: 0.2517\n",
      "Epoch 20/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.2473\n",
      "Epoch 21/500\n",
      "438/438 [==============================] - 0s 40us/sample - loss: 0.2430\n",
      "Epoch 22/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.2387\n",
      "Epoch 23/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.2344\n",
      "Epoch 24/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.2302\n",
      "Epoch 25/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.2260\n",
      "Epoch 26/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.2218\n",
      "Epoch 27/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.2176\n",
      "Epoch 28/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.2135\n",
      "Epoch 29/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.2094\n",
      "Epoch 30/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.2054\n",
      "Epoch 31/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.2013\n",
      "Epoch 32/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.1973\n",
      "Epoch 33/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.1933\n",
      "Epoch 34/500\n",
      "438/438 [==============================] - 0s 39us/sample - loss: 0.1893\n",
      "Epoch 35/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.1854\n",
      "Epoch 36/500\n",
      "438/438 [==============================] - 0s 39us/sample - loss: 0.1815\n",
      "Epoch 37/500\n",
      "438/438 [==============================] - 0s 39us/sample - loss: 0.1776\n",
      "Epoch 38/500\n",
      "438/438 [==============================] - 0s 39us/sample - loss: 0.1738\n",
      "Epoch 39/500\n",
      "438/438 [==============================] - 0s 39us/sample - loss: 0.1699\n",
      "Epoch 40/500\n",
      "438/438 [==============================] - 0s 39us/sample - loss: 0.1661\n",
      "Epoch 41/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.1624\n",
      "Epoch 42/500\n",
      "438/438 [==============================] - 0s 39us/sample - loss: 0.1587\n",
      "Epoch 43/500\n",
      "438/438 [==============================] - 0s 39us/sample - loss: 0.1550\n",
      "Epoch 44/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.1513\n",
      "Epoch 45/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.1477\n",
      "Epoch 46/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.1441\n",
      "Epoch 47/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.1406\n",
      "Epoch 48/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.1370\n",
      "Epoch 49/500\n",
      "438/438 [==============================] - 0s 39us/sample - loss: 0.1336\n",
      "Epoch 50/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.1302\n",
      "Epoch 51/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.1268\n",
      "Epoch 52/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.1235\n",
      "Epoch 53/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.1202\n",
      "Epoch 54/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.1169\n",
      "Epoch 55/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.1138\n",
      "Epoch 56/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.1106\n",
      "Epoch 57/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.1075\n",
      "Epoch 58/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.1045\n",
      "Epoch 59/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.1015\n",
      "Epoch 60/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0986\n",
      "Epoch 61/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0957\n",
      "Epoch 62/500\n",
      "438/438 [==============================] - 0s 39us/sample - loss: 0.0929\n",
      "Epoch 63/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0901\n",
      "Epoch 64/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0874\n",
      "Epoch 65/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0847\n",
      "Epoch 66/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0821\n",
      "Epoch 67/500\n",
      "438/438 [==============================] - 0s 39us/sample - loss: 0.0795\n",
      "Epoch 68/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0770\n",
      "Epoch 69/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0746\n",
      "Epoch 70/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0722\n",
      "Epoch 71/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0699\n",
      "Epoch 72/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0676\n",
      "Epoch 73/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0654\n",
      "Epoch 74/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0633\n",
      "Epoch 75/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0612\n",
      "Epoch 76/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0592\n",
      "Epoch 77/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0572\n",
      "Epoch 78/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0553\n",
      "Epoch 79/500\n",
      "438/438 [==============================] - 0s 39us/sample - loss: 0.0534\n",
      "Epoch 80/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0516\n",
      "Epoch 81/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0499\n",
      "Epoch 82/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0482\n",
      "Epoch 83/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0466\n",
      "Epoch 84/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0450\n",
      "Epoch 85/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0436\n",
      "Epoch 86/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0421\n",
      "Epoch 87/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0407\n",
      "Epoch 88/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0394\n",
      "Epoch 89/500\n",
      "438/438 [==============================] - 0s 59us/sample - loss: 0.0381\n",
      "Epoch 90/500\n",
      "438/438 [==============================] - 0s 61us/sample - loss: 0.0369\n",
      "Epoch 91/500\n",
      "438/438 [==============================] - 0s 50us/sample - loss: 0.0357\n",
      "Epoch 92/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0346\n",
      "Epoch 93/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0335\n",
      "Epoch 94/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0325\n",
      "Epoch 95/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0315\n",
      "Epoch 96/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0306\n",
      "Epoch 97/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0297\n",
      "Epoch 98/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0289\n",
      "Epoch 99/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0281\n",
      "Epoch 100/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0273\n",
      "Epoch 101/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0266\n",
      "Epoch 102/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0259\n",
      "Epoch 103/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0253\n",
      "Epoch 104/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0247\n",
      "Epoch 105/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0241\n",
      "Epoch 106/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0236\n",
      "Epoch 107/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0230\n",
      "Epoch 108/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0226\n",
      "Epoch 109/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0221\n",
      "Epoch 110/500\n",
      "438/438 [==============================] - 0s 39us/sample - loss: 0.0217\n",
      "Epoch 111/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0213\n",
      "Epoch 112/500\n",
      "438/438 [==============================] - 0s 39us/sample - loss: 0.0209\n",
      "Epoch 113/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0205\n",
      "Epoch 114/500\n",
      "438/438 [==============================] - 0s 39us/sample - loss: 0.0202\n",
      "Epoch 115/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0199\n",
      "Epoch 116/500\n",
      "438/438 [==============================] - 0s 41us/sample - loss: 0.0196\n",
      "Epoch 117/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0193\n",
      "Epoch 118/500\n",
      "438/438 [==============================] - 0s 39us/sample - loss: 0.0190\n",
      "Epoch 119/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0187\n",
      "Epoch 120/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0185\n",
      "Epoch 121/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0183\n",
      "Epoch 122/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0181\n",
      "Epoch 123/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0179\n",
      "Epoch 124/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0177\n",
      "Epoch 125/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0175\n",
      "Epoch 126/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0173\n",
      "Epoch 127/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0171\n",
      "Epoch 128/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0170\n",
      "Epoch 129/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0168\n",
      "Epoch 130/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0167\n",
      "Epoch 131/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0165\n",
      "Epoch 132/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0164\n",
      "Epoch 133/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0163\n",
      "Epoch 134/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0162\n",
      "Epoch 135/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0160\n",
      "Epoch 136/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0159\n",
      "Epoch 137/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0158\n",
      "Epoch 138/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0157\n",
      "Epoch 139/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0156\n",
      "Epoch 140/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0155\n",
      "Epoch 141/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0154\n",
      "Epoch 142/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0153\n",
      "Epoch 143/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0153\n",
      "Epoch 144/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0152\n",
      "Epoch 145/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0151\n",
      "Epoch 146/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0150\n",
      "Epoch 147/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0149\n",
      "Epoch 148/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0149\n",
      "Epoch 149/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0148\n",
      "Epoch 150/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0147\n",
      "Epoch 151/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0147\n",
      "Epoch 152/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0146\n",
      "Epoch 153/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0145\n",
      "Epoch 154/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0145\n",
      "Epoch 155/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0144\n",
      "Epoch 156/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0144\n",
      "Epoch 157/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0143\n",
      "Epoch 158/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0143\n",
      "Epoch 159/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0142\n",
      "Epoch 160/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0142\n",
      "Epoch 161/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0141\n",
      "Epoch 162/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0141\n",
      "Epoch 163/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0140\n",
      "Epoch 164/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0140\n",
      "Epoch 165/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0139\n",
      "Epoch 166/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0139\n",
      "Epoch 167/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0138\n",
      "Epoch 168/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0138\n",
      "Epoch 169/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0137\n",
      "Epoch 170/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0137\n",
      "Epoch 171/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0137\n",
      "Epoch 172/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0136\n",
      "Epoch 173/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0136\n",
      "Epoch 174/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0135\n",
      "Epoch 175/500\n",
      "438/438 [==============================] - 0s 41us/sample - loss: 0.0135\n",
      "Epoch 176/500\n",
      "438/438 [==============================] - 0s 39us/sample - loss: 0.0135\n",
      "Epoch 177/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0134\n",
      "Epoch 178/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0134\n",
      "Epoch 179/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0134\n",
      "Epoch 180/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0133\n",
      "Epoch 181/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0133\n",
      "Epoch 182/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0133\n",
      "Epoch 183/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0132\n",
      "Epoch 184/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0132\n",
      "Epoch 185/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0132\n",
      "Epoch 186/500\n",
      "438/438 [==============================] - 0s 41us/sample - loss: 0.0131\n",
      "Epoch 187/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0131\n",
      "Epoch 188/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0131\n",
      "Epoch 189/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0130\n",
      "Epoch 190/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0130\n",
      "Epoch 191/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0130\n",
      "Epoch 192/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0130\n",
      "Epoch 193/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0129\n",
      "Epoch 194/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0129\n",
      "Epoch 195/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0129\n",
      "Epoch 196/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0129\n",
      "Epoch 197/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0128\n",
      "Epoch 198/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0128\n",
      "Epoch 199/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0128\n",
      "Epoch 200/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0128\n",
      "Epoch 201/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0127\n",
      "Epoch 202/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0127\n",
      "Epoch 203/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0127\n",
      "Epoch 204/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0127\n",
      "Epoch 205/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0126\n",
      "Epoch 206/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0126\n",
      "Epoch 207/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0126\n",
      "Epoch 208/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0126\n",
      "Epoch 209/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0126\n",
      "Epoch 210/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0125\n",
      "Epoch 211/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0125\n",
      "Epoch 212/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0125\n",
      "Epoch 213/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0125\n",
      "Epoch 214/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0125\n",
      "Epoch 215/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0124\n",
      "Epoch 216/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0124\n",
      "Epoch 217/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0124\n",
      "Epoch 218/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0124\n",
      "Epoch 219/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0124\n",
      "Epoch 220/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0124\n",
      "Epoch 221/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0123\n",
      "Epoch 222/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0123\n",
      "Epoch 223/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0123\n",
      "Epoch 224/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0123\n",
      "Epoch 225/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0123\n",
      "Epoch 226/500\n",
      "438/438 [==============================] - 0s 39us/sample - loss: 0.0123\n",
      "Epoch 227/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0122\n",
      "Epoch 228/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0122\n",
      "Epoch 229/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0122\n",
      "Epoch 230/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0122\n",
      "Epoch 231/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0122\n",
      "Epoch 232/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0122\n",
      "Epoch 233/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0122\n",
      "Epoch 234/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0121\n",
      "Epoch 235/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0121\n",
      "Epoch 236/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0121\n",
      "Epoch 237/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0121\n",
      "Epoch 238/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0121\n",
      "Epoch 239/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0121\n",
      "Epoch 240/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0121\n",
      "Epoch 241/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0120\n",
      "Epoch 242/500\n",
      "438/438 [==============================] - 0s 39us/sample - loss: 0.0120\n",
      "Epoch 243/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0120\n",
      "Epoch 244/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0120\n",
      "Epoch 245/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0120\n",
      "Epoch 246/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0120\n",
      "Epoch 247/500\n",
      "438/438 [==============================] - 0s 100us/sample - loss: 0.0120\n",
      "Epoch 248/500\n",
      "438/438 [==============================] - 0s 46us/sample - loss: 0.0120\n",
      "Epoch 249/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0119\n",
      "Epoch 250/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0119\n",
      "Epoch 251/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0119\n",
      "Epoch 252/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0119\n",
      "Epoch 253/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0119\n",
      "Epoch 254/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0119\n",
      "Epoch 255/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0119\n",
      "Epoch 256/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0119\n",
      "Epoch 257/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0119\n",
      "Epoch 258/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0118\n",
      "Epoch 259/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0118\n",
      "Epoch 260/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0118\n",
      "Epoch 261/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0118\n",
      "Epoch 262/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0118\n",
      "Epoch 263/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0118\n",
      "Epoch 264/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0118\n",
      "Epoch 265/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0118\n",
      "Epoch 266/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0118\n",
      "Epoch 267/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0118\n",
      "Epoch 268/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0118\n",
      "Epoch 269/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0117\n",
      "Epoch 270/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0117\n",
      "Epoch 271/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0117\n",
      "Epoch 272/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0117\n",
      "Epoch 273/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0117\n",
      "Epoch 274/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0117\n",
      "Epoch 275/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0117\n",
      "Epoch 276/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0117\n",
      "Epoch 277/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0117\n",
      "Epoch 278/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0117\n",
      "Epoch 279/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0117\n",
      "Epoch 280/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0117\n",
      "Epoch 281/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0116\n",
      "Epoch 282/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0116\n",
      "Epoch 283/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0116\n",
      "Epoch 284/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0116\n",
      "Epoch 285/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0116\n",
      "Epoch 286/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0116\n",
      "Epoch 287/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0116\n",
      "Epoch 288/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0116\n",
      "Epoch 289/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0116\n",
      "Epoch 290/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0116\n",
      "Epoch 291/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0116\n",
      "Epoch 292/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0116\n",
      "Epoch 293/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0116\n",
      "Epoch 294/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0116\n",
      "Epoch 295/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0116\n",
      "Epoch 296/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0115\n",
      "Epoch 297/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0115\n",
      "Epoch 298/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0115\n",
      "Epoch 299/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0115\n",
      "Epoch 300/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0115\n",
      "Epoch 301/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0115\n",
      "Epoch 302/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0115\n",
      "Epoch 303/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0115\n",
      "Epoch 304/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0115\n",
      "Epoch 305/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0115\n",
      "Epoch 306/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0115\n",
      "Epoch 307/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0115\n",
      "Epoch 308/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0115\n",
      "Epoch 309/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0115\n",
      "Epoch 310/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0115\n",
      "Epoch 311/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0115\n",
      "Epoch 312/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0114\n",
      "Epoch 313/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0114\n",
      "Epoch 314/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0114\n",
      "Epoch 315/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0114\n",
      "Epoch 316/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0114\n",
      "Epoch 317/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0114\n",
      "Epoch 318/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0114\n",
      "Epoch 319/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0114\n",
      "Epoch 320/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0114\n",
      "Epoch 321/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0114\n",
      "Epoch 322/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0114\n",
      "Epoch 323/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0114\n",
      "Epoch 324/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0114\n",
      "Epoch 325/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0114\n",
      "Epoch 326/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0114\n",
      "Epoch 327/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0114\n",
      "Epoch 328/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0114\n",
      "Epoch 329/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0114\n",
      "Epoch 330/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0114\n",
      "Epoch 331/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0114\n",
      "Epoch 332/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0114\n",
      "Epoch 333/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0113\n",
      "Epoch 334/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0113\n",
      "Epoch 335/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0113\n",
      "Epoch 336/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0113\n",
      "Epoch 337/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0113\n",
      "Epoch 338/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0113\n",
      "Epoch 339/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0113\n",
      "Epoch 340/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0113\n",
      "Epoch 341/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0113\n",
      "Epoch 342/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0113\n",
      "Epoch 343/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0113\n",
      "Epoch 344/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0113\n",
      "Epoch 345/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0113\n",
      "Epoch 346/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0113\n",
      "Epoch 347/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0113\n",
      "Epoch 348/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0113\n",
      "Epoch 349/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0113\n",
      "Epoch 350/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0113\n",
      "Epoch 351/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0113\n",
      "Epoch 352/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0113\n",
      "Epoch 353/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0113\n",
      "Epoch 354/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0113\n",
      "Epoch 355/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0113\n",
      "Epoch 356/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0113\n",
      "Epoch 357/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0112\n",
      "Epoch 358/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0112\n",
      "Epoch 359/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0112\n",
      "Epoch 360/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0112\n",
      "Epoch 361/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0112\n",
      "Epoch 362/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0112\n",
      "Epoch 363/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0112\n",
      "Epoch 364/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0112\n",
      "Epoch 365/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0112\n",
      "Epoch 366/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0112\n",
      "Epoch 367/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0112\n",
      "Epoch 368/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0112\n",
      "Epoch 369/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0112\n",
      "Epoch 370/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0112\n",
      "Epoch 371/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0112\n",
      "Epoch 372/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0112\n",
      "Epoch 373/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0112\n",
      "Epoch 374/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0112\n",
      "Epoch 375/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0112\n",
      "Epoch 376/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0112\n",
      "Epoch 377/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0112\n",
      "Epoch 378/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0112\n",
      "Epoch 379/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0112\n",
      "Epoch 380/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0112\n",
      "Epoch 381/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0112\n",
      "Epoch 382/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0112\n",
      "Epoch 383/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0112\n",
      "Epoch 384/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0111\n",
      "Epoch 385/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0111\n",
      "Epoch 386/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0111\n",
      "Epoch 387/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0111\n",
      "Epoch 388/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0111\n",
      "Epoch 389/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0111\n",
      "Epoch 390/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0111\n",
      "Epoch 391/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0111\n",
      "Epoch 392/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0111\n",
      "Epoch 393/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0111\n",
      "Epoch 394/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0111\n",
      "Epoch 395/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0111\n",
      "Epoch 396/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0111\n",
      "Epoch 397/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0111\n",
      "Epoch 398/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0111\n",
      "Epoch 399/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0111\n",
      "Epoch 400/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0111\n",
      "Epoch 401/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0111\n",
      "Epoch 402/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0111\n",
      "Epoch 403/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0111\n",
      "Epoch 404/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0111\n",
      "Epoch 405/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0111\n",
      "Epoch 406/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0111\n",
      "Epoch 407/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0111\n",
      "Epoch 408/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0111\n",
      "Epoch 409/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0111\n",
      "Epoch 410/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0111\n",
      "Epoch 411/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0111\n",
      "Epoch 412/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0111\n",
      "Epoch 413/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0111\n",
      "Epoch 414/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0111\n",
      "Epoch 415/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0111\n",
      "Epoch 416/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0111\n",
      "Epoch 417/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0110\n",
      "Epoch 418/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0110\n",
      "Epoch 419/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0110\n",
      "Epoch 420/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0110\n",
      "Epoch 421/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0110\n",
      "Epoch 422/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0110\n",
      "Epoch 423/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0110\n",
      "Epoch 424/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0110\n",
      "Epoch 425/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0110\n",
      "Epoch 426/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0110\n",
      "Epoch 427/500\n",
      "438/438 [==============================] - 0s 91us/sample - loss: 0.0110\n",
      "Epoch 428/500\n",
      "438/438 [==============================] - 0s 41us/sample - loss: 0.0110\n",
      "Epoch 429/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0110\n",
      "Epoch 430/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0110\n",
      "Epoch 431/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0110\n",
      "Epoch 432/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0110\n",
      "Epoch 433/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0110\n",
      "Epoch 434/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0110\n",
      "Epoch 435/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0110\n",
      "Epoch 436/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0110\n",
      "Epoch 437/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0110\n",
      "Epoch 438/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0110\n",
      "Epoch 439/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0110\n",
      "Epoch 440/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0110\n",
      "Epoch 441/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0110\n",
      "Epoch 442/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0110\n",
      "Epoch 443/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0110\n",
      "Epoch 444/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0110\n",
      "Epoch 445/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0110\n",
      "Epoch 446/500\n",
      "438/438 [==============================] - 0s 41us/sample - loss: 0.0110\n",
      "Epoch 447/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0110\n",
      "Epoch 448/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0110\n",
      "Epoch 449/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0110\n",
      "Epoch 450/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0110\n",
      "Epoch 451/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0110\n",
      "Epoch 452/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0110\n",
      "Epoch 453/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0109\n",
      "Epoch 454/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0109\n",
      "Epoch 455/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0109\n",
      "Epoch 456/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0109\n",
      "Epoch 457/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0109\n",
      "Epoch 458/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0109\n",
      "Epoch 459/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0109\n",
      "Epoch 460/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0109\n",
      "Epoch 461/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0109\n",
      "Epoch 462/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0109\n",
      "Epoch 463/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0109\n",
      "Epoch 464/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0109\n",
      "Epoch 465/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0109\n",
      "Epoch 466/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0109\n",
      "Epoch 467/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0109\n",
      "Epoch 468/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0109\n",
      "Epoch 469/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0109\n",
      "Epoch 470/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0109\n",
      "Epoch 471/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0109\n",
      "Epoch 472/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0109\n",
      "Epoch 473/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0109\n",
      "Epoch 474/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0109\n",
      "Epoch 475/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0109\n",
      "Epoch 476/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0109\n",
      "Epoch 477/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0109\n",
      "Epoch 478/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0109\n",
      "Epoch 479/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0109\n",
      "Epoch 480/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0109\n",
      "Epoch 481/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0109\n",
      "Epoch 482/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0109\n",
      "Epoch 483/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0109\n",
      "Epoch 484/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0109\n",
      "Epoch 485/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0109\n",
      "Epoch 486/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0109\n",
      "Epoch 487/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0109\n",
      "Epoch 488/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0109\n",
      "Epoch 489/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0109\n",
      "Epoch 490/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0109\n",
      "Epoch 491/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0109\n",
      "Epoch 492/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0108\n",
      "Epoch 493/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0109\n",
      "Epoch 494/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0108\n",
      "Epoch 495/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0108\n",
      "Epoch 496/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0109\n",
      "Epoch 497/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0108\n",
      "Epoch 498/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0108\n",
      "Epoch 499/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0108\n",
      "Epoch 500/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0108\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18a8e8416d0>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 500 \n",
    "batchSize = 35\n",
    "lstm.fit(new_train_lstm,y_train_labels,epochs=epochs,verbose=1,batch_size=batchSize) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    }
   ],
   "source": [
    "LSTM_train_proba = lstm.predict(new_train_lstm)\n",
    "LSTM_train_pred =Predict(LSTM_train_proba)\n",
    "LSTM_test_proba = lstm.predict(new_test_lstm)\n",
    "LSTM_test_pred =Predict(LSTM_test_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN training with laryer  train score: 0.9840182648401826\n",
      "RNN training with laryer  test score: 0.898936170212766\n"
     ]
    }
   ],
   "source": [
    "LSTM_train_acc =  accuracy_score(y_train, LSTM_train_pred)\n",
    "LSTM_test_acc = accuracy_score(y_test, LSTM_test_pred)\n",
    "print(\"RNN training with laryer  train score: {}\".format(LSTM_train_acc))\n",
    "print(\"RNN training with laryer  test score: {}\".format(LSTM_test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC: 0.898936170212766\n",
      "macro-PRE: 0.8774838330393885\n",
      "macro-SEN: 0.8741252214462065\n",
      "macroF1-score: 0.8749616407415562\n"
     ]
    }
   ],
   "source": [
    "print('ACC:', metrics.accuracy_score(y_test,LSTM_test_pred)) \n",
    " \n",
    "print('macro-PRE:',metrics.precision_score(y_test,LSTM_test_pred,average='macro')) \n",
    " \n",
    "print('macro-SEN:',metrics.recall_score(y_test, LSTM_test_pred,average='macro'))\n",
    " \n",
    "print('macroF1-score:',metrics.f1_score(y_test, LSTM_test_pred,labels=[0,1,2],average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9916\\3893804580.py:19: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "  mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaMAAAEWCAYAAADLkvgyAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAABiiklEQVR4nO2dZ3gVRReA30MCofcivRlISKMXFQkCgoB0QUSkWQAFuyAqTRGxACIg+CmKCqIoIlYUVFBQmgREmvQWeksCIe18P3ZzuUlukhshuSnzPs8+987O7OzZ2d05O+0cUVUMBoPBYPAk+TwtgMFgMBgMRhkZDAaDweMYZWQwGAwGj2OUkcFgMBg8jlFGBoPBYPA4RhkZDAaDweMYZZRFiMj3IjLAA+d9SUROi8jxrD63K0SkpYjs8rQc2QERiRSRWll8ThWRG7PynJnFf32ncsMzKCKhInIkjfhq9vPl9R/yPiAiba9NwoyTYWUkIreIyFoRuSAiZ0VkjYg0yQzhsoKsKnhVvUNV52f2eZwRkarAk0A9Vb3BRXyoiCTYD22EiOwSkUGZKZOq/qaqdTPzHNkREflVRO533qeqRVV1n6dk8iTX471z951KroD/6zMoIuNF5OOMHpcVJC9PVT1kP1/xnpQrI2RIGYlIceAb4C2gNFAZmABcuf6iGa4D1YEzqnoyjTTHVLUoUBx4HPifiOQ4ZSEi3nnx3J7Cw+UtImJ6dXIbqur2BjQGzqcRnw94HjgInAQ+BErYcTUABQYBh4FzwFCgCbAVOA/MTJbfYGCHnXY5UD2NczcH1tr5bAFC7f03AaeBqnY4xE7jB3wEJACXgUjgmbTysuN+BV4E1gARwI9AWTuuIPAxcMY+dgNQwem4+zNQTgOAQ7bsz6Vx3SXs40/Z+T1v59/Wvq4E+9o+cHFsKHAk2b6TwF1Oco4G9trX9BlQ2intLU7ldBgYaO/3AV635T8BzAEKJT+nnffnyc7/JjDD6dreA8KBo8BLgJcdN9C+B9OAs8BLLq7PB5gOHLO36YCPsxzAGLuMDwD9kh2b5jUAo4DjWM9RKawPtVNYz+s3QBU7/SQgHoi278VMe78CN9r/PwBmAd9iPVfrgNpO8twO7AIuALOBVdjPk4vr9rKva6+d1yauPv+K9d79a8s5CxA7rjbws32vTwMLgJJO+R6wr3kr1geoN1efjwhgO9A9mSwPYL3DifEN+e/v3ST7nl8GbiTpO3WjXSYXbNk/tfevtq85yj5XH5I990BVYIl9786QrB6y03QAYoBYO58t9v5KwDKsZ3AP8EAa7+oH9r373s5jDXAD1nN5DtgJNHBK73g+nI5/ycV7lKI8uVqPeKchT4p743Sf29r/mwJ/2PckHJgJFLDjBOv9O2mX+1Yg0I7raOcZgfXuPpWufnFHCTkJX9y+WfOBO4BSLpTHHqAWUNS+wR8lq2TnYFXat2O9nEuB8litrJNAKzt9Nzsvf6yH/nlgbSpyVbbl6ohVgbazw+WcKoOfgUJ2gT2S7AVrm4G8fsV6+erY+f0KvGLHPQR8DRTGqhAaAcVdKCN3yul/dv4hWC++fyrX/iHwFVDMPnY3MCQ1ZZOaMrKvtQvWQ93A3vcY8CdQBatyngt8YsdVw3rQ+gL5gTJAfTtuOtYLWtqW62tgsotzVgcuOZWRF9YD39wOL7XPWQTrGVkPPOSkjOKAEVjPRyEX1zfRlr88UA6ronvRSY44YKp9ba2wKqy6bl5DHDDFPraQff097XtfDFgMLE1Wmd6fTL7kyugs1svvjaUIFtlxZYGLQA877lGsSjE1ZfQ08DdQF6vCCAHKOJ3zG6CkfQ9PAR2cKvR29jWVw6rIpyd7V8KwKu9ExXwXVoWcD6uijwIqOsUdxfrgFDv/6tfw3h0CAuwyyE/Sd+oT4Dn72ILALWlU6qFcfQa9sBTfNKznLMmxycp1PPBxsn2rsBRMQaC+XZ5t0lBGp7HqhYJYddJ+4D5bjpeAXzKqjFIpzxqkoYzcvTe2rM3tMq+Bpbwes+PaY33olLTz8He69+FAS/t/KWxFl6Z+SS+Bi4vwtwvlCNYLuYyrX/8rgeFOaetivTSJF6JAZaf4M0Afp/AXThf6PXal6lRZXsJF6wjra+2jZPuWAwPs//ntQvsb+AH7SzCVm5heXr8CzzvFDQd+sP8Pxqrwgl3I+CtXXxx3yqmKU/x64G4XeXphKap6TvseAn519cC6OD4US/mct/OJTyx/O34HTi8WUNFJzmeBL13kKVgVkvNXfQtgfyov0e/Affb/dsBe+38FW6ZCTmn7Yr+sWMroUDrP6l6go1O4PXDASY44oIhT/GfAC25eQwxQMI1z1wfOubr/TvuSK6N3neI6Ajvt//cBfyQr48PJ83OK3wV0TSVOSVpRfwaMTiVtN2BzsndlcDplHpZ4bqz35tFU0h0g4+/dxDTeqQ+Bd3B6b1yVc/Jn0L6vp0ijBeF03HiclBGWUo4Hijntm4yLXgine/w/p/AIYIdTOAinnicXcn/A9VNGbt+bZHGPYb/3wG1YH7/NgXzJ0h3CqouKp1euiVuG+11VdYeqDlTVKkAg1lfRdDu6ElZXUSIHsSquCk77Tjj9v+wiXNT+Xx14U0TOi8h5rK9GwfqCSk514K7EtHb6W7AqT1Q1FutGBgJvqF1aqZBmXjbOM9MuOcn8EdZNXiQix0TkVRHJ7+Ic7pRTaudwpixQwEVersooNY6pakmsVu8MrAcskerAl07lsAPr5auA9SLudZFfOazWwSan436w97tiIZaSAbjHDieeOz8Q7pTPXKxWTiKH07k2V+VcySl8TlWjXMS7cw2nVDU6MSAihUVkrogcFJGLWK2KkhmczZTaPa+E07Xaz2+qM6lI/d6keR4RKS8ii0TkqH0NH2M9Y84kKXMRuU9EwpzKKdDpmPTkcMad9y6t+/0MVv2wXkT+EZHBbp63KnBQVePcTO9MJeCsqkY47Uvv/XO3/rtu2LMHI+3tH3u3W/dGROqIyDcictx+Jl7Gvr+q+jNWt90s4ISIvGPPKwCrl6AjcFBEVolIi/TOdU2DgKq6k6uVPFj98tWdklTD+vo8QcY5jNUlU9JpK6Sqa1NJ+1GytEVU9RUAEakMjAPeB94QER/ny8hIXmmhqrGqOkFV62GNVXXG+qpNzvUqp9NYLZXkeR3NYD6o6hWsr9MgEelm7z4M3JGsLAqq6lE7rnYqMl0GApyOKaHWJAlXLAZCRaQK0J2ryugwVsuorFM+xVU1wFnsdC7LVTkfcwqXEpEiLuLduYbk534Sq4XbTFWLA7fa+8VNWdMiHKur1MpQRJzDLkjt3qTHZCw5g+1ruJer8ifiuA4RqY7VnfwIVjdgSWCb0zFpyfFf3rtUy1BVj6vqA6paCeuLfLabU9gPA9XcnJCR/PzHgNIiUsxp3396/1LhEtZHUSIpZsSmIdvVCGv2YFF7S3x/3H1G3sYay/K1n4kxOD0TqjpDVRthdZ/WweoiRlU3qGpXrI/HpVgt8DTJ6Gw6PxF50q44EqcO98Xqlwer3/ZxEakpIkWxtOin//GrYw7wrIgE2OcqISJ3pZL2Y+BOEWkvIl4iUtCetlzFfnE/wBoIH4L1Yr/odOwJrLGbdPNKT2ARaS0iQfbX8EUsReFqauV1KSe1pm1+BkwSkWJ25fCEfQ0ZRlVjgDeAsfauOXbe1QFEpJyIdLXjFgBtRaS3iHiLSBkRqa+qCVgV1DQRKW8fV1lE2qdyzlNY3S3vY3WD7bD3h2NNDnlDRIqLSD4RqS0irTJwSZ8Az9tyl7WvK3nZTBCRAiLSEuvjYXFGr8GmGJYCOy8ipbE+fpxJ/pxlhG+xPxLsSvNh0q6Y3gVeFBFfa+KZBItIGTfOUwxrAPy8/QH3dDrpi2BVgqcAxFoWEOgU/y7wlIg0suW4MfFZ4jq+d/a573JKe86WK/HdS6vs12PVCa+ISBH7vDenkvYEUEPsmXyqehirW36yfVwwVh2zwB2Z3SAMuMcujw5Y45qpkdHnK61740wxrLosUkT8gGGJESLSRESaidX7E4U1ByDefp/6iUgJu1fqIq7rwSRktGUUATQD1olIFJYS2ob1VQgwD6urajXWwFw0Vr9ohlHVL7EGiBfZzcNtWJMmXKU9DHTF0tqnsLT+01jXNxKrW+kFu3tjEDDIrnzA+hp8XqyugafSySs9bgA+xyr8HViDm64Uw3UrJ/u4KGAf1vjLQjv//8o8rC/FO7Fmti0DfhSRCKz73QysdQxYzfAnsbpQw7AGysFqYe0B/rTv3QqsVkNqLMSa/bcw2f77sLoht2NVMJ+TtNsmPV4CNmJNWvkb+Mvel8hxO99jWBXIULu1/1+uYTrWRIbTWOX0Q7L4N4FeInJORGZk4BpQ1dNYA86vYo2z1rOvK7UlFVOxPlJ+xHoW37NlS48JWLPdLmApwCXpyLUd6+PlD6zKMAhrhlhi/GKsyUMLseqOpVgTQuD6vndgDcSvE5FIrGf2UVXdb8eNB+bb5+qd7BrigTuxBvAPYXV/9knlHIvt3zMi8pf9vy/W+Mwx4EtgnKr+5KbM6fGoLdt5oB9W+aVGkvJML+N07o0zT2F1n0dgfaB96hRX3N53Dqt78gzWDFSA/sAB+90ZitXKTpPEKZ0GQ55CREKxBqPd+vLOTthf5kewpqL/4ml5DIbrgVk4ZjDkAOzuq5JijXcm9tv/mc5hBkOOwSgjgyFn0AJr9tNprK6bbqp62bMiGQzXD9NNZzAYDAaPY1pGBoPBYPA4Oc7AY9myZbVGjRqeFsNgMBhyFJs2bTqtqqktPvc4OU4Z1ahRg40bN3paDIPBYMhRiMjB9FN5DtNNZzAYDAaPY5SRwWAwGDyOUUYGg8Fg8DhGGRkMBoPB4xhlZDAYDAaPY5SRwWAwGDxOpk3tFpF5WCb5T6pqoIt4wbJk3BHLb8dAVf0reTqDIS8QEXGFiIgYoqJiiIqKxde3NEWKFEiR7tKlWFatOuAIFyqUn9DQGi7z3LXrNPv2nXOE69QpQ+3argwzw08/7SUuLsERbtu2Fvnzp/QLGB4eQVjYVd98N9xQlAYNXBtSX7/+KGfOXHKEmzSpTNmyhVOkM9eUNdeU3cnMdUYfYHkB/DCV+DsAX3trhuXEqVkmymMwXBf+/fcMUVGxDsXRrl0trG+rpKxZc4h3391MVFQMkZEx3HZbTZ566iaXed5224ds3HjV79+ffw6hWbOUBsWPH4+kY8ernjZq1izJvn2Puszz/ffDmDLF4dGByZPbMHr0LS7T9u79OefPOxzXcvbsM5QqldLrxO+/H6J3788d4V696rF4sWs3Y8899zMrVuxzhH/88V7atUvpz81cU+ZfU48edVzml53IVNt0IlID+CaVltFc4FdV/cQO7wJCbadqqdK4cWPNyKLXGd9E8Peh2AzJbTAYDLmFbb/MZdfajzh3bMcmVW3saXlSw5NjRpVJ6tP+CKn4jheRB0Vko4hsPHXqVIZOYhSRwWDIy5SuHMD58F2eFiNdPGkOKGW/Rip+3FX1HeAdsFpG/+Vk/xvuug/W4B6JN0sBVeXSpVgiImK4ePEKJUsWpHz5Ii6Pe+aZnzh5MoqLF60xkSVLelOsmE+KdJ999g99+lztVujZ05/PP++dIh1Ahw4fs3z5Xkf4u+/u4Y47fFOkO3YsgsqVpzrCFSoU4fhx104wJ05cxbhxvzrCzz3Xkpdeus1l2mrVpnH48EVHeP/+R6lRo2SKdD/+uJf27a86+m3TpiYrVtznMs+BA5fyww97KFKkAEWK5OeNN26n3dYbrcgnrz7y4eERDB68zBG+4YaivP9+1+TZATBv3mYWL97uCA8aVJ/evQNcpr3rrsVERsY4wp991svlffrtt4O8/PLvjvAtt1TluedudZnnmDEr2bz56rjFpEm30bBhyrEYc03X95oOHz7M9Onz2b69pmPf448vZOrUu13mmV3wpDI6AlR1ClfBct2bJ+kEfOdpIdxk7NhfeOml3xzh8eNbMW5cqMu0H3+8lfDwSEf4/PlolxVC8eJJ9128mJpHbffTFimSP0k4Kir1VnLKtDGppAR//3KUKlWIIkXyU6RIAby8XH1XQWBged59906KFi1AkSIFqFgx9UHkDz7olnLn1pS7KlYsxvff90s1H2cGD27A4MEN3Eqb2hhJclq2rM7331d3K+3LL7dxK525putzTXFxccyYMYOxY8cSFRXF6tWradmypSPeKKPUWQY8IiKLsCYuXEhvvCg3kxMUUUf7N7kySU9xOCujiAjXlXxyBZNaOuv8SWeZpZa2SJECtGhRxdHacKUEE+nWzY+6dcs6FEzlysVSTbt8+b2pxjlTqVIxhgxp6FZag+FaWLduHQ899BBbtmwBoGfPntSqVcvDUmWMzJza/QkQCpQVkSPAOCA/gKrOwap/OwJ7sKZ2D7pe587RkxZkguPv338PIzCwfIoka9ce5uab5znCLVpUYe3aIS6zGzhwKfPnb3GE583rwqBBKb/CYmPjKVDgJUfY2zsfsbEvuMwzY4rDPcVVunQhfH1LU7y4D8WK+RAQkLql+5Ejm3H33YEUK+ZD8eI+VKlS3GU6b+98qZZLcmrXLp3qdFqDIbty7tw5xowZw9y5c1FVatSowcyZM+nUqZOnRcswmaaMVLVvOvEKPJwZ506uiIKq5U8lZfYmJibe5f4CBZKuK7hyxXU6V2lTy9PbO+lclri4BBISlHz5UnZBFStWgIIFvW3FUYDSpVNOLU1k9OibuXDhiiOtn19Zl+n8/Mqye/eIVPNxJiTkhtQjl3SC/TmhnWkwXDsTJkxgzpw5eHt789RTT/HCCy9QuHDKdU85gRznzygj5PRJC6kpjqJFC1CnThkKFPCiQAGvVCt4gNDQGuTPn48CBbzw8fFOtSIXET77rBf583s58k2Ne+4Jol+/YLeuoWfPem6lu27kNkVUs2P6aQx5iri4OLy9rar7+eefZ//+/UyaNInAwBQraHIUmbrOKDNwZ53RA7PPAtlHGZ0+fYmZM9ezd+85vn+kCWdcLGZMJPx4JD4+ljIoVCi/y5aJIQ3esMvryZz1XBsM6REdHc2UKVNYunQp69ato0CBlBY60kJEsvU6o1zdMsps3n57A3/8cYRjxyI4eTKKWbM60rJlylk50dFxTJiwygp81D3V/DqSM8x2GAyGrGXlypUMGzaMf//9F4Dly5dz5513eliq64tRRsk4dSqK1asPsm/fOfbtO0eVKsVTXXPwww97Wbbs6mIy57UnzpQrl7IP9/kXfubFF12vYzEYDAaAEydO8OSTT7JgwQIA/P39efvtt2nVqpWHJbv+GGWUjC1bTtCr12JHuGnTyqkqo6pVi8M3faGTZfepn72lwMcbdFyK3arq0qaZwWAwfPzxx4wYMYLz589TsGBBxo4dy5NPPpnh7rmcglFGyahVq1SS8OHDF1JNW7VqcYciygi3XY41rSKDwZAmCQkJnD9/ng4dOjBr1qwct24oo+Q5ZRQZGcPevWfZs+csAQHlU8xEq1q1OF5eQny8NQB+/HgkMTHxLmeX3XGHL6Pt/5v+Cqd69RKUKePGtMpC+c0UZIPBkITIyEj++OMP2rVrB0D//v2pVKkSbdq0yRM9KHlKGY0Zs5LJk6/aoHr55dt49tmWSdLkz+9Fv37BFC2an1q1SrGwXzA+qU1zDq7g+OvKPlWaGEWUeZjp0IYcxtKlSxkxYgSnTp1i27Zt3HjjjYgIbdu29bRoWUaeUkbJZ6rt2XPWZbr587s5/rs2q5mUa6r6zBRkgyHPcvDgQUaOHMmyZZZR1caNG3PlSurmtXIzecrt+I03Jl13tGfPuVRSpkTT2L69bhIaDIa8QGxsLK+99hr16tVj2bJlFCtWjLfeeos///yTgADXFrtzO3mqZeTrWxovL6FmzVLceGNp9rzZwaUfC4PBYMhMRo4cyZw5cwDo3bs306ZNo1KlSh6WyrPkKWVUu3ZpLl9+zuEz3l1FZEYgDAbD9eSxxx5j1apVTJ06lQ4dOnhanGxBjlVG/8Uyd758Qr58KScjmFEbg8GQWagqH3/8Md999x0LFy5ERKhbty7btm0jX748NVKSJjlWGaWniA5Vy+9+F5yZZm0wGDKBXbt2MWzYMH755RfAmq7dsaPV12IUUVJyrDJKJLkx1IyOAXUEzykiMwXZYMiVXL58mcmTJzNlyhRiYmIoU6YMb7zxBnfccYenRcu25HhllCpOTuoKFfLm/PnRabpFAMw0a4PBcM2sWLGCoUOHsnfvXgCGDBnClClTKFOmjIcly97kXmXkRPPmVdJXRAaDwXAdWLt2LXv37iUgIIA5c+Zwyy23eFqkHEGOU0YHT8U5/BWlxZo1g1m9+iC//XaI1q1rZL5gBoMhTxIfH8+ePXuoW7cuAKNGjaJs2bLcf//9udaoaWaQ45SRM2m5E7/ppqrcdFNVRo9ONYnBYDBcE5s3b2bo0KHs27ePXbt2Ubp0aXx8fBg+fLinRctx5EhllCEPrmamnMFguM5EREQwduxYZsyYQUJCApUrV2bv3r2ULp09vEvnRHL/3EJ3FZGZ2WYwGNJBVfniiy/w9/dn+vTpADz++OPs2LGDJk2aeFa4HE6ObBn9J8xMOYPBcI089thjzJgxA4AmTZowd+5cGjRo4GGpcge5v2VkMBgM14nu3btTokQJZs2axR9//GEU0XUkVymjH37Y4/h/5UqcByUxGAy5gd9//50XX3zREQ4NDeXQoUMMHz4cLy+zXOR6kquU0YIFfzv+lyo1hQ8/3OJBaQwGQ07lzJkz3H///bRs2ZKxY8eydu1aR1zx4sU9KFnuJdeMGakqK1fuc4QvX46jZs2ScMpzMhkMhpyFqvLhhx/y1FNPcfr0afLnz8/o0aNNd1wWkGuU0e7dZwgPj3SECxfOT7NmVWC9B4UyGAw5hh07djBs2DBWrVoFQOvWrZk9ezZ+fn4elixvkGu66erWLcvffw9zhFu3rmFMABkMBreZOnUqq1atoly5cnz00UesXLnSKKIsJFe0jDoB3wEElnfse+21dp4Sx2Aw5BAuXLhAiRIlAJg8eTJFihRh7NixZvGqB8gVLaPky1o7Av7+5TwhisFgyAEcO3aMPn360Lx5c2JiYgAoW7Ys06dPN4rIQ+QKZZSI2tu3nhbEYDBkS+Lj43nrrbfw8/Pjs88+49ChQ/z111+eFstALlNGBoPBkBqbNm2iWbNmjBw5koiICLp06cKOHTto3ry5p0UzkMnKSEQ6iMguEdkjIinsZ4tICRH5WkS2iMg/IjIoM+UxGAx5k/Hjx9O0aVM2bdpE1apVWbp0KV999RXVqlXztGgGm0xTRiLiBcwC7gDqAX1FpF6yZA8D21U1BAgF3hCRDDkACQs77vh/6NAFY3nBYDCkoFatWogITz75JNu3b6dr166eFsmQjMxsGTUF9qjqPlWNARYByZ8ABYqJiABFgbNAhrTJW2+tc/yvXn068+cbqwsGQ15n3759fPrpp45w//79+eeff3j99dcpWrSoByUzpEZmKqPKwGGn8BF7nzMzAX/gGPA38KiqJiTPSEQeFJGNIrIxedzp05eThMuUKXSNYhsMhpxKTEwML7/8MgEBAQwYMIA9eyx7lSLi8MRqyJ5kpjISF/uS+3FoD4QBlYD6wEwRSWH4SVXfUdXGqto4edyZM5eShMuUKfwfxTUYDDmZ1atXU79+fZ577jmio6Pp1auXsSOXg8hMZXQEqOoUroLVAnJmELBELfYA+4EMLXlu2vRqY6tSpWKUK2eUkcGQlzh9+jSDBg2iVatW7NixA19fX1asWMHHH39M+fLl08/AkC3ITGW0AfAVkZr2pIS7gWXJ0hwC2gCISAWgLrCPDDB1anvH/6NHnyAgwDx8BkNeYujQoXzwwQf4+PgwYcIEtm7dSps2bTwtliGDZJo5IFWNE5FHgOWAFzBPVf8RkaF2/BzgReADEfkbq1tvlKqeziyZDAZD7iAhIYF8+axv6UmTJnH58mWmT5+Or6+vhyUz/FdENWe54y5Xvb6eOhiWZF/i4JTLK3nDjjVuxw2GHM+lS5d48cUXCQsL47vvvsOaiGtwBxHZ5GrcPbuQKwylGgyG3M+3337LI488woEDBxAR1q9fT7NmzTwtluE6YcwBGQyGbM2RI0fo2bMnnTt35sCBA4SEhLB27VqjiHIZRhkZDIZsy+zZs/H392fJkiUUKVKEqVOnsnHjRmNPLheSo5VRv35LaNz4HUf40KELHpTGYDBcb06fPk1kZCTdu3dnx44dPP7443h7m9GF3EiOvqsbNx5j9+4zjvDJk1FUq1bCgxIZDIZr4fz58+zcudPR8hk1ahRNmzalQ4cOHpbMkNnk2JZRTEw8e/eeTbLPz6/s1cCSTldn0hkMhmyNqrJo0SL8/f3p0qULZ89a77aPj49RRHmEHKuM9u49S3x80unaRYs6Gfze7+T/tWbHLJLKYDBklD179tChQwf69u3L8ePH8fX15cIF0+We18ix3XR+fmU5duwJdu48zW1pJTTriwyGbMmVK1d49dVXmTRpEleuXKFUqVK8+uqrDB482LGg1ZB3cFsZiUgRVY3KTGEygohQsWIxKlYs5mlRDAbDf6BPnz589dVXANx333289tprxpZcHibdzw8RuUlEtgM77HCIiMzOdMkMBkOu5rHHHsPPz4+ff/6Z+fPnG0WUx3GnZTQNy9XDMgBV3SIit2aqVAaDIVeRkJDAvHnz2LFjB2+88QYAoaGhbNu2DS8vLw9LZ8gOuNVNp6qHk9mAis8ccQwGQ27j77//ZujQoaxduxawuuRCQkIAjCIyOHBHGR0WkZsAtV1BjMTusstOfLOkU9IZdAaDwaNERUUxYcIEpk6dSnx8PDfccAPTp08nODjY06IZsiHuKKOhwJtYLsOPAD8CwzNTqPT4998z/P77Ifz8ylpri0oVopMrRWSmdBsMHuHrr7/mkUce4dChQ4gIDz/8MJMmTaJECbMo3eAad5RRXVXt57xDRG4G1mSOSOnz4497eeSR76/u0HFX/5up3AaDx1m6dCmHDh2iQYMGzJ07lyZNmnhaJEM2x53J/G+5uS/L+OOPI548vcFgSEZcXBwHDx50hKdMmcJbb73F+vXrjSIyuEWqLSMRaQHcBJQTkSecoopjeW71GL/9dsiTpzcYDE78+eefDB06lCtXrrBlyxYKFChA2bJleeSRRzwtmiEHkVY3XQGgqJ3GeWXpRaBXZgqVHs2aVcbPryzR0XFcuRLHOk8KYzDkUc6dO8eYMWOYO3cuqkqNGjU4cOAAderU8bRohhxIqspIVVcBq0TkA1U9mFo6T/DZZ3clCRtzqAZD1qGqfPLJJzz++OOcPHkSb29vnn76aZ5//nkKFy7safEMORR3JjBcEpHXgACgYOJOVU3TJJzBYMid9OvXj08++QSAli1b8vbbbxMQEOBhqQw5HXcmMCwAdgI1gQnAAWBDJspkMBiyMR06dKBMmTLMmzePX3/91Sgiw3XBHWVURlXfA2JVdZWqDgaMz1+DIY+wYsUK5s6d6wj379+f3bt3M2jQIGNd23DdcKebLtb+DReRTsAxoErmiWQwGLIDJ06c4IknnmDhwoX4+PjQtm1bateujYhQunRpT4tnyGW4o4xeEpESwJNY64uKA49lplAGg8FzJCQk8M477zB69GguXLhAwYIFGTt2LFWrVvW0aIZcTLrKSFW/sf9eAFqDwwKDx+jceSE+Pt74+HhRs2ZJmNTGk+IYDLmGLVu28NBDD7FunbVg4o477mDmzJnUqlXLw5IZcjtpLXr1Anpj2aT7QVW3iUhnYAxQCGiQNSKm5Ntv/3X8DwmpYJSRwXCdeOaZZ1i3bh2VKlXizTffpGfPniSz2G8wZApptYzeA6oC64EZInIQaAGMVtWlWSCbW/j45FjP6QaDx1FVLl26RJEiRQCYMWMGc+bMYcKECRQvXtzD0hnyEmnV5I2BYFVNEJGCwGngRlU9njWiuUfBgkYZGQz/hYMHDzJixAiioqJYsWIFIkLdunWZNm2ap0Uz5EHSqsljVDUBQFWjRWR3dlFEX3/dl+joOKKj4yhbtjCrPS2QwZCDiI2NZdq0aUyYMIFLly5RrFgx/v33X2PGx+BR0lJGfiKy1f4vQG07LICqqsc8ZHXubF4ag+G/sGbNGoYOHcq2bdsA6NOnD1OnTqVSpUoelsyQ10lLGflnmRQGgyHTGTFiBDNnzgSgVq1azJo1iw4dOnhYKoPBIi1DqdnKOKrBYLg2ypUrR/78+Rk1ahRjxoyhUKFCnhbJYHCQqbY8RKSDiOwSkT0iMjqVNKEiEiYi/4jIqsyUx2DIS+zcuZMff/zRER41ahRbt27lxRdfNIrIkO3INGVkr1OaBdwB1AP6iki9ZGlKArOBLqoaANyVPJ/U6IQ1eGVWQBgMSbl8+TIvvPACwcHB3HvvvZw9exYAHx8f/Pz8PCydweAat5SRiBQSkboZzLspsEdV96lqDLAI6JoszT3AElU9BKCqJ93J+NixCL7LoDAGQ17gxx9/JCgoiJdeeonY2Fi6dOliFq0acgTpKiMRuRMIA36ww/VFZJkbeVcGDjuFj9j7nKkDlBKRX0Vkk4jc547QlStPdfx/dsxK1J2DDIZcTHh4OHfffTft27dn7969BAQE8Ntvv/Huu+9SqlQpT4tnMKSLOy2j8VitnPMAqhoG1HDjOFefY8n1hjfQCKvXrT3wgoikmLctIg+KyEYR2Zg8zsfHyw1RDIbcTY8ePfj0008pVKgQU6ZMYfPmzdxyyy2eFstgcBt3lFGcql74D3kfwTInlEgVLPcTydP8oKpRqnoaWA2EJM9IVd9R1caq2jh5nDEHZMirqF79tnvllVfo3Lkz27dv55lnniF//vwelMxgyDjuKKNtInIP4CUiviLyFrDWjeM2AL4iUlNECgB3A8m7974CWoqIt4gUBpoBO9LLuEKFIo7/hQoZZWTIW0RERPD444/z0EMPOfa1atWKr7/+mho1anhOMIPhGhDnryuXCSwl8Rxwu71rOfCSqkanm7lIR2A64AXMU9VJIjIUQFXn2GmeBgYBCcC7qjo9rTzLVa+vp6ZVhv2pTGF40owgGXInqsqSJUt49NFHOXr0KN7e3vz7779GARncQkQ2uepdyi64o4waqOrmLJInXcpVr6+nRm5xHVmzI/T4NmsFMhiygP379/PII4/w3XfWR1jTpk2ZM2cODRp4zJOLIYeR3ZWRO31cU0WkIrAYWKSq/2SyTG4jT6qZSWfI1agqr776KhMmTODy5cuUKFGCyZMn8+CDD+LlZSbvGHIP6Y4ZqWprIBQ4BbwjIn+LyPOZLZjBYAARYffu3Vy+fJm+ffuyc+dOhg0bZhSRIdeRbjddksQiQcAzQB9VLZBpUqWBczedaRkZciOnT5/m+PHjBAYGOsKbN2+mXbt2HpbMkJPJ7t107ix69ReR8SKyDZiJNZOuSqZLZjDkMVSVDz74AD8/P+666y5iYmIAKFu2rFFEhlyPO2NG7wOfALeravJ1QgaD4TqwY8cOhg4dyurVlqvIkJAQzp07R4UKFTwsmcGQNaSrjFS1eVYIYjDkRS5dusSkSZN47bXXiI2NpVy5ckydOpV+/foZm3KGPEWqykhEPlPV3iLyN0nN+Hjc06vBkBtQVW677TbWrVsHwEMPPcTkyZONLTlDniStltGj9m/nrBDEYMhriAjDhw/n0qVLzJ07lxYtWnhaJIPBY6Q6gUFVw+2/w1X1oPMGDM8a8QyG3EN8fDxvvfUWU6detTrfv39/Nm3aZBSRIc/jjm06V9N47rjeghgMuZmNGzfSrFkzRo4cyZgxYzh2zJoLJCLGqKnBQBrKSESG2eNFdUVkq9O2H9iadSIaDDmXCxcuMGLECJo2bcqmTZuoWrUqn376KZUqVfK0aAZDtiKtMaOFwPfAZGC00/4IVT2bqVIZDDkcVWXx4sU89thjhIeH4+XlxeOPP864ceMoWrSop8UzGLIdaSkjVdUDIvJw8ggRKW0UksGQNnPnziU8PJzmzZszZ84cQkJSuOoyGAw26bWMOgObsKZ2Oy96UKBWJsplMOQ4rly5wvnz56lQoQIiwuzZs/n111954IEHyJfPneFZgyHvkqoyUtXO9m/NrBPHYMiZrFq1iqFDh1KpUiVWrFiBiFC3bl3q1q3radEMhhyBO7bpbhaRIvb/e0VkqohUy3zRDIbsz6lTpxg4cCChoaHs3LmTw4cPc+LECU+LZTDkONzpO3gbuCQiIVgWuw8CH2WqVAZDNichIYH33nsPPz8/5s+fj4+PDxMmTGDr1q3ccMMNnhbPYMhxuGMoNU5VVUS6Am+q6nsiMiCzBTMYsiuqSvv27VmxYgUAbdu2Zfbs2fj6+npYMoMh5+JOyyhCRJ4F+gPfiogXYFbpGfIsIkLLli2pUKECCxcu5McffzSKyGC4RtJ1riciNwD3ABtU9Td7vChUVT/MCgGTY5zrGTzBt99+S2xsLN26dQOsmXOXL1+mZMmSHpXLYHCXHO9cT1WPAwuAEiLSGYj2lCIyGLKaI0eO0LNnTzp37swDDzzA2bPW8jofHx+jiAyG64g7s+l6A+uBu4DewDoR6ZXZghkMniQuLo5p06bh7+/PkiVLKFKkCGPGjKF48eKeFs1gyJW4M4HhOaCJqp4EEJFywArg88wUzGDwFOvXr+ehhx4iLCwMgO7du/Pmm29StWpVzwpmMORi3FFG+RIVkc0Z3Jv4YDDkOBISEhg0aBDbt2+nWrVqzJw5kzvvvNPTYhkMuR53lNEPIrIc+MQO9wG+yzyRDIasRVW5cuUKBQsWJF++fMyaNYvvv/+esWPHUqRIEU+LZzDkCdKdTQcgIj2AW7Ds061W1S8zW7DUMLPpDNeTPXv2MHz4cKpWrcp7773naXEMhkwjx86mExFfEflKRLZhTV54Q1Uf96QiMhiuF1euXGHixIkEBgby008/sXTpUs6cOeNpsQyGPEtaYz/zgG+AnliWu9/KEokMhkzm559/Jjg4mHHjxnHlyhUGDBjAzp07KVOmjKdFMxjyLGmNGRVT1f/Z/3eJyF9ZIZDBkFnEx8czaNAgPvrIMq1Yt25d5syZQ2hoqGcFMxgMaSqjgiLSgKt+jAo5h1XVKCdDjsLLywtvb28KFizI888/z1NPPYWPj4+nxTIYDKQxgUFEfknjOFXV2zJHpLQxExgMGeHvv/8mOjqaJk2aAHDmzBnOnz9P7dq1PSyZwZC1ZPcJDGk512udlYIYDNeTqKgoxo8fz7Rp0/D19WXLli0UKFCAMmXKmLEhgyEb4s46I4MhR7Fs2TJGjBjBoUOHEBHatm1LbGwsBQoU8LRoBoMhFTLVkoKIdBCRXSKyR0RGp5GuiYjEG5t3hmvh0KFDdOvWja5du3Lo0CEaNmzI+vXreeutt8ziVYMhm5NpLSPb79EsoB1wBNggIstUdbuLdFOA5ZkliyH3Ex8fT2hoKPv376dYsWK89NJLDB8+HG9v0/g3GHIC7ljtFhG5V0TG2uFqItLUjbybAntUdZ+qxgCLgK4u0o0AvgBOuogzGNIkcQKOl5cX48ePp1evXuzYsYORI0caRWQw5CDc6aabDbQA+trhCKwWT3pUBg47hY/Y+xyISGWgOzAnrYxE5EER2SgiG904ryEPcO7cOYYOHcrLL7/s2Ne/f38WL15M5cqV0zjSYDBkR9xRRs1U9WEgGkBVzwHujASLi33JZ2JPB0apanxaGanqO6raODtPSzRkDarKggUL8PPzY+7cuUyZMoULFy4Aljtwg8GQM3GnHyPWHtdRcPgzSnDjuCOAswOYKsCxZGkaA4vsSqQs0FFE4lR1qRv5G/IYu3fvZvjw4axcuRKAli1b8vbbb1OiRAkPS2YwGK4Vd1pGM4AvgfIiMgn4HXg57UMA2AD4ikhNESkA3A0sc06gqjVVtYaq1sBy1jfcKCJDcuLi4hg/fjxBQUGsXLmSMmXKMG/ePFatWkVAQICnxTMYDNeBdFtGqrpARDYBbbC63rqp6g43josTkUewZsl5AfNU9R8RGWrHpzlOZDAk4uXlxW+//UZMTAyDBw9mypQplC1b1tNiGQyG60i6/oxEpJqr/ap6KFMkSgdjDihvcOLECaKjo6levToA//77L+Hh4dx6660elsxgyJlkd3NA7nTTfYvlSuJbYCWwD/g+M4Uy5F0SEhKYM2cOdevWZciQIY6p276+vkYRGQy5GHe66YKcwyLSEHgo0yQy5FnCwsIYOnQo69atA6BAgQJERkZSrFgxD0tmMBgymwybA7JdRzTJBFkMeZSIiAieeOIJGjVqxLp166hUqRKLFy/m22+/NYrIYMgjpNsyEpEnnIL5gIbAqUyTyJCniImJoWHDhuzZs4d8+fLx6KOPMnHiRIoXL+5p0QwGQxbizjoj50/TOKyxoy8yRxxDXqNAgQL079+fr7/+mjlz5tCoUSNPi2QwGDxAmrPp7MWur6jq01knUtqY2XQ5m9jYWKZNm0a1atW4++67Aat15OXlhZeXl4elMxhyL9l9Nl2qLSMR8bbXCjXMSoEMuZc1a9YwdOhQtm3bRrly5ejcuTNFixY1foYMBkOa3XTrscaHwkRkGbAYiEqMVNUlmSybIZdw9uxZRo0axbvvvgtArVq1mD17NkWLFvWwZAaDIbvgzphRaeAMcBuWfTqxf40yMqSJqvLRRx/x5JNPcvr0afLnz8+oUaMYM2YMhQoV8rR4BoMhG5GWMipvz6TbxlUllIgZqsmhxMbGcuTIEaKjozP9XKpK5cqV+eijj/Dx8aFMmTLkz5+fAwcOZPq5DYa8SsGCBalSpQr58+f3tCgZIi1l5AUUxT1XEFnOtzU7elqEHMmRI0coVqwYNWrUyBSXCwkJCSQkJDgc21WtWpUrV65QpkwZ4+LBYMhkVJUzZ85w5MgRatas6WlxMkRayihcVSdmmSQZ4Umls6dlyKFER0dnmiK6cOEChw4dcig7gGLFipmFqwZDFiEilClThlOnct5S0LSUkfmMzaVcb0UUExPD4cOHOXfuHAD58uUjPj7eTNU2GDxATu2BSEsZtckyKQw5ElXl1KlTHD16lPj4ePLly0elSpUoX748+fJl2NKUwWDIw6RaY6jq2awUxJCzSEhIYOfOnRw6dIj4+HhKlChBQEAAN9xwg1FEmcSBAwcoVKgQ9evXp169etx3333ExsY64n///XeaNm2Kn58ffn5+vPPOO0mO//DDDwkMDCQgIIB69erx+uuvZ/UlpMvSpUuZODF7jg6AtUyhXbt2+Pr60q5dO0dvQHLefPNNR1lPnz7dsb9Pnz7Ur1+f+vXrU6NGDerXrw9YE4sGDBhAUFAQ/v7+TJ482XFMhw4dCAkJISAggKFDhxIfHw/AzJkzef/99zPtWrMcVc1RW9lqIapOOwwZY/v27UnCMD7Jlhpz525Mku6BB5bp/v37dcuWLXr27FlNSEjIbNHdJi4uzmPnTkhI0Pj4+EzJe//+/RoQEKCq1jW2bt1aP/74Y1VVDQ8P16pVq+qmTZtUVfXUqVPasGFD/eabb1RV9bvvvtMGDRro0aNHVVX18uXL+s4771xX+WJjY685jxYtWuipU6ey9JwZ4emnn9bJkyerqurkyZP1mWeeSZHm77//1oCAAI2KitLY2Fht06aN7t69O0W6J554QidMmKCqqgsWLNA+ffqoqmpUVJRWr15d9+/fr6qqFy5cUFXr2erRo4d+8sknjnT169d3KWfy91xVFdio2aAOT20zn7AGt1AXZqOqVKlCQEAApUqVcruf+sCBA/j5+XH//fcTGBhIv379WLFiBTfffDO+vr6sX78egPXr13PTTTfRoEEDbrrpJnbt2gVAfHw8Tz31FEFBQQQHB/PWW28BUKNGDSZOnMgtt9zC4sWL+eSTTwgKCiIwMJBRo0a5lCUyMpI2bdrQsGFDgoKC+OqrrwAYNWoUs2fPdqQbP348b7zxBgCvvfYaTZo0ITg4mHHjxjmuyd/fn+HDh9OwYUMOHz7MsGHDaNy4MQEBAY50AN999x1+fn7ccsstjBw5ks6drak4UVFRDB48mCZNmtCgQQOHLKnh5eVF06ZNOXr0KACzZs1i4MCBNGxoGUwpW7Ysr776Kq+88goAkydP5vXXX6dSpUqANf33gQceSJHviRMn6N69OyEhIYSEhLB27VoOHDhAYGCgI83rr7/O+PHjAQgNDWXMmDG0atWKSZMmUaNGDRISEgC4dOkSVatWJTY2lr1799KhQwcaNWpEy5Yt2blzZ4pz7969Gx8fH4cX36+//ppmzZrRoEED2rZty4kTJxz348EHH+T222/nvvvu49SpU/Ts2ZMmTZrQpEkT1qxZA6T+DF0LX331FQMGDABgwIABLF26NEWaHTt20Lx5cwoXLoy3tzetWrXiyy+/TJJGVfnss8/o27cvYI3zREVFERcXx+XLlylQoIDDWHDib1xcHDExMY53rXDhwtSoUcPxzuR4PK0NM7qZltG18V9aRtHR0Tpx4ncpWkb/hf3796uXl5du3bpV4+PjtWHDhjpo0CBNSEjQpUuXateuXVXV+hpM/Or96aeftEePHqqqOnv2bO3Ro4cj7syZM6qqWr16dZ0yZYqqqh49elSrVq2qJ0+e1NjYWG3durV++eWXKWSJjY11fHWeOnVKa9eurQkJCfrXX3/prbfe6kjn7++vBw8e1OXLl+sDDzzgaP106tRJV61apfv371cR0T/++MNxTKJccXFx2qpVK92yZYtevnxZq1Spovv27VNV1bvvvls7deqkqqrPPvusfvTRR6qqeu7cOfX19dXIyMgUZZfYMrp8+bKGhobqli1bVFW1e/fuunTp0iTpz58/r6VKlVJV1VKlSun58+fTvT+9e/fWadOmOWQ/f/58kvOqqr722ms6btw4VVVt1aqVDhs2zBHXpUsX/fnnn1VVddGiRTpkyBBVVb3tttscrYM///xTW7duneLc8+bN0yeeeMIRdm5x/+9//3PEjRs3Ths2bKiXLl1SVdW+ffvqb7/9pqqqBw8eVD8/P1VN/Rly5uLFixoSEuJy++eff1KkL1GiRJJwyZIlU6TZvn27+vr66unTpzUqKkqbN2+ujzzySJI0q1at0kaNGjnCMTEx2qdPHy1btqwWLlxY586dmyT97bffriVLltS+ffsmafm/9NJL+vrrr7uUITlk85aROxYYDHmUhIQETpw4QXh4OJcvX75u+dasWZOgIMtnY0BAAG3atEFECAoKciyIvXDhAgMGDODff/9FRBxjIytWrGDo0KGOdUylS5d25NunTx8ANmzYQGhoKOXKlQOgX79+rF69mm7duiWRQ1UZM2YMq1evJl++fBw9epQTJ07QoEEDTp48ybFjxzh16hSlSpWiWrVqzJgxgx9//JEGDRoAVsvq33//pVq1alSvXp3mzZs78v7ss8945513iIuLIzw8nO3bt5OQkECtWrUc6z/69u3rGNf58ccfWbZsmWMcJzo6mkOHDuHv759E5r1791K/fn3+/fdfevXqRXBwsONaXLVOMzqz6ueff+bDDz8ErNZXiRIlUh0XSSSx3BP/f/rpp7Ru3ZpFixYxfPhwIiMjWbt2LXfddZcj3ZUrV1LkEx4e7rhnYK2J69OnD+Hh4cTExCRZN9OlSxeHFY8VK1awfft2R9zFixeJiIhI9RlyplixYoSFhaVTKhnD39+fUaNG0a5dO4oWLUpISIjjeU3kk08+cbSKwGrFeXl5cezYMc6dO0fLli1p27YttWrVAmD58uVER0fTr18/fv75Z9q1awdA+fLlXbYycyJGGeVxVMe53B8REcHBgwcdlhruv78+EybceV1Wdfv4+Dj+58uXzxHOly8fcXFxALzwwgu0bt2aL7/8kgMHDhAaGmrL67rSBShSpIgjjSvWrVvHQw9ZToonTpzI2bNnOXXqFJs2bSJ//vzUqFHDcb29evXi888/5/jx4w7r4qrKs88+68gjkQMHDjjODbB//35ef/11NmzYQKlSpRg4cCDR0dGpypWY9xdffEHdunVTTQNQu3ZtwsLCCA8PJzQ0lGXLltGlSxcCAgLYuHEjXbp0caTdtGkT9erVAyylv2nTJm677bY083eFt7e3o+sNSGG9w/nau3TpwrPPPsvZs2cd54uKiqJkyZLpVvqFChXiwoULjvCIESN44okn6NKlC7/++qujazD5ORMSEvjjjz9SmJgaMWKEy2fImYiICFq2bOlSnoULFzrKL5EKFSoQHh5OxYoVCQ8Pp3z58i6PHTJkCEOGDAFgzJgxVKlSxREXFxfHkiVL2LRpU5JzdejQgfz581O+fHluvvlmNm7c6FBGYHWtdunSha+++sqhjKKjo3ONaS0zZmRIQUJCAnv37iU6OhofHx/q1KlDrVq1stS8yIULF6hcuTIAH3zwgWP/7bffzpw5cxxK6+zZlJM+mzVrxqpVqzh9+jTx8fF88skntGrVimbNmhEWFkZYWBhdunThwoULlC9fnvz58/PLL79w8OBBRx533303ixYt4vPPP6dXr14AtG/fnnnz5hEZGQnA0aNHOXnyZIrzX7x4kSJFilCiRAlOnDjB999/D4Cfnx/79u1ztP4+/fRTxzHt27fnrbfeciiszZs3p1k+FStW5JVXXnHMunr44Yf54IMPHBX+mTNnGDVqFM888wwAzz77LM888wzHjx8HrJbJjBkzUuTbpk0b3n77bcAan7t48SIVKlTg5MmTnDlzhitXrvDNN9+kKlfRokVp2rQpjz76KJ07d8bLy4vixYtTs2ZNFi9eDFiKd8uWLSmO9ff3Z8+ePY6w8zMwf/78VM95++23M3PmTEc4sQxSe4acSWwZudqSKyKwlG2iLPPnz6dr164u8018Lg4dOsSSJUuStIJWrFiBn59fEgVVrVo1fv75Z1SVqKgo/vzzT/z8/IiMjCQ8PBywlFjimGMiu3fvTjKel5MxysgAXB07BKuFUrVqVSpWrEhAQIBHvK4+88wzPPvss9x8882OqawA999/P9WqVSM4OJiQkBAWLlyY4tiKFSsyefJkWrduTUhICA0bNnRZafTr14+NGzfSuHFjFixYkOQlDwgIICIigsqVK1OxYkXAqvTuueceWrRoQVBQEL169SIiIiJFviEhITRo0ICAgAAGDx7MzTffDFhf/rNnz6ZDhw7ccsstVKhQgRIlSgBWSzA2Npbg4GACAwN54YUX0i2jbt26cenSJX777TcqVqzIxx9/zAMPPICfnx833XQTgwcP5s477wSgY8eOPPzww7Rt25aAgAAaNWrkUOjOvPnmm/zyyy8EBQXRqFEj/vnnH/Lnz8/YsWNp1qwZnTt3TlJOrujTpw8ff/xxku67BQsW8N577zmmKLuaoHHrrbeyefNmx3M4fvx47rrrLlq2bOmY1OCKGTNmsHHjRoKDg6lXrx5z5swBUn+GroXRo0fz008/4evry08//cTo0aMBOHbsGB07XjVR1rNnT+rVq8edd97JrFmzKFWqlCNu0aJFSZQTWB8TkZGRBAYG0qRJEwYNGkRwcDBRUVF06dLF8byXL1+eoUOHOo5bs2YNbdu2vS7X5mnSdK6XHSlXvb6eOhjmMA+Rs6T3PDt27EgxDnH58mUOHjxI8eLFHbOtDJlDZGQkRYsWRVV5+OGH8fX15fHHH/e0WNmGRx99lDvvvDPXVLCZyebNm5k6dSofffRRijhX73l2d65nWkZ5mPj4eI4cOcL27duJjIzk9OnTScYGDNef//3vf9SvX5+AgAAuXLiQYvwprzNmzBguXbrkaTFyBKdPn+bFF1/0tBjXDdMyymMkfjElGjVNnNVUrlw5KleunGLWj8FgyHnkxJaRqXnyGImTExKn6xYqVIjq1asbr6sGg8Gj5EhllDNt0mYPEqdPJxo1rVChQo618mswGHIPOVIZJWLc67nHxo0bKVmyJDfeeCOAw9eQ83ofg8Fg8CQ5cgKD2tu3nhYkm3PhwgVGjBhB06ZNGTp0qGPKrI+Pj1FEBoMhW5EjlZEhbVSVTz/9FD8/P2bOnEm+fPlo2LChy3UlnsDLy4v69esTGBjInXfeyfnz5x1x//zzD7fddht16tTB19eXF198MYnlgu+//57GjRvj7++Pn58fTz31lAeu4L/Rt29fgoODmTZtmlvpM2scT1UZOXIkN954I8HBwfz111+pprvtttu4ePFipshxPZg/fz6+vr74+vqmujD24MGDtGnThuDgYEJDQzly5AgAv/zyi8OdQ/369SlYsKDD8GnLli0d+ytVquQwJZVa2cXExHDrrbdmm3csR+Jp43gZ3RINpRpcs2fPHm3fvn1i41FbtGjhMKap6tqAYlZTpEgRx//77rtPX3rpJVVVvXTpktaqVUuXL1+uqpaJ/A4dOujMmTNV1TLNX6tWLd2xY4eqWoZOZ82adV1lyyyXBOHh4VqtWrUMHeNcTteTb7/9Vjt06KAJCQn6xx9/aNOmTV2m++abb/Sxxx7LUN5Z6b7jzJkzWrNmTT1z5oyePXtWa9asqWfPnk2RrlevXvrBBx+oqurKlSv13nvvdZlXqVKlNCoqKkVcjx49dP78+aqadtmNHz/e4dLD0+REQ6keFyCjm1FGqXPx4kUtWbKkAlqyZEmdO3duCt86zg9pZt2k9HCuZN9++22H1ed3331X+/fvnyTtnj17tEqVKqqq2r9/f33vvffSzT8iIkIHDhyogYGBGhQUpJ9//nmK8y5evFgHDBigqqoDBgzQxx9/XENDQ/Wxxx7T6tWr67lz5xxpa9eurcePH9eTJ09qjx49tHHjxtq4cWP9/fffU5z78uXLjnPXr1/fYcE6KChICxYsqCEhIbp69eokxxw/fly7deumwcHBGhwcrGvWrEkib0REhN52223aoEEDDQwMdFjnjoyM1I4dO2pwcLAGBATookWLVFV11KhR6u/vr0FBQfrkk0+mkPHBBx/UhQsXOsJ16tTRY8eOpUjXt29f/eWXXxzhrl27asOGDbVevXpJrEoXKVJEX3jhBW3atKn+9ttv+tFHH2mTJk00JCREH3zwQYeCGjp0qDZq1Ejr1aunY8eOTXG+jLJw4UJ98MEHU72uROrVq6eHDx9WVcsnULFixVKkmTt3rt5zzz0p9ie+U4nW3dMqu7CwML3jjjuu7aKuE0YZJc8cOgC7gD3AaBfx/YCt9rYWCEkvT6OM0mbChAnav39/PXHihMv47KSM4uLitFevXvr999+rqurjjz+u06dPT5E+sTJo0KCBhoWFpZv/M888o48++qgjnPi1nJYy6tSpk6PSHDlypM6bN09VLXcHbdq0UdXUXRU48/rrr+vAgQNVVXXHjh1atWpVvXz5cgo3DM64ctvgLG9qri4+//xzvf/++x35nD9/Xs+cOaN16tRxuF5wVqqJdOrUyXEdqpZ7hw0bNqRIV61aNb148aIjnOgW49KlSxoQEKCnT59WVVVAP/30U1W1nq/OnTtrTEyMqqoOGzbM0apw5VYjOa+++qpLdw4jRoxIkfa1117TF1980RGeOHGivvbaaynS9e3b1/FcffHFFwo4ZE+kdevW+vXXX6c4dv78+dqzZ09HOK2yi4uL07Jly6bIwxPkRGWUabPpRMQLmAW0A44AG0Rkmapud0q2H2ilqudE5A7gHaBZZsmU2zh16hRPP/00bdq0oX///oBl48zdqdqeWjB8+fJl6tevz4EDB2jUqJHDArFq6ha5MzL9fMWKFSxatMgRdrYLlhp33XUXXl5egGVbbeLEiQwaNIhFixY5bKyl5qqgWLFijn2///47I0aMACzDqNWrV2f37t1p2vdz5bbBGVXXri6CgoJ46qmnGDVqFJ07d6Zly5bExcVRsGBB7r//fjp16uRw3pc8v+S4Kt+zZ88mubYZM2Y4nMQdPnyYf//9lzJlyuDl5UXPnj0BWLlyJZs2baJJkyaAda8TLVu7cquR6AIjkaeffpqnn3461bL6L9fx+uuv88gjj/DBBx9w6623pljcHR4ezt9//0379u1THPvJJ59w//33u3VOLy8vChQokOKZMLhHZk5gaArsUdV9qhoDLAKSWKtU1bWqmugs5U+gCoZ0SUhI4N1336Vu3brMnz+f5557zuGrJSesGSpUqBBhYWEcPHiQmJgYZs2aBeBwg+DMvn37KFq0KMWKFXO4QUiP1JSa87603CC0aNGCPXv2cOrUKZYuXUqPHj2Aq64KEq06Hz16NEWl46qyulYWLFjgcHURFhZGhQoViI6Opk6dOmzatImgoCCeffZZJk6ciLe3N+vXr6dnz54sXbqUDh06pMivSpUqHD582BE+cuSIS5uEzq4jfv31V1asWMEff/zBli1baNCggaMMCxYs6FDkqsqAAQMcZbRr1y7Gjx/vcKuxcuVKtm7dSqdOnVLcA7A86TpPKkjcRo4c+Z+vo1KlSixZsoTNmzczadIkgCQK/7PPPqN79+4prNKfOXOG9evX06lTJ7fPeeXKFQoWLJhCBkP6ZKYyqgwcdgofsfelxhDge1cRIvKgiGwUkY2u4vMS27Zt49Zbb+WBBx7g3LlztG3blpUrV2ape4frRYkSJZgxYwavv/46sbGx9OvXj99//50VK1YA1lf1yJEjHW4Qnn76aV5++WV2794NWMph6tSpKfJN7lIg0dpEhQoV2LFjBwkJCSncQDsjInTv3p0nnngCf39/ypQp4zJfV/55br31VhYsWABY5v0PHTqUro8iV24bnEnN1cWxY8coXLgw9957L0899RR//fUXkZGRXLhwgY4dOzJ9+nSXMnbp0oUPP/wQVeXPP/+kRIkSDsvkztStW5d9+/Y5ZChVqhSFCxdm586d/Pnnn6ley+eff+5woXD27FkOHjyYqluN5Dz99NMu3Tm4cnfRvn17fvzxR86dO8e5c+f48ccfXbZunG0uTp48mcGDByeJT+7oLpHFixfTuXPnJMolrbI7c+YM5cqVy5HvYrYgs/r/gLuAd53C/YG3UknbGtgBlEkv37w6ZnTp0iV95pln1NvbWwGtUKGCLly40DE24C7ZbTadqmrnzp31ww8/VFXVrVu3aqtWrbROnTpau3ZtHT9+fJJr/Prrr7Vhw4bq5+en/v7++tRTT6XIPyIiQu+77z4NCAjQ4OBg/eKLL1TVGieqVauWtmrVSh9++OEkY0aLFy9OkseGDRsUcMzCUrXGa3r37q1BQUHq7++vDz30UIpzX758WQcMGJBiAkNaY0bHjx/XLl26aGBgoIaEhOjatWuTlNOpU6e0efPm2qhRIx0yZIj6+fnp/v379YcfftCgoCANCQnRxo0b64YNG/TYsWPapEkTDQoK0sDAwCTyJ5KQkKDDhw/XWrVqaWBgoMvxIlVrDOZ///ufqlqu5zt06KBBQUHaq1cvbdWqlWNyQ/L7uWjRIg0JCdGgoCBt2LChwx37gAED1M/PTzt27Kjdu3fX999/3+V5M8J7772ntWvX1tq1azvG+VRVX3jhBf3qq69U1brvN954o/r6+uqQIUM0OjrakW7//v1aqVKlFBN9VC2X6onjmYmkVXaLFy9O4jbdk+TEMaPMVEYtgOVO4WeBZ12kCwb2AnXcyTevKqPo6Gj18/NTEdHhw4e7HJh2h+ygjAw5g2PHjmnbtm09LUaOoXv37rpz505Pi6GqOVMZZaY5oA2Ar4jUBI4CdwP3OCcQkWrAEqC/qu7ORFlyJEeOHKFw4cKULl0aHx8fh7fKZs3MHA9D5lOxYkUeeOABLl686BEHizmJmJgYunXrlm6XrCF1Mm3MSFXjgEeA5VhdcJ+p6j8iMlREEl0VjgXKALNFJMyMCVnExcUxbdo0/P39k8wsatasmVFEhiyld+/eRhG5QYECBbjvvvs8LUaOJlMNparqd8B3yfbNcfp/P3B/8uPyMuvWreOhhx5iy5YtgDVwHBcXZ/wMGQyGXI2xTZdNOH/+PMOHD6dFixZs2bKF6tWr8/XXX/P5558bRWQwGHI9ppbLBpw7d4569epx/PhxvL29efLJJ3nhhReSrH0xGAyG3IxRRtmAUqVKcccdd7B7927efvttgoKCPC2SwWAwZCmmm84DXLlyhYkTJ7Jq1SrHvpkzZ7J69eo8oYiMCwnPupDYuXMnLVq0wMfHh9dffz3VdKq524UEXH0W69evT5cuXRz7Z86cyY033oiIcPr0acd+ZwsRgYGBeHl5cfbsWeNC4nrg6bnlGd1y+jqjlStXap06dRRQf3//LDW5r5o91hkZFxLukVkuJE6cOKHr16/XMWPGuDQsmkhecCGRWhn/9ddfun//fq1evbqeOnXKZZply5Zp69atHWHjQuLaNtMyyiJOnjxJ//79adOmDbt378bPz4/Zs2c7bHp5hDckc7YM0KJFC44ePQrAwoULufnmm7n99tsBKFy4MDNnzuSVV14B4NVXX+W5557Dz88PsGynDR8+PEWekZGRDBo0iKCgIIKDg/niiy+ApC2Nzz//nIEDBwIwcOBAnnjiCVq3bs3TTz9NjRo1krTWbrzxRk6cOMGpU6fo2bMnTZo0oUmTJqxZsybFuaOjox3nbtCgAb/88gtgmRI6efIk9evX57fffktyzIkTJ+jevTshISGEhISwdu3aFNfTpk0bGjZsSFBQEF999RUAUVFRdOrUiZCQEAIDA/n0008BGD16NPXq1SM4ONhly7F8+fI0adIkXbM1CxYsoGvXq+Yku3XrRqNGjQgICOCdd95x7C9atChjx46lWbNm/PHHH3z88cc0bdqU+vXr89BDDxEfHw/AsGHDaNy4MQEBAYwbNy7Nc7vD8uXLadeuHaVLl6ZUqVK0a9eOH374IUW67du306ZNGwBat27tKL+0aNCgATVq1EgzTXIzQt26dXOYgjJkHDNmlMkkGjUdNWoU58+fp2DBgjz//PM8/fTTFChQwNPieZT4+HhWrlzJkCFDAKuLrlGjRknS1K5dm8jISC5evMi2bdt48skn0833xRdfpESJEvz999/AVdt0abF7925WrFiBl5eXw3bdoEGDWLduHTVq1KBChQrcc889PP7449xyyy0cOnSI9u3bs2PHjiT5JBp9/fvvv9m5cye33347u3fvZtmyZXTu3NmlrbiRI0fSqlUrvvzyS+Lj44mMjEwSX7BgQb788kuKFy/O6dOnad68OV26dOGHH36gUqVKfPvtt4C1DODs2bN8+eWX7Ny5ExFJolQzypo1a5g7d64jPG/ePEqXLs3ly5dp0qQJPXv2pEyZMkRFRREYGMjEiRPZsWMHU6ZMYc2aNeTPn5/hw4ezYMEC7rvvPiZNmkTp0qWJj4+nTZs2bN26NYXV7tdee81lhX7rrbemsE939OhRqlat6ghXqVLF8WHjTEhICF988QWPPvooX375JREREZw5c4YyZcoQHR1N48aN8fb2ZvTo0Q6Prulx6dIlfvjhhyS2CgMDA9mwYYNbxxtSYpRRJnPhwgWee+45zp8/T/v27Zk1axa1a9f2tFgWT3rGiYRxIZGUrHYh4S55wYXEoUOHqFSpEvv27eO2224jKCjIrffz66+/5uabb6Z06dKOfcaFxLVhlFEmEBUVhbe3Nz4+PpQqVYo5c+YQHx/PXXfdlSNcPGQ2iS4kLly4QOfOnZk1axYjR44kICCA1atXJ0nryoVESEhImvmnptT+qwuJ559/HrjqQqJQoUJpnvt64+xCIn/+/NSoUSOJC4nvvvuOZ599lttvv52xY8eyfv16Vq5cyaJFi5g5cyY///zzfzpvoguJfPnyJXEhUbhwYUJDQ9N0ITF58uQkeSW6kNiwYQOlSpVi4MCBqbqQcLdlVKVKFX799VdH+MiRI4SGhqY4NtGFBFhdnl988YVD4Se6f6hVqxahoaFs3rzZLWW0aNEil5a+jQuJ/44ZM7rOLFu2jHr16vHqq6869vXs2ZPevXsbRZQM40LCIqtdSLhLbnchce7cOa5cueJIs2bNGurVq5duuVy4cIFVq1YlGU8D40LimvH0DIqMbtl1Nt3Bgwe1a9euiuVAVW+++WaXZuk9TXabTadqXEhktQuJ8PBwrVy5shYrVkxLlCihlStXdrg1dya3u5BYs2aNBgYGanBwsAYGBuq7777rOP7NN9/UypUrq5eXl1asWFGHDBniiHv//fe1T58+KWQxLiSubfO4ABndspsyiomJ0ddee00LFy6sgBYrVkzffPPNLJ+y7S7ZQRkZcgbGhUTGMC4krm0zY0bXwOnTpx2zgsAaBJ82bRqVK6fl0NZgyBkYFxLuY1xIXDtGGV0DZcqUoWzZstSsWZOZM2fSsWNHT4tkMFxXevfu7WkRcgTGhcS1Y5RRBlBVFixYQNOmTalTpw4iwscff0yJEiUoXLiwp8UzGAyGHIuZTecmu3btom3btvTv35/hw4dbA25YXRlGERkMBsO1YZRROkRHRzNu3DiCg4P5+eefKVOmDPfee6+nxTIYDIZchemmS4MVK1YwbNgw9uzZA8DgwYN59dVXHetODAaDwXB9MC2jVDhx4gSdO3dmz5491KtXj9WrV/Pee+8ZRXQdMC4kPOtCYsGCBQQHBxMcHMxNN93kcHGfHNXc70Ji1KhRBAYGJjE0C9a1P/fcc9SpUwd/f3/HotsLFy5w5513EhISQkBAAO+//z6AcSFxPfD03PKMbpm5zig+Pj7JAsspU6bo5MmT9cqVK5l2zqwmO6wzMi4k3COzXEisWbPG4Wrhu+++06ZNm7pMl9tdSHzzzTfatm1bjY2N1cjISG3UqJFj8e+8efO0f//+joXrJ06cUFXVSZMm6TPPPKOqqidPntRSpUo56gfjQsKsM7ouhIWFMXToUB5++GH69+8P4DBDk1t5YPbZTMn3f8NLp5/IpkWLFo51Wqm5kAgNDeXhhx/OkAuJESNGsHHjRkSEcePG0bNnT4oWLeqwiP3555/zzTff8MEHHzBw4EBKly7N5s2bqV+/Pl9++SVhYWGULFkSsFxIrFmzhnz58jF06FAOHToEwPTp07n55puTnDs6Opphw4axceNGvL29mTp1Kq1bt07iQuKtt96iZcuWjmNOnDjB0KFDHaZ33n77bW666aYk19O1a1fOnTtHbGwsL730El27diUqKorevXtz5MgR4uPjeeGFF+jTpw+jR49m2bJleHt7c/vtt6dwoOecd/PmzZO0FJxZsGABDz74oCPcrVs3Dh8+THR0NI8++qgjrmjRojzxxBMsX76cN954gwMHDjBjxgxiYmJo1qyZw1XKsGHD2LBhA5cvX6ZXr15MmDDB5XndxdmFBOBwIZHcZtz27dsdrdHWrVs7LHNv376dVq1a4e3tjbe3NyEhIfzwww/07t2bt99+m4ULF5Ivn9V5lGjsVUSIiIhAVYmMjKR06dIOo6vdunXj2WefpV+/ftd0XXmVPK+MIiIiGDduHG+++SYJCQlcuXKFe++919iRywKMCwkLT7qQeO+997jjjjtcxuV2FxIhISFMmDCBJ554gkuXLvHLL784bNPt3buXTz/9lC+//JJy5coxY8YMfH19eeSRR+jSpQuVKlUiIiKCTz/91KGwjAuJayPPKiNVZenSpYwcOZIjR46QL18+Hn30USZOnJhnFFFGWjDXE+NCIimeciHxyy+/8N577/H777+7jM/tLiRuv/12NmzYwE033US5cuVo0aKFo5WTaH1748aNLFmyhMGDB/Pbb7+xfPly6tevz88//8zevXtp164dLVu2pHjx4saFxDWSJycwnD59mi5dutCjRw+OHDlC48aN2bBhA9OnTzdmT7KARBcSBw8eJCYmxtGaCAgIYOPGjUnSunIhkR6pKbX/6kKiR48ewFUXEomWpI8ePZqi0nFVQV4rzi4kwsLCqFChQhIXEkFBQTz77LNMnDgRb29v1q9fT8+ePVm6dCkdOnRwmefWrVu5//77+eqrr1KdlJPoQgJI4kJiy5YtNGjQIE0XEolltGvXLsaPH+9wIbFy5Uq2bt1Kp06dUnUhUb9+/RTbyJEjU6StUqUKhw8fdoSPHDnicAnhTKILic2bNzNp0iQAh8J/7rnnCAsL46effkJV8fX1deSdqGC7d+/u6Ep+//336dGjByLCjTfeSM2aNdm5c6fjXMaFxH8nTyqjYsWKsWfPHooXL87MmTP5888/adiwoafFynMYFxIWWe1C4tChQ/To0YOPPvqIOnXqpCpXbnchER8fz5kzZwBLOW/dutUxXtmtWzeHH6hVq1Y5yqlatWqsXLkSsMb6du3aRa1atQDjQuKa8fQMioxu/3U23e+//66nT592hMPCwvTYsWP/Ka+cTHabTadqXEhktQuJIUOGaMmSJTUkJERDQkK0UaNGLuXK7S4kLl++rP7+/urv76/NmjXTzZs3O44/d+6cduzYUQMDA7V58+YaFhamqqpHjx7Vdu3aaWBgoAYEBOhHH33kOMa4kLi2zeMCZHTLqDI6ffq03n///Qok8UmSV8kOysiQMzAuJDKGcSFxbVuu7aZTVebPn4+fnx/vvvsu+fPnp1KlSpYGNhgM6eLsQsKQNsaFxLWTK2fT7dy5k6FDh7Jq1SoAQkNDefvttx3rUwwGg3sYFxLuYVxIXDu5ThkdOXKEkJAQYmJiKFu2LG+88Qb9+/fPM9O13UE19SnUBoMhZ5NTe39ynTKqUqUK/fv3J1++fLzyyiuO1dkGi4IFC3LmzBnKlCljFJLBkMtQVc6cOZMjp5dLTtOi5arX11MHwxzh8PBwHn/8cYYOHUpoaChgTflNXBVtSEpsbCxHjhxxucbDYDDkfAoWLEiVKlVSTDEXkU2q2thDYqVLjm0ZxcfH8/bbb/Pcc89x8eJF9uzZw4YNGxARo4jSIH/+/NSsWdPTYhgMBkMSMrXWFpEOIrJLRPaIyGgX8SIiM+z4rSLi1srTv/76i+bNmzNixAguXrzInXfeyRdffGG6nQwGgyGHkmktIxHxAmYB7YAjwAYRWaaq252S3QH42lsz4G37N1Uizx6lSZMmJCQkUKVKFd566y26du1qFJHBYDDkYDKzZdQU2KOq+1Q1BlgEdE2Wpivwob0m60+gpIhUTCvTK5fOISI88cQT7Nixg27duhlFZDAYDDmczBwzqgwcdgofIWWrx1WaykC4cyIReRBIdKxyJR62TZ061aVdsjxGWeC0p4XIJpiyuIopi6uYsrhKtl6Rm5nKyFVzJfnUPXfSoKrvAO8AiMjG7DwjJCsxZXEVUxZXMWVxFVMWVxGRjemn8hyZ2U13BKjqFK4CHPsPaQwGg8GQy8lMZbQB8BWRmiJSALgbWJYszTLgPntWXXPggqqGJ8/IYDAYDLmbTOumU9U4EXkEWA54AfNU9R8RGWrHzwG+AzoCe4BLwCA3sn4nk0TOiZiyuIopi6uYsriKKYurZOuyyHEWGAwGg8GQ+zCmCgwGg8HgcYwyMhgMBoPHybbKKLNMCeVE3CiLfnYZbBWRtSIS4gk5s4L0ysIpXRMRiReRXlkpX1biTlmISKiIhInIPyKyKqtlzCrceEdKiMjXIrLFLgt3xqdzHCIyT0ROisi2VOKzb73paVezrjasCQ97gVpAAWALUC9Zmo7A91hrlZoD6zwttwfL4iaglP3/jrxcFk7pfsaaINPL03J78LkoCWwHqtnh8p6W24NlMQaYYv8vB5wFCnha9kwoi1uBhsC2VOKzbb2ZXVtGmWJKKIeSblmo6lpVPWcH/8Rar5Ubcee5ABgBfAGczErhshh3yuIeYImqHgJQ1dxaHu6UhQLFxLIdVhRLGcVlrZiZj6quxrq21Mi29WZ2VUapmQnKaJrcQEavcwjWl09uJN2yEJHKQHdgThbK5QnceS7qAKVE5FcR2SQiudUvtjtlMRPwx1pU/zfwqKomZI142YpsW29mV39G182UUC7A7esUkdZYyuiWTJXIc7hTFtOBUaoan8sN6LpTFt5AI6ANUAj4Q0T+VNXdmS1cFuNOWbQHwoDbgNrATyLym6pezGTZshvZtt7MrsrImBK6ilvXKSLBwLvAHap6Jotky2rcKYvGwCJbEZUFOopInKouzRIJsw5335HTqhoFRInIaiAEyG3KyJ2yGAS8otbAyR4R2Q/4AeuzRsRsQ7atN7NrN50xJXSVdMtCRKoBS4D+ufCr15l0y0JVa6pqDVWtAXwODM+Figjce0e+AlqKiLeIFMaymr8ji+XMCtwpi0NYLUREpAKWBet9WSpl9iDb1pvZsmWkmWdKKMfhZlmMBcoAs+0WQZzmQkvFbpZFnsCdslDVHSLyA7AVSADeVVWXU35zMm4+Fy8CH4jI31hdVaNUNde5lhCRT4BQoKyIHAHGAfkh+9ebxhyQwWAwGDxOdu2mMxgMBkMewigjg8FgMHgco4wMBoPB4HGMMjIYDAaDxzHKyGAwGAwexygjQ7bEtrgd5rTVSCNt5HU43wcist8+118i0uI/5PGuiNSz/49JFrf2WmW080ksl222FeqS6aSvLyIdr8e5DYbMxEztNmRLRCRSVYte77Rp5PEB8I2qfi4itwOvq2rwNeR3zTKll6+IzAd2q+qkNNIPBBqr6iPXWxaD4XpiWkaGHIGIFBWRlXar5W8RSWGtW0Qqishqp5ZDS3v/7SLyh33sYhFJT0msBm60j33CzmubiDxm7ysiIt/avnG2iUgfe/+vItJYRF4BCtlyLLDjIu3fT51bKnaLrKeIeInIayKyQSw/Mw+5USx/YBu5FJGmYvmy2mz/1rWtEUwE+tiy9LFln2efZ7OrcjQYPIKnfViYzWyuNiAey7BlGPAllrWQ4nZcWawV5Ikt+0j790ngOfu/F1DMTrsaKGLvHwWMdXG+D7B9HwF3AeuwjIz+DRTBcjvwD9AA6An8z+nYEvbvr1itEIdMTmkSZewOzLf/F8CyoFwIeBB43t7vA2wEarqQM9Lp+hYDHexwccDb/t8W+ML+PxCY6XT8y8C99v+SWHbqinj6fpvNbNnSHJDBAFxW1fqJARHJD7wsIrdimbapDFQAjjsdswGYZ6ddqqphItIKqAessU0lFcBqUbjiNRF5HjiFZf28DfClWoZGEZElQEvgB+B1EZmC1bX3Wwau63tghoj4AB2A1ap62e4aDJarnmlLAL7A/mTHFxKRMKAGsAn4ySn9fBHxxbLCnD+V898OdBGRp+xwQaAaudNmnSEHYZSRIafQD8tDZyNVjRWRA1gVqQNVXW0rq07ARyLyGnAO+ElV+7pxjqdV9fPEgIi0dZVIVXeLSCMsG1+TReRHVZ3ozkWoarSI/Irl0qAP8Eni6YARqro8nSwuq2p9ESkBfAM8DMzAsr32i6p2tyd7/JrK8QL0VNVd7shrMGQVZszIkFMoAZy0FVFroHryBCJS3U7zP+A9LPfLfwI3i0jiGFBhEanj5jlXA93sY4pgdbH9JiKVgEuq+jHwun2e5MTaLTRXLMIyUNkSy7gn9u+wxGNEpI59Tpeo6gVgJPCUfUwJ4KgdPdApaQRWd2Uiy4ERYjcTRaRBaucwGLISo4wMOYUFQGMR2YjVStrpIk0oECYim7HGdd5U1VNYlfMnIrIVSzn5uXNCVf0LayxpPdYY0ruquhkIAtbb3WXPAS+5OPwdYGviBIZk/AjcCqxQy002WL6otgN/icg2YC7p9FzYsmzBcpnwKlYrbQ3WeFIivwD1EicwYLWg8tuybbPDBoPHMVO7DQaDweBxTMvIYDAYDB7HKCODwWAweByjjAwGg8HgcYwyMhgMBoPHMcrIYDAYDB7HKCODwWAweByjjAwGg8Hgcf4P/Dij5V2m1UcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 计算每一类的ROC\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes): # 遍历三个类别\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_label[:, i], LSTM_test_proba[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area（方法二）\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test_label.ravel(), LSTM_test_proba.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# Compute macro-average ROC curve and ROC area（方法一）\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "lw=2\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.4f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='ROC curve of class {0} (area = {1:0.4f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Some extension of Receiver operating characteristic to multi-class')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
