{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, GroupKFold\n",
    "from sklearn.metrics import accuracy_score,auc,roc_curve\n",
    "from sklearn.preprocessing import LabelBinarizer,StandardScaler\n",
    "\n",
    "from sklearn import model_selection as cv\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Administrator\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "from dbn.tensorflow import SupervisedDBNClassification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input,Dense,LSTM,GRU,BatchNormalization\n",
    "from tensorflow.keras.layers import PReLU\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras as K\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import mean_absolute_error as MAE\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./dataset/joint/2.storke_combine_code_multi_balance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.array(df.pop('NIHSS'))\n",
    "X = np.array(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, stratify=Y,random_state = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Class = [0,1,2]\n",
    "Class_dict = dict(zip(Class, range(len(Class))))\n",
    "Class_dict\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(list(Class_dict.values()))\n",
    "y_train_labels = lb.transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_lstm = x_train.reshape(x_train.shape[0],1,x_train.shape[1])\n",
    "x_test_lstm = x_test.reshape(x_test.shape[0],1,x_test.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Predict(X):\n",
    "    RNN_test_label = []\n",
    "    Class = [0,1,2]\n",
    "    Class_dict = dict(zip(Class, range(len(Class))))\n",
    "    Class_dict\n",
    "    for i in range(0,X.shape[0]):\n",
    "        RNN_test_label.append(Class_dict[np.argmax(X[i])])\n",
    "    RNN_test_label = np.array(RNN_test_label,dtype = 'int64')\n",
    "    return RNN_test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 10)                910       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                110       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 3)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,053\n",
      "Trainable params: 1,053\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#DNN\n",
    "def buildDNN(layer1,layer2,n_class):\n",
    "    init = K.initializers.glorot_uniform(seed=1)\n",
    "    simple_adam = tf.keras.optimizers.Adam()\n",
    "    model = K.models.Sequential()\n",
    "    model.add(K.layers.Dense(units=layer1, input_dim=90, kernel_initializer=init, activation='relu'))\n",
    "    model.add(K.layers.Dense(units=layer2, kernel_initializer=init, activation='relu'))\n",
    "    model.add(K.layers.Dense(units=n_class, kernel_initializer=init, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=simple_adam, metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "dnn = buildDNN(layer1=10,layer2=10,n_class=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DBN\n",
    "def buildDBN(layer1,layer2,epoca,K=500):\n",
    "    \n",
    "    # cantidad de neuronas ocultas \n",
    "    hidden_layers = []\n",
    "    hidden_layers.append( int(layer1))\n",
    "    hidden_layers.append( int(layer2))\n",
    "\n",
    "    DBN_classifier = SupervisedDBNClassification(hidden_layers_structure = hidden_layers,\n",
    "                                                    learning_rate_rbm=0.05,\n",
    "                                                    learning_rate=0.1,\n",
    "                                                    n_epochs_rbm=epoca,\n",
    "                                                    n_iter_backprop=K,\n",
    "                                                    batch_size=32,\n",
    "                                                    activation_function='relu',\n",
    "                                                    dropout_p=0.2)\n",
    "    return DBN_classifier\n",
    "\n",
    "dbn = buildDBN(layer1=10,layer2=10,epoca=20,K=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1, 90)]           0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 10)                4040      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                110       \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 3)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,183\n",
      "Trainable params: 4,183\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#LSTM_RNN\n",
    "def buildLSTM(timeStep,inputColNum,outStep,learnRate=1e-4):\n",
    "    inputLayer = Input(shape=(timeStep,inputColNum))\n",
    "    middle = LSTM(10,activation='tanh')(inputLayer)\n",
    "    middle = Dense(10,activation='tanh')(middle)\n",
    "    outputLayer = Dense(outStep)(middle)\n",
    "    model = Model(inputs=inputLayer,outputs=outputLayer)\n",
    "    optimizer = Adam(learning_rate=learnRate)\n",
    "    model.compile(optimizer=optimizer,loss='mse') \n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "lstm = buildLSTM(timeStep=1,inputColNum=90,outStep=3,learnRate=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "_N_FOLDS = 5  # 采用5折交叉验证\n",
    "kf = StratifiedKFold(n_splits=5, random_state=2020, shuffle=True)  # sklearn的交叉验证模块，用于划分数据\n",
    "_N_CLASS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 350 samples\n",
      "Epoch 1/100\n",
      "350/350 [==============================] - 0s 222us/sample - loss: 1.0624 - acc: 0.4771\n",
      "Epoch 2/100\n",
      "350/350 [==============================] - 0s 27us/sample - loss: 0.9941 - acc: 0.5857\n",
      "Epoch 3/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.9304 - acc: 0.6657\n",
      "Epoch 4/100\n",
      "350/350 [==============================] - 0s 31us/sample - loss: 0.8835 - acc: 0.7086\n",
      "Epoch 5/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.8361 - acc: 0.7343\n",
      "Epoch 6/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.7866 - acc: 0.7514\n",
      "Epoch 7/100\n",
      "350/350 [==============================] - 0s 31us/sample - loss: 0.7378 - acc: 0.7629\n",
      "Epoch 8/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.6906 - acc: 0.7629\n",
      "Epoch 9/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.6443 - acc: 0.7714\n",
      "Epoch 10/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.5950 - acc: 0.7857\n",
      "Epoch 11/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.5455 - acc: 0.7943\n",
      "Epoch 12/100\n",
      "350/350 [==============================] - 0s 40us/sample - loss: 0.4983 - acc: 0.7971\n",
      "Epoch 13/100\n",
      "350/350 [==============================] - 0s 34us/sample - loss: 0.4599 - acc: 0.8400\n",
      "Epoch 14/100\n",
      "350/350 [==============================] - 0s 31us/sample - loss: 0.4218 - acc: 0.8371\n",
      "Epoch 15/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.3962 - acc: 0.8829\n",
      "Epoch 16/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.3669 - acc: 0.8714\n",
      "Epoch 17/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.3433 - acc: 0.8857\n",
      "Epoch 18/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.3238 - acc: 0.8914\n",
      "Epoch 19/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.3065 - acc: 0.8943\n",
      "Epoch 20/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.2847 - acc: 0.9257\n",
      "Epoch 21/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.2727 - acc: 0.9114\n",
      "Epoch 22/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.2572 - acc: 0.9200\n",
      "Epoch 23/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.2469 - acc: 0.9286\n",
      "Epoch 24/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.2348 - acc: 0.9200\n",
      "Epoch 25/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.2251 - acc: 0.9314\n",
      "Epoch 26/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.2172 - acc: 0.9314\n",
      "Epoch 27/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.2081 - acc: 0.9457\n",
      "Epoch 28/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.2027 - acc: 0.9543\n",
      "Epoch 29/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.1972 - acc: 0.9371\n",
      "Epoch 30/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.1859 - acc: 0.9514\n",
      "Epoch 31/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.1801 - acc: 0.9571\n",
      "Epoch 32/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.1752 - acc: 0.9514\n",
      "Epoch 33/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.1672 - acc: 0.9571\n",
      "Epoch 34/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.1608 - acc: 0.9571\n",
      "Epoch 35/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.1592 - acc: 0.9600\n",
      "Epoch 36/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.1527 - acc: 0.9571\n",
      "Epoch 37/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.1502 - acc: 0.9543\n",
      "Epoch 38/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.1446 - acc: 0.9600\n",
      "Epoch 39/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.1402 - acc: 0.9600\n",
      "Epoch 40/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.1370 - acc: 0.9686\n",
      "Epoch 41/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.1322 - acc: 0.9657\n",
      "Epoch 42/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.1263 - acc: 0.9686\n",
      "Epoch 43/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.1217 - acc: 0.9714\n",
      "Epoch 44/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.1200 - acc: 0.9657\n",
      "Epoch 45/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.1199 - acc: 0.9686\n",
      "Epoch 46/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.1140 - acc: 0.9686\n",
      "Epoch 47/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.1106 - acc: 0.9771\n",
      "Epoch 48/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.1061 - acc: 0.9743\n",
      "Epoch 49/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.1063 - acc: 0.9743\n",
      "Epoch 50/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.1055 - acc: 0.9743\n",
      "Epoch 51/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.1046 - acc: 0.9714\n",
      "Epoch 52/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0998 - acc: 0.9714\n",
      "Epoch 53/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0935 - acc: 0.9800\n",
      "Epoch 54/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0911 - acc: 0.9800\n",
      "Epoch 55/100\n",
      "350/350 [==============================] - 0s 34us/sample - loss: 0.0912 - acc: 0.9771\n",
      "Epoch 56/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0882 - acc: 0.9829\n",
      "Epoch 57/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0854 - acc: 0.9771\n",
      "Epoch 58/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0859 - acc: 0.9829\n",
      "Epoch 59/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0815 - acc: 0.9857\n",
      "Epoch 60/100\n",
      "350/350 [==============================] - 0s 34us/sample - loss: 0.0806 - acc: 0.9800\n",
      "Epoch 61/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0773 - acc: 0.9886\n",
      "Epoch 62/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0786 - acc: 0.9771\n",
      "Epoch 63/100\n",
      "350/350 [==============================] - 0s 31us/sample - loss: 0.0758 - acc: 0.9886\n",
      "Epoch 64/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0720 - acc: 0.9914\n",
      "Epoch 65/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0712 - acc: 0.9829\n",
      "Epoch 66/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0702 - acc: 0.9886\n",
      "Epoch 67/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0705 - acc: 0.9857\n",
      "Epoch 68/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0671 - acc: 0.9886\n",
      "Epoch 69/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0657 - acc: 0.9857\n",
      "Epoch 70/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0643 - acc: 0.9914\n",
      "Epoch 71/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0625 - acc: 0.9914\n",
      "Epoch 72/100\n",
      "350/350 [==============================] - 0s 29us/sample - loss: 0.0607 - acc: 0.9914\n",
      "Epoch 73/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0610 - acc: 0.9914\n",
      "Epoch 74/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0602 - acc: 0.9914\n",
      "Epoch 75/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0611 - acc: 0.9829\n",
      "Epoch 76/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0574 - acc: 0.9943\n",
      "Epoch 77/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0547 - acc: 0.9914\n",
      "Epoch 78/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0562 - acc: 0.9914\n",
      "Epoch 79/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0558 - acc: 0.9800\n",
      "Epoch 80/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0522 - acc: 0.9943\n",
      "Epoch 81/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0512 - acc: 0.9943\n",
      "Epoch 82/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0504 - acc: 0.9943\n",
      "Epoch 83/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0487 - acc: 0.9943\n",
      "Epoch 84/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0486 - acc: 0.9943\n",
      "Epoch 85/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0486 - acc: 0.9943\n",
      "Epoch 86/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0472 - acc: 0.9943\n",
      "Epoch 87/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0464 - acc: 0.9943\n",
      "Epoch 88/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0452 - acc: 0.9943\n",
      "Epoch 89/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0443 - acc: 0.9943\n",
      "Epoch 90/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0441 - acc: 0.9943\n",
      "Epoch 91/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0424 - acc: 0.9943\n",
      "Epoch 92/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0414 - acc: 0.9943\n",
      "Epoch 93/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0407 - acc: 0.9943\n",
      "Epoch 94/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0407 - acc: 0.9943\n",
      "Epoch 95/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0397 - acc: 0.9943\n",
      "Epoch 96/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0381 - acc: 0.9943\n",
      "Epoch 97/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0393 - acc: 0.9971\n",
      "Epoch 98/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0402 - acc: 0.9943\n",
      "Epoch 99/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0379 - acc: 0.9971\n",
      "Epoch 100/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0367 - acc: 0.9971\n",
      "Train on 350 samples\n",
      "Epoch 1/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0979 - acc: 0.9714\n",
      "Epoch 2/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0894 - acc: 0.9686\n",
      "Epoch 3/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0784 - acc: 0.9743\n",
      "Epoch 4/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0700 - acc: 0.9800\n",
      "Epoch 5/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0645 - acc: 0.9800\n",
      "Epoch 6/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0684 - acc: 0.9857\n",
      "Epoch 7/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0588 - acc: 0.9829\n",
      "Epoch 8/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0546 - acc: 0.9857\n",
      "Epoch 9/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0530 - acc: 0.9886\n",
      "Epoch 10/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0487 - acc: 0.9914\n",
      "Epoch 11/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0483 - acc: 0.9886\n",
      "Epoch 12/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0475 - acc: 0.9829\n",
      "Epoch 13/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0456 - acc: 0.9914\n",
      "Epoch 14/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0440 - acc: 0.9886\n",
      "Epoch 15/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0407 - acc: 0.9886\n",
      "Epoch 16/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0415 - acc: 0.9914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0398 - acc: 0.9943\n",
      "Epoch 18/100\n",
      "350/350 [==============================] - 0s 31us/sample - loss: 0.0399 - acc: 0.9886\n",
      "Epoch 19/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0394 - acc: 0.9886\n",
      "Epoch 20/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0395 - acc: 0.9914\n",
      "Epoch 21/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0352 - acc: 0.9971\n",
      "Epoch 22/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0346 - acc: 0.9971\n",
      "Epoch 23/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0356 - acc: 0.9971\n",
      "Epoch 24/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0327 - acc: 0.9943\n",
      "Epoch 25/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0341 - acc: 0.9971\n",
      "Epoch 26/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0316 - acc: 0.9943\n",
      "Epoch 27/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0294 - acc: 0.9971\n",
      "Epoch 28/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0317 - acc: 0.9971\n",
      "Epoch 29/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0295 - acc: 0.9971\n",
      "Epoch 30/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0276 - acc: 1.0000\n",
      "Epoch 31/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0278 - acc: 0.9971\n",
      "Epoch 32/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0264 - acc: 1.0000\n",
      "Epoch 33/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0264 - acc: 0.9971\n",
      "Epoch 34/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0254 - acc: 1.0000\n",
      "Epoch 35/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0256 - acc: 0.9971\n",
      "Epoch 36/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0249 - acc: 1.0000\n",
      "Epoch 37/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0252 - acc: 1.0000\n",
      "Epoch 38/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0244 - acc: 1.0000\n",
      "Epoch 39/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0227 - acc: 0.9971\n",
      "Epoch 40/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0226 - acc: 0.9971\n",
      "Epoch 41/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0217 - acc: 1.0000\n",
      "Epoch 42/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0218 - acc: 1.0000\n",
      "Epoch 43/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0218 - acc: 0.9971\n",
      "Epoch 44/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0201 - acc: 1.0000\n",
      "Epoch 45/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0199 - acc: 1.0000\n",
      "Epoch 46/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0197 - acc: 1.0000\n",
      "Epoch 47/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0191 - acc: 1.0000\n",
      "Epoch 48/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0190 - acc: 1.0000\n",
      "Epoch 49/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0192 - acc: 1.0000\n",
      "Epoch 50/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0178 - acc: 1.0000\n",
      "Epoch 51/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0177 - acc: 1.0000\n",
      "Epoch 52/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0181 - acc: 1.0000\n",
      "Epoch 53/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0183 - acc: 1.0000\n",
      "Epoch 54/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0167 - acc: 1.0000\n",
      "Epoch 55/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0165 - acc: 1.0000\n",
      "Epoch 56/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0160 - acc: 1.0000\n",
      "Epoch 57/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0162 - acc: 1.0000\n",
      "Epoch 58/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0154 - acc: 1.0000\n",
      "Epoch 59/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0156 - acc: 1.0000\n",
      "Epoch 60/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0145 - acc: 1.0000\n",
      "Epoch 61/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0143 - acc: 1.0000\n",
      "Epoch 62/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0144 - acc: 1.0000\n",
      "Epoch 63/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0144 - acc: 1.0000\n",
      "Epoch 64/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0146 - acc: 1.0000\n",
      "Epoch 65/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0137 - acc: 1.0000\n",
      "Epoch 66/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0134 - acc: 1.0000\n",
      "Epoch 67/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0130 - acc: 1.0000\n",
      "Epoch 68/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0129 - acc: 1.0000\n",
      "Epoch 69/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0150 - acc: 1.0000\n",
      "Epoch 70/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0131 - acc: 1.0000\n",
      "Epoch 71/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0120 - acc: 1.0000\n",
      "Epoch 72/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0122 - acc: 1.0000\n",
      "Epoch 73/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0119 - acc: 1.0000\n",
      "Epoch 74/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0116 - acc: 1.0000\n",
      "Epoch 75/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0109 - acc: 1.0000\n",
      "Epoch 76/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0124 - acc: 1.0000\n",
      "Epoch 77/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0117 - acc: 1.0000\n",
      "Epoch 78/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0117 - acc: 1.0000\n",
      "Epoch 79/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0127 - acc: 1.0000\n",
      "Epoch 80/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0124 - acc: 1.0000\n",
      "Epoch 81/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0103 - acc: 1.0000\n",
      "Epoch 82/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0099 - acc: 1.0000\n",
      "Epoch 83/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0095 - acc: 1.0000\n",
      "Epoch 84/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0092 - acc: 1.0000\n",
      "Epoch 85/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0090 - acc: 1.0000\n",
      "Epoch 86/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0094 - acc: 1.0000\n",
      "Epoch 87/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0091 - acc: 1.0000\n",
      "Epoch 88/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0099 - acc: 1.0000\n",
      "Epoch 89/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0091 - acc: 1.0000\n",
      "Epoch 90/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0091 - acc: 1.0000\n",
      "Epoch 91/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0082 - acc: 1.0000\n",
      "Epoch 92/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0081 - acc: 1.0000\n",
      "Epoch 93/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0081 - acc: 1.0000\n",
      "Epoch 94/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0081 - acc: 1.0000\n",
      "Epoch 95/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0084 - acc: 1.0000\n",
      "Epoch 96/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0080 - acc: 1.0000\n",
      "Epoch 97/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0077 - acc: 1.0000\n",
      "Epoch 98/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0073 - acc: 1.0000\n",
      "Epoch 99/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0074 - acc: 1.0000\n",
      "Epoch 100/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0072 - acc: 1.0000\n",
      "Train on 350 samples\n",
      "Epoch 1/100\n",
      "350/350 [==============================] - 0s 31us/sample - loss: 0.0326 - acc: 0.9914\n",
      "Epoch 2/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0253 - acc: 0.9914\n",
      "Epoch 3/100\n",
      "350/350 [==============================] - 0s 60us/sample - loss: 0.0185 - acc: 0.9943\n",
      "Epoch 4/100\n",
      "350/350 [==============================] - 0s 46us/sample - loss: 0.0174 - acc: 0.9943\n",
      "Epoch 5/100\n",
      "350/350 [==============================] - 0s 34us/sample - loss: 0.0142 - acc: 1.0000\n",
      "Epoch 6/100\n",
      "350/350 [==============================] - 0s 30us/sample - loss: 0.0135 - acc: 1.0000\n",
      "Epoch 7/100\n",
      "350/350 [==============================] - 0s 0s/sample - loss: 0.0128 - acc: 1.0000\n",
      "Epoch 8/100\n",
      "350/350 [==============================] - 0s 85us/sample - loss: 0.0113 - acc: 1.0000\n",
      "Epoch 9/100\n",
      "350/350 [==============================] - 0s 33us/sample - loss: 0.0121 - acc: 1.0000\n",
      "Epoch 10/100\n",
      "350/350 [==============================] - 0s 34us/sample - loss: 0.0114 - acc: 1.0000\n",
      "Epoch 11/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0104 - acc: 1.0000\n",
      "Epoch 12/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0106 - acc: 1.0000\n",
      "Epoch 13/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0100 - acc: 1.0000\n",
      "Epoch 14/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0098 - acc: 1.0000\n",
      "Epoch 15/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0096 - acc: 1.0000\n",
      "Epoch 16/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0091 - acc: 1.0000\n",
      "Epoch 17/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0087 - acc: 1.0000\n",
      "Epoch 18/100\n",
      "350/350 [==============================] - 0s 13us/sample - loss: 0.0082 - acc: 1.0000\n",
      "Epoch 19/100\n",
      "350/350 [==============================] - 0s 45us/sample - loss: 0.0081 - acc: 1.0000\n",
      "Epoch 20/100\n",
      "350/350 [==============================] - 0s 31us/sample - loss: 0.0082 - acc: 1.0000\n",
      "Epoch 21/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0084 - acc: 1.0000\n",
      "Epoch 22/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0095 - acc: 1.0000\n",
      "Epoch 23/100\n",
      "350/350 [==============================] - 0s 0s/sample - loss: 0.0075 - acc: 1.0000\n",
      "Epoch 24/100\n",
      "350/350 [==============================] - 0s 45us/sample - loss: 0.0072 - acc: 1.0000\n",
      "Epoch 25/100\n",
      "350/350 [==============================] - 0s 0s/sample - loss: 0.0070 - acc: 1.0000\n",
      "Epoch 26/100\n",
      "350/350 [==============================] - 0s 0s/sample - loss: 0.0069 - acc: 1.0000\n",
      "Epoch 27/100\n",
      "350/350 [==============================] - 0s 45us/sample - loss: 0.0065 - acc: 1.0000\n",
      "Epoch 28/100\n",
      "350/350 [==============================] - 0s 0s/sample - loss: 0.0065 - acc: 1.0000\n",
      "Epoch 29/100\n",
      "350/350 [==============================] - 0s 45us/sample - loss: 0.0062 - acc: 1.0000\n",
      "Epoch 30/100\n",
      "350/350 [==============================] - 0s 45us/sample - loss: 0.0063 - acc: 1.0000\n",
      "Epoch 31/100\n",
      "350/350 [==============================] - 0s 0s/sample - loss: 0.0061 - acc: 1.0000\n",
      "Epoch 32/100\n",
      "350/350 [==============================] - 0s 45us/sample - loss: 0.0059 - acc: 1.0000\n",
      "Epoch 33/100\n",
      "350/350 [==============================] - 0s 45us/sample - loss: 0.0057 - acc: 1.0000\n",
      "Epoch 34/100\n",
      "350/350 [==============================] - 0s 0s/sample - loss: 0.0058 - acc: 1.0000\n",
      "Epoch 35/100\n",
      "350/350 [==============================] - 0s 45us/sample - loss: 0.0056 - acc: 1.0000\n",
      "Epoch 36/100\n",
      "350/350 [==============================] - 0s 45us/sample - loss: 0.0055 - acc: 1.0000\n",
      "Epoch 37/100\n",
      "350/350 [==============================] - 0s 0s/sample - loss: 0.0055 - acc: 1.0000\n",
      "Epoch 38/100\n",
      "350/350 [==============================] - 0s 45us/sample - loss: 0.0053 - acc: 1.0000\n",
      "Epoch 39/100\n",
      "350/350 [==============================] - 0s 45us/sample - loss: 0.0051 - acc: 1.0000\n",
      "Epoch 40/100\n",
      "350/350 [==============================] - 0s 0s/sample - loss: 0.0052 - acc: 1.0000\n",
      "Epoch 41/100\n",
      "350/350 [==============================] - 0s 45us/sample - loss: 0.0049 - acc: 1.0000\n",
      "Epoch 42/100\n",
      "350/350 [==============================] - 0s 45us/sample - loss: 0.0048 - acc: 1.0000\n",
      "Epoch 43/100\n",
      "350/350 [==============================] - 0s 0s/sample - loss: 0.0047 - acc: 1.0000\n",
      "Epoch 44/100\n",
      "350/350 [==============================] - 0s 45us/sample - loss: 0.0046 - acc: 1.0000\n",
      "Epoch 45/100\n",
      "350/350 [==============================] - 0s 45us/sample - loss: 0.0046 - acc: 1.0000\n",
      "Epoch 46/100\n",
      "350/350 [==============================] - 0s 0s/sample - loss: 0.0045 - acc: 1.0000\n",
      "Epoch 47/100\n",
      "350/350 [==============================] - 0s 45us/sample - loss: 0.0044 - acc: 1.0000\n",
      "Epoch 48/100\n",
      "350/350 [==============================] - 0s 45us/sample - loss: 0.0043 - acc: 1.0000\n",
      "Epoch 49/100\n",
      "350/350 [==============================] - 0s 0s/sample - loss: 0.0042 - acc: 1.0000\n",
      "Epoch 50/100\n",
      "350/350 [==============================] - 0s 45us/sample - loss: 0.0043 - acc: 1.0000\n",
      "Epoch 51/100\n",
      "350/350 [==============================] - 0s 0s/sample - loss: 0.0044 - acc: 1.0000\n",
      "Epoch 52/100\n",
      "350/350 [==============================] - 0s 0s/sample - loss: 0.0039 - acc: 1.0000\n",
      "Epoch 53/100\n",
      "350/350 [==============================] - 0s 45us/sample - loss: 0.0041 - acc: 1.0000\n",
      "Epoch 54/100\n",
      "350/350 [==============================] - 0s 0s/sample - loss: 0.0042 - acc: 1.0000\n",
      "Epoch 55/100\n",
      "350/350 [==============================] - 0s 45us/sample - loss: 0.0041 - acc: 1.0000\n",
      "Epoch 56/100\n",
      "350/350 [==============================] - 0s 45us/sample - loss: 0.0037 - acc: 1.0000\n",
      "Epoch 57/100\n",
      "350/350 [==============================] - 0s 0s/sample - loss: 0.0036 - acc: 1.0000\n",
      "Epoch 58/100\n",
      "350/350 [==============================] - 0s 45us/sample - loss: 0.0035 - acc: 1.0000\n",
      "Epoch 59/100\n",
      "350/350 [==============================] - 0s 45us/sample - loss: 0.0034 - acc: 1.0000\n",
      "Epoch 60/100\n",
      "350/350 [==============================] - 0s 0s/sample - loss: 0.0034 - acc: 1.0000\n",
      "Epoch 61/100\n",
      "350/350 [==============================] - 0s 45us/sample - loss: 0.0034 - acc: 1.0000\n",
      "Epoch 62/100\n",
      "350/350 [==============================] - 0s 45us/sample - loss: 0.0032 - acc: 1.0000\n",
      "Epoch 63/100\n",
      "350/350 [==============================] - 0s 0s/sample - loss: 0.0032 - acc: 1.0000\n",
      "Epoch 64/100\n",
      "350/350 [==============================] - 0s 45us/sample - loss: 0.0031 - acc: 1.0000\n",
      "Epoch 65/100\n",
      "350/350 [==============================] - 0s 45us/sample - loss: 0.0031 - acc: 1.0000\n",
      "Epoch 66/100\n",
      "350/350 [==============================] - 0s 0s/sample - loss: 0.0030 - acc: 1.0000\n",
      "Epoch 67/100\n",
      "350/350 [==============================] - 0s 45us/sample - loss: 0.0030 - acc: 1.0000\n",
      "Epoch 68/100\n",
      "350/350 [==============================] - 0s 45us/sample - loss: 0.0030 - acc: 1.0000\n",
      "Epoch 69/100\n",
      "350/350 [==============================] - 0s 0s/sample - loss: 0.0029 - acc: 1.0000\n",
      "Epoch 70/100\n",
      "350/350 [==============================] - 0s 45us/sample - loss: 0.0028 - acc: 1.0000\n",
      "Epoch 71/100\n",
      "350/350 [==============================] - 0s 45us/sample - loss: 0.0028 - acc: 1.0000\n",
      "Epoch 72/100\n",
      "350/350 [==============================] - 0s 0s/sample - loss: 0.0029 - acc: 1.0000\n",
      "Epoch 73/100\n",
      "350/350 [==============================] - 0s 45us/sample - loss: 0.0028 - acc: 1.0000\n",
      "Epoch 74/100\n",
      "350/350 [==============================] - 0s 45us/sample - loss: 0.0027 - acc: 1.0000\n",
      "Epoch 75/100\n",
      "350/350 [==============================] - 0s 0s/sample - loss: 0.0026 - acc: 1.0000\n",
      "Epoch 76/100\n",
      "350/350 [==============================] - 0s 45us/sample - loss: 0.0027 - acc: 1.0000\n",
      "Epoch 77/100\n",
      "350/350 [==============================] - 0s 45us/sample - loss: 0.0026 - acc: 1.0000\n",
      "Epoch 78/100\n",
      "350/350 [==============================] - 0s 0s/sample - loss: 0.0024 - acc: 1.0000\n",
      "Epoch 79/100\n",
      "350/350 [==============================] - 0s 45us/sample - loss: 0.0024 - acc: 1.0000\n",
      "Epoch 80/100\n",
      "350/350 [==============================] - 0s 45us/sample - loss: 0.0024 - acc: 1.0000\n",
      "Epoch 81/100\n",
      "350/350 [==============================] - 0s 0s/sample - loss: 0.0024 - acc: 1.0000\n",
      "Epoch 82/100\n",
      "350/350 [==============================] - 0s 45us/sample - loss: 0.0023 - acc: 1.0000\n",
      "Epoch 83/100\n",
      "350/350 [==============================] - 0s 45us/sample - loss: 0.0023 - acc: 1.0000\n",
      "Epoch 84/100\n",
      "350/350 [==============================] - 0s 29us/sample - loss: 0.0023 - acc: 1.0000\n",
      "Epoch 85/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0023 - acc: 1.0000\n",
      "Epoch 86/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0023 - acc: 1.0000\n",
      "Epoch 87/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0023 - acc: 1.0000\n",
      "Epoch 88/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0021 - acc: 1.0000\n",
      "Epoch 89/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0022 - acc: 1.0000\n",
      "Epoch 90/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0021 - acc: 1.0000\n",
      "Epoch 91/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0021 - acc: 1.0000\n",
      "Epoch 92/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 93/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0019 - acc: 1.0000\n",
      "Epoch 94/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0019 - acc: 1.0000\n",
      "Epoch 95/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 96/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0019 - acc: 1.0000\n",
      "Epoch 97/100\n",
      "350/350 [==============================] - 0s 28us/sample - loss: 0.0018 - acc: 1.0000\n",
      "Epoch 98/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0018 - acc: 1.0000\n",
      "Epoch 99/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0018 - acc: 1.0000\n",
      "Epoch 100/100\n",
      "350/350 [==============================] - 0s 26us/sample - loss: 0.0017 - acc: 1.0000\n",
      "Train on 351 samples\n",
      "Epoch 1/100\n",
      "351/351 [==============================] - 0s 29us/sample - loss: 0.0170 - acc: 0.9943\n",
      "Epoch 2/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 0.0087 - acc: 0.9972\n",
      "Epoch 3/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 0.0051 - acc: 1.0000\n",
      "Epoch 4/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 0.0036 - acc: 1.0000\n",
      "Epoch 5/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 0.0029 - acc: 1.0000\n",
      "Epoch 6/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 0.0027 - acc: 1.0000\n",
      "Epoch 7/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 0.0024 - acc: 1.0000\n",
      "Epoch 8/100\n",
      "351/351 [==============================] - 0s 22us/sample - loss: 0.0021 - acc: 1.0000\n",
      "Epoch 9/100\n",
      "351/351 [==============================] - 0s 0s/sample - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 10/100\n",
      "351/351 [==============================] - 0s 65us/sample - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 11/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 0.0021 - acc: 1.0000\n",
      "Epoch 12/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 13/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 14/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 0.0018 - acc: 1.0000\n",
      "Epoch 15/100\n",
      "351/351 [==============================] - 0s 7us/sample - loss: 0.0018 - acc: 1.0000\n",
      "Epoch 16/100\n",
      "351/351 [==============================] - 0s 47us/sample - loss: 0.0018 - acc: 1.0000\n",
      "Epoch 17/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 0.0018 - acc: 1.0000\n",
      "Epoch 18/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 0.0017 - acc: 1.0000\n",
      "Epoch 19/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 0.0016 - acc: 1.0000\n",
      "Epoch 20/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 0.0017 - acc: 1.0000\n",
      "Epoch 21/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 0.0016 - acc: 1.0000\n",
      "Epoch 22/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 0.0016 - acc: 1.0000\n",
      "Epoch 23/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 0.0016 - acc: 1.0000\n",
      "Epoch 24/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 0.0015 - acc: 1.0000\n",
      "Epoch 25/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 0.0015 - acc: 1.0000\n",
      "Epoch 26/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 0.0015 - acc: 1.0000\n",
      "Epoch 27/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 28/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 29/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 30/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 31/100\n",
      "351/351 [==============================] - 0s 37us/sample - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 32/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 33/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 34/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 35/100\n",
      "351/351 [==============================] - 0s 68us/sample - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 36/100\n",
      "351/351 [==============================] - 0s 37us/sample - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 37/100\n",
      "351/351 [==============================] - 0s 37us/sample - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 38/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 39/100\n",
      "351/351 [==============================] - 0s 31us/sample - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 40/100\n",
      "351/351 [==============================] - 0s 31us/sample - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 41/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 42/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 43/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 44/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 45/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 46/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 47/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 48/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 49/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 50/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 51/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 9.8544e-04 - acc: 1.0000\n",
      "Epoch 52/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 0.0010 - acc: 1.0000\n",
      "Epoch 53/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 9.5859e-04 - acc: 1.0000\n",
      "Epoch 54/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 0.0010 - acc: 1.0000\n",
      "Epoch 55/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 9.3940e-04 - acc: 1.0000\n",
      "Epoch 56/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 9.3347e-04 - acc: 1.0000\n",
      "Epoch 57/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 9.1815e-04 - acc: 1.0000\n",
      "Epoch 58/100\n",
      "351/351 [==============================] - 0s 31us/sample - loss: 9.1125e-04 - acc: 1.0000\n",
      "Epoch 59/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 9.1149e-04 - acc: 1.0000\n",
      "Epoch 60/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 8.9506e-04 - acc: 1.0000\n",
      "Epoch 61/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 8.7462e-04 - acc: 1.0000\n",
      "Epoch 62/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 8.7330e-04 - acc: 1.0000\n",
      "Epoch 63/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 8.6736e-04 - acc: 1.0000\n",
      "Epoch 64/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 8.4001e-04 - acc: 1.0000\n",
      "Epoch 65/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 8.3564e-04 - acc: 1.0000\n",
      "Epoch 66/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 8.4346e-04 - acc: 1.0000\n",
      "Epoch 67/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 8.2319e-04 - acc: 1.0000\n",
      "Epoch 68/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 8.0283e-04 - acc: 1.0000\n",
      "Epoch 69/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 8.0698e-04 - acc: 1.0000\n",
      "Epoch 70/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 8.0331e-04 - acc: 1.0000\n",
      "Epoch 71/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 7.6934e-04 - acc: 1.0000\n",
      "Epoch 72/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 8.1738e-04 - acc: 1.0000\n",
      "Epoch 73/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 7.6445e-04 - acc: 1.0000\n",
      "Epoch 74/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 7.4788e-04 - acc: 1.0000\n",
      "Epoch 75/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 7.4517e-04 - acc: 1.0000\n",
      "Epoch 76/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 7.3093e-04 - acc: 1.0000\n",
      "Epoch 77/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 7.1680e-04 - acc: 1.0000\n",
      "Epoch 78/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 7.0841e-04 - acc: 1.0000\n",
      "Epoch 79/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 7.0177e-04 - acc: 1.0000\n",
      "Epoch 80/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 6.9288e-04 - acc: 1.0000\n",
      "Epoch 81/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 6.9286e-04 - acc: 1.0000\n",
      "Epoch 82/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 6.7307e-04 - acc: 1.0000\n",
      "Epoch 83/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 7.0487e-04 - acc: 1.0000\n",
      "Epoch 84/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 7.0327e-04 - acc: 1.0000\n",
      "Epoch 85/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 6.3356e-04 - acc: 1.0000\n",
      "Epoch 86/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 6.7106e-04 - acc: 1.0000\n",
      "Epoch 87/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 6.4750e-04 - acc: 1.0000\n",
      "Epoch 88/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 6.3898e-04 - acc: 1.0000\n",
      "Epoch 89/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 6.2767e-04 - acc: 1.0000\n",
      "Epoch 90/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 6.1818e-04 - acc: 1.0000\n",
      "Epoch 91/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 6.2500e-04 - acc: 1.0000\n",
      "Epoch 92/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 5.9869e-04 - acc: 1.0000\n",
      "Epoch 93/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 5.8632e-04 - acc: 1.0000\n",
      "Epoch 94/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 5.8062e-04 - acc: 1.0000\n",
      "Epoch 95/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 5.6782e-04 - acc: 1.0000\n",
      "Epoch 96/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 5.6655e-04 - acc: 1.0000\n",
      "Epoch 97/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 5.7236e-04 - acc: 1.0000\n",
      "Epoch 98/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 5.6180e-04 - acc: 1.0000\n",
      "Epoch 99/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 5.6041e-04 - acc: 1.0000\n",
      "Epoch 100/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 5.4496e-04 - acc: 1.0000\n",
      "Train on 351 samples\n",
      "Epoch 1/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 0.0046 - acc: 1.0000\n",
      "Epoch 2/100\n",
      "351/351 [==============================] - 0s 31us/sample - loss: 0.0031 - acc: 1.0000\n",
      "Epoch 3/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 0.0054 - acc: 0.9972\n",
      "Epoch 4/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 5/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 6/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 7.5994e-04 - acc: 1.0000\n",
      "Epoch 7/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 7.6174e-04 - acc: 1.0000\n",
      "Epoch 8/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 7.1351e-04 - acc: 1.0000\n",
      "Epoch 9/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 7.1279e-04 - acc: 1.0000\n",
      "Epoch 10/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 6.8860e-04 - acc: 1.0000\n",
      "Epoch 11/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 6.6443e-04 - acc: 1.0000\n",
      "Epoch 12/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 6.3884e-04 - acc: 1.0000\n",
      "Epoch 13/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 6.4199e-04 - acc: 1.0000\n",
      "Epoch 14/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 6.2487e-04 - acc: 1.0000\n",
      "Epoch 15/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 6.1913e-04 - acc: 1.0000\n",
      "Epoch 16/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 5.9925e-04 - acc: 1.0000\n",
      "Epoch 17/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 5.9053e-04 - acc: 1.0000\n",
      "Epoch 18/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 5.8125e-04 - acc: 1.0000\n",
      "Epoch 19/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 5.5875e-04 - acc: 1.0000\n",
      "Epoch 20/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 5.5982e-04 - acc: 1.0000\n",
      "Epoch 21/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 5.4797e-04 - acc: 1.0000\n",
      "Epoch 22/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 5.3482e-04 - acc: 1.0000\n",
      "Epoch 23/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 5.3760e-04 - acc: 1.0000\n",
      "Epoch 24/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 5.1776e-04 - acc: 1.0000\n",
      "Epoch 25/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 5.1659e-04 - acc: 1.0000\n",
      "Epoch 26/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 5.0469e-04 - acc: 1.0000\n",
      "Epoch 27/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 5.0215e-04 - acc: 1.0000\n",
      "Epoch 28/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 4.8830e-04 - acc: 1.0000\n",
      "Epoch 29/100\n",
      "351/351 [==============================] - 0s 14us/sample - loss: 4.8485e-04 - acc: 1.0000\n",
      "Epoch 30/100\n",
      "351/351 [==============================] - 0s 45us/sample - loss: 4.9110e-04 - acc: 1.0000\n",
      "Epoch 31/100\n",
      "351/351 [==============================] - 0s 30us/sample - loss: 4.6901e-04 - acc: 1.0000\n",
      "Epoch 32/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 4.7024e-04 - acc: 1.0000\n",
      "Epoch 33/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 4.6883e-04 - acc: 1.0000\n",
      "Epoch 34/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 4.4356e-04 - acc: 1.0000\n",
      "Epoch 35/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 4.5328e-04 - acc: 1.0000\n",
      "Epoch 36/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 4.4079e-04 - acc: 1.0000\n",
      "Epoch 37/100\n",
      "351/351 [==============================] - 0s 27us/sample - loss: 4.3981e-04 - acc: 1.0000\n",
      "Epoch 38/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 4.3531e-04 - acc: 1.0000\n",
      "Epoch 39/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 4.1840e-04 - acc: 1.0000\n",
      "Epoch 40/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 4.2905e-04 - acc: 1.0000\n",
      "Epoch 41/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 4.3921e-04 - acc: 1.0000\n",
      "Epoch 42/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 4.0244e-04 - acc: 1.0000\n",
      "Epoch 43/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 4.0072e-04 - acc: 1.0000\n",
      "Epoch 44/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 3.9363e-04 - acc: 1.0000\n",
      "Epoch 45/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 3.8354e-04 - acc: 1.0000\n",
      "Epoch 46/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 3.8788e-04 - acc: 1.0000\n",
      "Epoch 47/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 3.7908e-04 - acc: 1.0000\n",
      "Epoch 48/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 3.7434e-04 - acc: 1.0000\n",
      "Epoch 49/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 3.7167e-04 - acc: 1.0000\n",
      "Epoch 50/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 3.6734e-04 - acc: 1.0000\n",
      "Epoch 51/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 3.5879e-04 - acc: 1.0000\n",
      "Epoch 52/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 3.6247e-04 - acc: 1.0000\n",
      "Epoch 53/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 3.5101e-04 - acc: 1.0000\n",
      "Epoch 54/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 3.4257e-04 - acc: 1.0000\n",
      "Epoch 55/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 3.4093e-04 - acc: 1.0000\n",
      "Epoch 56/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 3.3573e-04 - acc: 1.0000\n",
      "Epoch 57/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 3.3608e-04 - acc: 1.0000\n",
      "Epoch 58/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 3.2583e-04 - acc: 1.0000\n",
      "Epoch 59/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 3.2626e-04 - acc: 1.0000\n",
      "Epoch 60/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 3.2950e-04 - acc: 1.0000\n",
      "Epoch 61/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 3.1525e-04 - acc: 1.0000\n",
      "Epoch 62/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 3.1761e-04 - acc: 1.0000\n",
      "Epoch 63/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 3.1175e-04 - acc: 1.0000\n",
      "Epoch 64/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 3.0732e-04 - acc: 1.0000\n",
      "Epoch 65/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 3.0456e-04 - acc: 1.0000\n",
      "Epoch 66/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 3.0053e-04 - acc: 1.0000\n",
      "Epoch 67/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 3.0568e-04 - acc: 1.0000\n",
      "Epoch 68/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 3.0108e-04 - acc: 1.0000\n",
      "Epoch 69/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 2.8883e-04 - acc: 1.0000\n",
      "Epoch 70/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 2.9719e-04 - acc: 1.0000\n",
      "Epoch 71/100\n",
      "351/351 [==============================] - 0s 57us/sample - loss: 2.7834e-04 - acc: 1.0000\n",
      "Epoch 72/100\n",
      "351/351 [==============================] - 0s 37us/sample - loss: 2.7798e-04 - acc: 1.0000\n",
      "Epoch 73/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 2.7745e-04 - acc: 1.0000\n",
      "Epoch 74/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 2.7389e-04 - acc: 1.0000\n",
      "Epoch 75/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 2.6997e-04 - acc: 1.0000\n",
      "Epoch 76/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 2.7181e-04 - acc: 1.0000\n",
      "Epoch 77/100\n",
      "351/351 [==============================] - 0s 31us/sample - loss: 2.5902e-04 - acc: 1.0000\n",
      "Epoch 78/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 2.6269e-04 - acc: 1.0000\n",
      "Epoch 79/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 2.5412e-04 - acc: 1.0000\n",
      "Epoch 80/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 2.5618e-04 - acc: 1.0000\n",
      "Epoch 81/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 2.4949e-04 - acc: 1.0000\n",
      "Epoch 82/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 2.4958e-04 - acc: 1.0000\n",
      "Epoch 83/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 2.5083e-04 - acc: 1.0000\n",
      "Epoch 84/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 2.4097e-04 - acc: 1.0000\n",
      "Epoch 85/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 2.4159e-04 - acc: 1.0000\n",
      "Epoch 86/100\n",
      "351/351 [==============================] - 0s 34us/sample - loss: 2.3411e-04 - acc: 1.0000\n",
      "Epoch 87/100\n",
      "351/351 [==============================] - 0s 31us/sample - loss: 2.3346e-04 - acc: 1.0000\n",
      "Epoch 88/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 2.2790e-04 - acc: 1.0000\n",
      "Epoch 89/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 2.2439e-04 - acc: 1.0000\n",
      "Epoch 90/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 2.2395e-04 - acc: 1.0000\n",
      "Epoch 91/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 2.2280e-04 - acc: 1.0000\n",
      "Epoch 92/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 2.2133e-04 - acc: 1.0000\n",
      "Epoch 93/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 2.2093e-04 - acc: 1.0000\n",
      "Epoch 94/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 2.2859e-04 - acc: 1.0000\n",
      "Epoch 95/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 2.0809e-04 - acc: 1.0000\n",
      "Epoch 96/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 2.0801e-04 - acc: 1.0000\n",
      "Epoch 97/100\n",
      "351/351 [==============================] - 0s 28us/sample - loss: 2.0427e-04 - acc: 1.0000\n",
      "Epoch 98/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 2.0446e-04 - acc: 1.0000\n",
      "Epoch 99/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 2.0466e-04 - acc: 1.0000\n",
      "Epoch 100/100\n",
      "351/351 [==============================] - 0s 26us/sample - loss: 2.0602e-04 - acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "oof_train1 = np.zeros((x_train.shape[0], _N_CLASS))  #  _N_CLASS\n",
    "oof_test1 = np.empty((x_test.shape[0], _N_CLASS))  #  _N_CLASS\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kf.split(x_train,y_train)):\n",
    "    kf_X_train = x_train[train_index]  #  交叉验证划分此时的训练集和验证集\n",
    "    kf_y_train = y_train[train_index]  \n",
    "    kf_y_train = lb.transform(kf_y_train)\n",
    "    kf_X_test = x_train[test_index]  # 验证集\n",
    "\n",
    "    dnn.fit(kf_X_train, kf_y_train,batch_size=20, epochs=100, shuffle=True, verbose=1)  # 当前模型进行训练\n",
    "    \n",
    "    oof_train1[test_index] = dnn.predict(kf_X_test)  # 当前验证集进行概率预测， 200 * _N_CLASS\n",
    "    oof_test1 += dnn.predict(x_test)  # 对测试集概率预测 oof_test_skf[i, :] ，  500 * _N_CLASS\n",
    "\n",
    "oof_test1 /= _N_FOLDS  # 对每一则交叉验证的结果取平均"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oof(clf, X_train, y_train, X_test):\n",
    "    oof_train = np.zeros((X_train.shape[0], _N_CLASS))  # _N_CLASS\n",
    "    oof_test = np.empty((X_test.shape[0], _N_CLASS))  # _N_CLASS\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(X_train,y_train)):\n",
    "        kf_X_train = X_train[train_index]  # 交叉验证划分此时的训练集和验证集\n",
    "        kf_y_train = y_train[train_index]  # \n",
    "        kf_X_test = X_train[test_index]  #  验证集\n",
    "\n",
    "        clf.fit(kf_X_train, kf_y_train)  # 当前模型进行训练\n",
    "\n",
    "        oof_train[test_index] = clf.predict_proba(kf_X_test)  # 当前验证集进行概率预测， _N_CLASS\n",
    "        oof_test += clf.predict_proba(X_test)  # 对测试集概率预测 oof_test_skf[i, :] ， _N_CLASS\n",
    "\n",
    "    oof_test /= _N_FOLDS  # 对每一则交叉验证的结果取平均\n",
    "    return oof_train, oof_test  # 返回当前分类器对训练集和测试集的预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      "WARNING:tensorflow:From C:\\Users\\Administrator\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 9.277476\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 10.360649\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 11.032143\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 11.386614\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 11.213879\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 10.010457\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 10.012874\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 9.295369\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 8.747927\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 7.771922\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 8.008360\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 7.648359\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 7.673761\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 6.994595\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 7.024721\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 7.123480\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 7.078439\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 6.965693\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 7.005249\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 7.400004\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 16.521845\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 23.021763\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 20.488211\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 19.811234\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 17.353733\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 16.537010\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 17.939491\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 17.262058\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 18.035408\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 16.647610\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 16.109533\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 16.723494\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 16.631626\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 16.159618\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 16.848776\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 18.143509\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 17.851057\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 18.294149\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 18.233843\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 18.283779\n",
      "[END] Pre-training step\n",
      "WARNING:tensorflow:From C:\\Users\\Administrator\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.948441\n",
      ">> Epoch 1 finished \tANN training loss 0.985276\n",
      ">> Epoch 2 finished \tANN training loss 0.861928\n",
      ">> Epoch 3 finished \tANN training loss 0.870348\n",
      ">> Epoch 4 finished \tANN training loss 0.814562\n",
      ">> Epoch 5 finished \tANN training loss 0.780435\n",
      ">> Epoch 6 finished \tANN training loss 0.769821\n",
      ">> Epoch 7 finished \tANN training loss 0.713762\n",
      ">> Epoch 8 finished \tANN training loss 0.708067\n",
      ">> Epoch 9 finished \tANN training loss 0.682074\n",
      ">> Epoch 10 finished \tANN training loss 0.661598\n",
      ">> Epoch 11 finished \tANN training loss 0.641939\n",
      ">> Epoch 12 finished \tANN training loss 0.610054\n",
      ">> Epoch 13 finished \tANN training loss 0.568133\n",
      ">> Epoch 14 finished \tANN training loss 0.552771\n",
      ">> Epoch 15 finished \tANN training loss 0.548425\n",
      ">> Epoch 16 finished \tANN training loss 0.515948\n",
      ">> Epoch 17 finished \tANN training loss 0.485233\n",
      ">> Epoch 18 finished \tANN training loss 0.463109\n",
      ">> Epoch 19 finished \tANN training loss 0.469987\n",
      ">> Epoch 20 finished \tANN training loss 0.472759\n",
      ">> Epoch 21 finished \tANN training loss 0.473104\n",
      ">> Epoch 22 finished \tANN training loss 0.446176\n",
      ">> Epoch 23 finished \tANN training loss 0.436759\n",
      ">> Epoch 24 finished \tANN training loss 0.442801\n",
      ">> Epoch 25 finished \tANN training loss 0.421779\n",
      ">> Epoch 26 finished \tANN training loss 0.430991\n",
      ">> Epoch 27 finished \tANN training loss 0.429743\n",
      ">> Epoch 28 finished \tANN training loss 0.405853\n",
      ">> Epoch 29 finished \tANN training loss 0.407151\n",
      ">> Epoch 30 finished \tANN training loss 0.401201\n",
      ">> Epoch 31 finished \tANN training loss 0.380705\n",
      ">> Epoch 32 finished \tANN training loss 0.374849\n",
      ">> Epoch 33 finished \tANN training loss 0.395043\n",
      ">> Epoch 34 finished \tANN training loss 0.386954\n",
      ">> Epoch 35 finished \tANN training loss 0.377514\n",
      ">> Epoch 36 finished \tANN training loss 0.359963\n",
      ">> Epoch 37 finished \tANN training loss 0.356582\n",
      ">> Epoch 38 finished \tANN training loss 0.350471\n",
      ">> Epoch 39 finished \tANN training loss 0.338983\n",
      ">> Epoch 40 finished \tANN training loss 0.332173\n",
      ">> Epoch 41 finished \tANN training loss 0.339301\n",
      ">> Epoch 42 finished \tANN training loss 0.356692\n",
      ">> Epoch 43 finished \tANN training loss 0.325796\n",
      ">> Epoch 44 finished \tANN training loss 0.352036\n",
      ">> Epoch 45 finished \tANN training loss 0.324689\n",
      ">> Epoch 46 finished \tANN training loss 0.314194\n",
      ">> Epoch 47 finished \tANN training loss 0.322457\n",
      ">> Epoch 48 finished \tANN training loss 0.315796\n",
      ">> Epoch 49 finished \tANN training loss 0.303085\n",
      ">> Epoch 50 finished \tANN training loss 0.304865\n",
      ">> Epoch 51 finished \tANN training loss 0.341037\n",
      ">> Epoch 52 finished \tANN training loss 0.282973\n",
      ">> Epoch 53 finished \tANN training loss 0.284550\n",
      ">> Epoch 54 finished \tANN training loss 0.284366\n",
      ">> Epoch 55 finished \tANN training loss 0.272606\n",
      ">> Epoch 56 finished \tANN training loss 0.277579\n",
      ">> Epoch 57 finished \tANN training loss 0.295125\n",
      ">> Epoch 58 finished \tANN training loss 0.283998\n",
      ">> Epoch 59 finished \tANN training loss 0.274078\n",
      ">> Epoch 60 finished \tANN training loss 0.272334\n",
      ">> Epoch 61 finished \tANN training loss 0.261624\n",
      ">> Epoch 62 finished \tANN training loss 0.251989\n",
      ">> Epoch 63 finished \tANN training loss 0.245069\n",
      ">> Epoch 64 finished \tANN training loss 0.259762\n",
      ">> Epoch 65 finished \tANN training loss 0.239354\n",
      ">> Epoch 66 finished \tANN training loss 0.243614\n",
      ">> Epoch 67 finished \tANN training loss 0.248862\n",
      ">> Epoch 68 finished \tANN training loss 0.259531\n",
      ">> Epoch 69 finished \tANN training loss 0.237747\n",
      ">> Epoch 70 finished \tANN training loss 0.245087\n",
      ">> Epoch 71 finished \tANN training loss 0.224646\n",
      ">> Epoch 72 finished \tANN training loss 0.247052\n",
      ">> Epoch 73 finished \tANN training loss 0.217964\n",
      ">> Epoch 74 finished \tANN training loss 0.211102\n",
      ">> Epoch 75 finished \tANN training loss 0.245832\n",
      ">> Epoch 76 finished \tANN training loss 0.227366\n",
      ">> Epoch 77 finished \tANN training loss 0.210111\n",
      ">> Epoch 78 finished \tANN training loss 0.206541\n",
      ">> Epoch 79 finished \tANN training loss 0.213980\n",
      ">> Epoch 80 finished \tANN training loss 0.190174\n",
      ">> Epoch 81 finished \tANN training loss 0.205766\n",
      ">> Epoch 82 finished \tANN training loss 0.203547\n",
      ">> Epoch 83 finished \tANN training loss 0.197628\n",
      ">> Epoch 84 finished \tANN training loss 0.213671\n",
      ">> Epoch 85 finished \tANN training loss 0.187783\n",
      ">> Epoch 86 finished \tANN training loss 0.219823\n",
      ">> Epoch 87 finished \tANN training loss 0.191305\n",
      ">> Epoch 88 finished \tANN training loss 0.219458\n",
      ">> Epoch 89 finished \tANN training loss 0.191048\n",
      ">> Epoch 90 finished \tANN training loss 0.206050\n",
      ">> Epoch 91 finished \tANN training loss 0.207989\n",
      ">> Epoch 92 finished \tANN training loss 0.204879\n",
      ">> Epoch 93 finished \tANN training loss 0.192455\n",
      ">> Epoch 94 finished \tANN training loss 0.228084\n",
      ">> Epoch 95 finished \tANN training loss 0.181568\n",
      ">> Epoch 96 finished \tANN training loss 0.197348\n",
      ">> Epoch 97 finished \tANN training loss 0.196605\n",
      ">> Epoch 98 finished \tANN training loss 0.179072\n",
      ">> Epoch 99 finished \tANN training loss 0.198841\n",
      ">> Epoch 100 finished \tANN training loss 0.186812\n",
      ">> Epoch 101 finished \tANN training loss 0.192138\n",
      ">> Epoch 102 finished \tANN training loss 0.192719\n",
      ">> Epoch 103 finished \tANN training loss 0.174964\n",
      ">> Epoch 104 finished \tANN training loss 0.182141\n",
      ">> Epoch 105 finished \tANN training loss 0.174174\n",
      ">> Epoch 106 finished \tANN training loss 0.171883\n",
      ">> Epoch 107 finished \tANN training loss 0.166616\n",
      ">> Epoch 108 finished \tANN training loss 0.173310\n",
      ">> Epoch 109 finished \tANN training loss 0.192150\n",
      ">> Epoch 110 finished \tANN training loss 0.173337\n",
      ">> Epoch 111 finished \tANN training loss 0.160697\n",
      ">> Epoch 112 finished \tANN training loss 0.165689\n",
      ">> Epoch 113 finished \tANN training loss 0.160580\n",
      ">> Epoch 114 finished \tANN training loss 0.167545\n",
      ">> Epoch 115 finished \tANN training loss 0.160177\n",
      ">> Epoch 116 finished \tANN training loss 0.154388\n",
      ">> Epoch 117 finished \tANN training loss 0.164141\n",
      ">> Epoch 118 finished \tANN training loss 0.166488\n",
      ">> Epoch 119 finished \tANN training loss 0.165409\n",
      ">> Epoch 120 finished \tANN training loss 0.163959\n",
      ">> Epoch 121 finished \tANN training loss 0.183486\n",
      ">> Epoch 122 finished \tANN training loss 0.175045\n",
      ">> Epoch 123 finished \tANN training loss 0.159420\n",
      ">> Epoch 124 finished \tANN training loss 0.176466\n",
      ">> Epoch 125 finished \tANN training loss 0.162845\n",
      ">> Epoch 126 finished \tANN training loss 0.158686\n",
      ">> Epoch 127 finished \tANN training loss 0.157952\n",
      ">> Epoch 128 finished \tANN training loss 0.163788\n",
      ">> Epoch 129 finished \tANN training loss 0.156160\n",
      ">> Epoch 130 finished \tANN training loss 0.147254\n",
      ">> Epoch 131 finished \tANN training loss 0.169295\n",
      ">> Epoch 132 finished \tANN training loss 0.146576\n",
      ">> Epoch 133 finished \tANN training loss 0.157352\n",
      ">> Epoch 134 finished \tANN training loss 0.153977\n",
      ">> Epoch 135 finished \tANN training loss 0.178584\n",
      ">> Epoch 136 finished \tANN training loss 0.144007\n",
      ">> Epoch 137 finished \tANN training loss 0.147952\n",
      ">> Epoch 138 finished \tANN training loss 0.147785\n",
      ">> Epoch 139 finished \tANN training loss 0.147758\n",
      ">> Epoch 140 finished \tANN training loss 0.194585\n",
      ">> Epoch 141 finished \tANN training loss 0.157992\n",
      ">> Epoch 142 finished \tANN training loss 0.170758\n",
      ">> Epoch 143 finished \tANN training loss 0.145921\n",
      ">> Epoch 144 finished \tANN training loss 0.145619\n",
      ">> Epoch 145 finished \tANN training loss 0.155968\n",
      ">> Epoch 146 finished \tANN training loss 0.165941\n",
      ">> Epoch 147 finished \tANN training loss 0.155986\n",
      ">> Epoch 148 finished \tANN training loss 0.145666\n",
      ">> Epoch 149 finished \tANN training loss 0.145023\n",
      ">> Epoch 150 finished \tANN training loss 0.176771\n",
      ">> Epoch 151 finished \tANN training loss 0.139426\n",
      ">> Epoch 152 finished \tANN training loss 0.147417\n",
      ">> Epoch 153 finished \tANN training loss 0.139059\n",
      ">> Epoch 154 finished \tANN training loss 0.166102\n",
      ">> Epoch 155 finished \tANN training loss 0.136530\n",
      ">> Epoch 156 finished \tANN training loss 0.136897\n",
      ">> Epoch 157 finished \tANN training loss 0.140460\n",
      ">> Epoch 158 finished \tANN training loss 0.147433\n",
      ">> Epoch 159 finished \tANN training loss 0.145237\n",
      ">> Epoch 160 finished \tANN training loss 0.140045\n",
      ">> Epoch 161 finished \tANN training loss 0.169909\n",
      ">> Epoch 162 finished \tANN training loss 0.172021\n",
      ">> Epoch 163 finished \tANN training loss 0.156543\n",
      ">> Epoch 164 finished \tANN training loss 0.141241\n",
      ">> Epoch 165 finished \tANN training loss 0.131048\n",
      ">> Epoch 166 finished \tANN training loss 0.144020\n",
      ">> Epoch 167 finished \tANN training loss 0.139931\n",
      ">> Epoch 168 finished \tANN training loss 0.147346\n",
      ">> Epoch 169 finished \tANN training loss 0.127898\n",
      ">> Epoch 170 finished \tANN training loss 0.140770\n",
      ">> Epoch 171 finished \tANN training loss 0.167444\n",
      ">> Epoch 172 finished \tANN training loss 0.129669\n",
      ">> Epoch 173 finished \tANN training loss 0.143117\n",
      ">> Epoch 174 finished \tANN training loss 0.150140\n",
      ">> Epoch 175 finished \tANN training loss 0.123745\n",
      ">> Epoch 176 finished \tANN training loss 0.130817\n",
      ">> Epoch 177 finished \tANN training loss 0.127600\n",
      ">> Epoch 178 finished \tANN training loss 0.136066\n",
      ">> Epoch 179 finished \tANN training loss 0.131752\n",
      ">> Epoch 180 finished \tANN training loss 0.131344\n",
      ">> Epoch 181 finished \tANN training loss 0.129349\n",
      ">> Epoch 182 finished \tANN training loss 0.147203\n",
      ">> Epoch 183 finished \tANN training loss 0.134196\n",
      ">> Epoch 184 finished \tANN training loss 0.168497\n",
      ">> Epoch 185 finished \tANN training loss 0.136935\n",
      ">> Epoch 186 finished \tANN training loss 0.155186\n",
      ">> Epoch 187 finished \tANN training loss 0.151447\n",
      ">> Epoch 188 finished \tANN training loss 0.126659\n",
      ">> Epoch 189 finished \tANN training loss 0.124497\n",
      ">> Epoch 190 finished \tANN training loss 0.136461\n",
      ">> Epoch 191 finished \tANN training loss 0.125514\n",
      ">> Epoch 192 finished \tANN training loss 0.123884\n",
      ">> Epoch 193 finished \tANN training loss 0.134618\n",
      ">> Epoch 194 finished \tANN training loss 0.140156\n",
      ">> Epoch 195 finished \tANN training loss 0.133400\n",
      ">> Epoch 196 finished \tANN training loss 0.129866\n",
      ">> Epoch 197 finished \tANN training loss 0.125599\n",
      ">> Epoch 198 finished \tANN training loss 0.130496\n",
      ">> Epoch 199 finished \tANN training loss 0.162060\n",
      ">> Epoch 200 finished \tANN training loss 0.171121\n",
      ">> Epoch 201 finished \tANN training loss 0.134513\n",
      ">> Epoch 202 finished \tANN training loss 0.128766\n",
      ">> Epoch 203 finished \tANN training loss 0.124802\n",
      ">> Epoch 204 finished \tANN training loss 0.138636\n",
      ">> Epoch 205 finished \tANN training loss 0.143202\n",
      ">> Epoch 206 finished \tANN training loss 0.122036\n",
      ">> Epoch 207 finished \tANN training loss 0.126992\n",
      ">> Epoch 208 finished \tANN training loss 0.125895\n",
      ">> Epoch 209 finished \tANN training loss 0.116589\n",
      ">> Epoch 210 finished \tANN training loss 0.115590\n",
      ">> Epoch 211 finished \tANN training loss 0.112896\n",
      ">> Epoch 212 finished \tANN training loss 0.124548\n",
      ">> Epoch 213 finished \tANN training loss 0.128123\n",
      ">> Epoch 214 finished \tANN training loss 0.123249\n",
      ">> Epoch 215 finished \tANN training loss 0.114967\n",
      ">> Epoch 216 finished \tANN training loss 0.114479\n",
      ">> Epoch 217 finished \tANN training loss 0.131404\n",
      ">> Epoch 218 finished \tANN training loss 0.116621\n",
      ">> Epoch 219 finished \tANN training loss 0.123772\n",
      ">> Epoch 220 finished \tANN training loss 0.121208\n",
      ">> Epoch 221 finished \tANN training loss 0.113206\n",
      ">> Epoch 222 finished \tANN training loss 0.121130\n",
      ">> Epoch 223 finished \tANN training loss 0.122993\n",
      ">> Epoch 224 finished \tANN training loss 0.128921\n",
      ">> Epoch 225 finished \tANN training loss 0.163657\n",
      ">> Epoch 226 finished \tANN training loss 0.130915\n",
      ">> Epoch 227 finished \tANN training loss 0.128061\n",
      ">> Epoch 228 finished \tANN training loss 0.117552\n",
      ">> Epoch 229 finished \tANN training loss 0.109278\n",
      ">> Epoch 230 finished \tANN training loss 0.117980\n",
      ">> Epoch 231 finished \tANN training loss 0.118981\n",
      ">> Epoch 232 finished \tANN training loss 0.111815\n",
      ">> Epoch 233 finished \tANN training loss 0.115331\n",
      ">> Epoch 234 finished \tANN training loss 0.114012\n",
      ">> Epoch 235 finished \tANN training loss 0.130326\n",
      ">> Epoch 236 finished \tANN training loss 0.122716\n",
      ">> Epoch 237 finished \tANN training loss 0.124285\n",
      ">> Epoch 238 finished \tANN training loss 0.114224\n",
      ">> Epoch 239 finished \tANN training loss 0.115637\n",
      ">> Epoch 240 finished \tANN training loss 0.112516\n",
      ">> Epoch 241 finished \tANN training loss 0.132585\n",
      ">> Epoch 242 finished \tANN training loss 0.149583\n",
      ">> Epoch 243 finished \tANN training loss 0.133111\n",
      ">> Epoch 244 finished \tANN training loss 0.121869\n",
      ">> Epoch 245 finished \tANN training loss 0.122746\n",
      ">> Epoch 246 finished \tANN training loss 0.114130\n",
      ">> Epoch 247 finished \tANN training loss 0.142753\n",
      ">> Epoch 248 finished \tANN training loss 0.130719\n",
      ">> Epoch 249 finished \tANN training loss 0.112739\n",
      ">> Epoch 250 finished \tANN training loss 0.111329\n",
      ">> Epoch 251 finished \tANN training loss 0.144966\n",
      ">> Epoch 252 finished \tANN training loss 0.110567\n",
      ">> Epoch 253 finished \tANN training loss 0.156703\n",
      ">> Epoch 254 finished \tANN training loss 0.136599\n",
      ">> Epoch 255 finished \tANN training loss 0.116064\n",
      ">> Epoch 256 finished \tANN training loss 0.115771\n",
      ">> Epoch 257 finished \tANN training loss 0.137013\n",
      ">> Epoch 258 finished \tANN training loss 0.119357\n",
      ">> Epoch 259 finished \tANN training loss 0.116669\n",
      ">> Epoch 260 finished \tANN training loss 0.135244\n",
      ">> Epoch 261 finished \tANN training loss 0.117904\n",
      ">> Epoch 262 finished \tANN training loss 0.126663\n",
      ">> Epoch 263 finished \tANN training loss 0.145023\n",
      ">> Epoch 264 finished \tANN training loss 0.106240\n",
      ">> Epoch 265 finished \tANN training loss 0.117242\n",
      ">> Epoch 266 finished \tANN training loss 0.114721\n",
      ">> Epoch 267 finished \tANN training loss 0.109539\n",
      ">> Epoch 268 finished \tANN training loss 0.120206\n",
      ">> Epoch 269 finished \tANN training loss 0.110784\n",
      ">> Epoch 270 finished \tANN training loss 0.113852\n",
      ">> Epoch 271 finished \tANN training loss 0.114744\n",
      ">> Epoch 272 finished \tANN training loss 0.108732\n",
      ">> Epoch 273 finished \tANN training loss 0.128724\n",
      ">> Epoch 274 finished \tANN training loss 0.110062\n",
      ">> Epoch 275 finished \tANN training loss 0.187709\n",
      ">> Epoch 276 finished \tANN training loss 0.138944\n",
      ">> Epoch 277 finished \tANN training loss 0.108378\n",
      ">> Epoch 278 finished \tANN training loss 0.112366\n",
      ">> Epoch 279 finished \tANN training loss 0.106980\n",
      ">> Epoch 280 finished \tANN training loss 0.101374\n",
      ">> Epoch 281 finished \tANN training loss 0.103954\n",
      ">> Epoch 282 finished \tANN training loss 0.104615\n",
      ">> Epoch 283 finished \tANN training loss 0.108580\n",
      ">> Epoch 284 finished \tANN training loss 0.140895\n",
      ">> Epoch 285 finished \tANN training loss 0.126079\n",
      ">> Epoch 286 finished \tANN training loss 0.114394\n",
      ">> Epoch 287 finished \tANN training loss 0.105185\n",
      ">> Epoch 288 finished \tANN training loss 0.108069\n",
      ">> Epoch 289 finished \tANN training loss 0.101491\n",
      ">> Epoch 290 finished \tANN training loss 0.114300\n",
      ">> Epoch 291 finished \tANN training loss 0.097441\n",
      ">> Epoch 292 finished \tANN training loss 0.112593\n",
      ">> Epoch 293 finished \tANN training loss 0.099026\n",
      ">> Epoch 294 finished \tANN training loss 0.101923\n",
      ">> Epoch 295 finished \tANN training loss 0.136738\n",
      ">> Epoch 296 finished \tANN training loss 0.122343\n",
      ">> Epoch 297 finished \tANN training loss 0.098518\n",
      ">> Epoch 298 finished \tANN training loss 0.104980\n",
      ">> Epoch 299 finished \tANN training loss 0.108765\n",
      ">> Epoch 300 finished \tANN training loss 0.126099\n",
      ">> Epoch 301 finished \tANN training loss 0.116884\n",
      ">> Epoch 302 finished \tANN training loss 0.107590\n",
      ">> Epoch 303 finished \tANN training loss 0.122901\n",
      ">> Epoch 304 finished \tANN training loss 0.097500\n",
      ">> Epoch 305 finished \tANN training loss 0.106308\n",
      ">> Epoch 306 finished \tANN training loss 0.095362\n",
      ">> Epoch 307 finished \tANN training loss 0.108298\n",
      ">> Epoch 308 finished \tANN training loss 0.111121\n",
      ">> Epoch 309 finished \tANN training loss 0.100671\n",
      ">> Epoch 310 finished \tANN training loss 0.101096\n",
      ">> Epoch 311 finished \tANN training loss 0.143728\n",
      ">> Epoch 312 finished \tANN training loss 0.104222\n",
      ">> Epoch 313 finished \tANN training loss 0.151515\n",
      ">> Epoch 314 finished \tANN training loss 0.100397\n",
      ">> Epoch 315 finished \tANN training loss 0.107149\n",
      ">> Epoch 316 finished \tANN training loss 0.106914\n",
      ">> Epoch 317 finished \tANN training loss 0.097438\n",
      ">> Epoch 318 finished \tANN training loss 0.136446\n",
      ">> Epoch 319 finished \tANN training loss 0.109409\n",
      ">> Epoch 320 finished \tANN training loss 0.110233\n",
      ">> Epoch 321 finished \tANN training loss 0.099317\n",
      ">> Epoch 322 finished \tANN training loss 0.136499\n",
      ">> Epoch 323 finished \tANN training loss 0.110900\n",
      ">> Epoch 324 finished \tANN training loss 0.107870\n",
      ">> Epoch 325 finished \tANN training loss 0.111417\n",
      ">> Epoch 326 finished \tANN training loss 0.106825\n",
      ">> Epoch 327 finished \tANN training loss 0.103238\n",
      ">> Epoch 328 finished \tANN training loss 0.111556\n",
      ">> Epoch 329 finished \tANN training loss 0.098608\n",
      ">> Epoch 330 finished \tANN training loss 0.094800\n",
      ">> Epoch 331 finished \tANN training loss 0.093677\n",
      ">> Epoch 332 finished \tANN training loss 0.182782\n",
      ">> Epoch 333 finished \tANN training loss 0.100951\n",
      ">> Epoch 334 finished \tANN training loss 0.105457\n",
      ">> Epoch 335 finished \tANN training loss 0.106311\n",
      ">> Epoch 336 finished \tANN training loss 0.105771\n",
      ">> Epoch 337 finished \tANN training loss 0.103701\n",
      ">> Epoch 338 finished \tANN training loss 0.126206\n",
      ">> Epoch 339 finished \tANN training loss 0.092146\n",
      ">> Epoch 340 finished \tANN training loss 0.097088\n",
      ">> Epoch 341 finished \tANN training loss 0.128093\n",
      ">> Epoch 342 finished \tANN training loss 0.100190\n",
      ">> Epoch 343 finished \tANN training loss 0.096763\n",
      ">> Epoch 344 finished \tANN training loss 0.094118\n",
      ">> Epoch 345 finished \tANN training loss 0.105094\n",
      ">> Epoch 346 finished \tANN training loss 0.110014\n",
      ">> Epoch 347 finished \tANN training loss 0.097680\n",
      ">> Epoch 348 finished \tANN training loss 0.098000\n",
      ">> Epoch 349 finished \tANN training loss 0.106124\n",
      ">> Epoch 350 finished \tANN training loss 0.101232\n",
      ">> Epoch 351 finished \tANN training loss 0.097019\n",
      ">> Epoch 352 finished \tANN training loss 0.097269\n",
      ">> Epoch 353 finished \tANN training loss 0.097876\n",
      ">> Epoch 354 finished \tANN training loss 0.103964\n",
      ">> Epoch 355 finished \tANN training loss 0.103990\n",
      ">> Epoch 356 finished \tANN training loss 0.121314\n",
      ">> Epoch 357 finished \tANN training loss 0.104364\n",
      ">> Epoch 358 finished \tANN training loss 0.099497\n",
      ">> Epoch 359 finished \tANN training loss 0.097570\n",
      ">> Epoch 360 finished \tANN training loss 0.092511\n",
      ">> Epoch 361 finished \tANN training loss 0.102765\n",
      ">> Epoch 362 finished \tANN training loss 0.102392\n",
      ">> Epoch 363 finished \tANN training loss 0.109977\n",
      ">> Epoch 364 finished \tANN training loss 0.093451\n",
      ">> Epoch 365 finished \tANN training loss 0.098986\n",
      ">> Epoch 366 finished \tANN training loss 0.103094\n",
      ">> Epoch 367 finished \tANN training loss 0.101701\n",
      ">> Epoch 368 finished \tANN training loss 0.105582\n",
      ">> Epoch 369 finished \tANN training loss 0.109778\n",
      ">> Epoch 370 finished \tANN training loss 0.140134\n",
      ">> Epoch 371 finished \tANN training loss 0.110246\n",
      ">> Epoch 372 finished \tANN training loss 0.178853\n",
      ">> Epoch 373 finished \tANN training loss 0.106111\n",
      ">> Epoch 374 finished \tANN training loss 0.095809\n",
      ">> Epoch 375 finished \tANN training loss 0.097503\n",
      ">> Epoch 376 finished \tANN training loss 0.096881\n",
      ">> Epoch 377 finished \tANN training loss 0.093006\n",
      ">> Epoch 378 finished \tANN training loss 0.094359\n",
      ">> Epoch 379 finished \tANN training loss 0.101407\n",
      ">> Epoch 380 finished \tANN training loss 0.091897\n",
      ">> Epoch 381 finished \tANN training loss 0.105350\n",
      ">> Epoch 382 finished \tANN training loss 0.093945\n",
      ">> Epoch 383 finished \tANN training loss 0.112151\n",
      ">> Epoch 384 finished \tANN training loss 0.094365\n",
      ">> Epoch 385 finished \tANN training loss 0.092383\n",
      ">> Epoch 386 finished \tANN training loss 0.088100\n",
      ">> Epoch 387 finished \tANN training loss 0.093650\n",
      ">> Epoch 388 finished \tANN training loss 0.096445\n",
      ">> Epoch 389 finished \tANN training loss 0.094329\n",
      ">> Epoch 390 finished \tANN training loss 0.101678\n",
      ">> Epoch 391 finished \tANN training loss 0.091168\n",
      ">> Epoch 392 finished \tANN training loss 0.095728\n",
      ">> Epoch 393 finished \tANN training loss 0.097958\n",
      ">> Epoch 394 finished \tANN training loss 0.090998\n",
      ">> Epoch 395 finished \tANN training loss 0.104986\n",
      ">> Epoch 396 finished \tANN training loss 0.101945\n",
      ">> Epoch 397 finished \tANN training loss 0.099241\n",
      ">> Epoch 398 finished \tANN training loss 0.091931\n",
      ">> Epoch 399 finished \tANN training loss 0.093237\n",
      ">> Epoch 400 finished \tANN training loss 0.102611\n",
      ">> Epoch 401 finished \tANN training loss 0.093106\n",
      ">> Epoch 402 finished \tANN training loss 0.094455\n",
      ">> Epoch 403 finished \tANN training loss 0.105631\n",
      ">> Epoch 404 finished \tANN training loss 0.101859\n",
      ">> Epoch 405 finished \tANN training loss 0.087519\n",
      ">> Epoch 406 finished \tANN training loss 0.085488\n",
      ">> Epoch 407 finished \tANN training loss 0.086007\n",
      ">> Epoch 408 finished \tANN training loss 0.086516\n",
      ">> Epoch 409 finished \tANN training loss 0.092471\n",
      ">> Epoch 410 finished \tANN training loss 0.087294\n",
      ">> Epoch 411 finished \tANN training loss 0.086513\n",
      ">> Epoch 412 finished \tANN training loss 0.091115\n",
      ">> Epoch 413 finished \tANN training loss 0.092913\n",
      ">> Epoch 414 finished \tANN training loss 0.091956\n",
      ">> Epoch 415 finished \tANN training loss 0.085404\n",
      ">> Epoch 416 finished \tANN training loss 0.087413\n",
      ">> Epoch 417 finished \tANN training loss 0.093989\n",
      ">> Epoch 418 finished \tANN training loss 0.080307\n",
      ">> Epoch 419 finished \tANN training loss 0.083248\n",
      ">> Epoch 420 finished \tANN training loss 0.131616\n",
      ">> Epoch 421 finished \tANN training loss 0.087716\n",
      ">> Epoch 422 finished \tANN training loss 0.081811\n",
      ">> Epoch 423 finished \tANN training loss 0.100653\n",
      ">> Epoch 424 finished \tANN training loss 0.082869\n",
      ">> Epoch 425 finished \tANN training loss 0.086747\n",
      ">> Epoch 426 finished \tANN training loss 0.095869\n",
      ">> Epoch 427 finished \tANN training loss 0.136919\n",
      ">> Epoch 428 finished \tANN training loss 0.100066\n",
      ">> Epoch 429 finished \tANN training loss 0.090370\n",
      ">> Epoch 430 finished \tANN training loss 0.087492\n",
      ">> Epoch 431 finished \tANN training loss 0.098613\n",
      ">> Epoch 432 finished \tANN training loss 0.089773\n",
      ">> Epoch 433 finished \tANN training loss 0.099115\n",
      ">> Epoch 434 finished \tANN training loss 0.083056\n",
      ">> Epoch 435 finished \tANN training loss 0.082994\n",
      ">> Epoch 436 finished \tANN training loss 0.084656\n",
      ">> Epoch 437 finished \tANN training loss 0.088493\n",
      ">> Epoch 438 finished \tANN training loss 0.085738\n",
      ">> Epoch 439 finished \tANN training loss 0.088045\n",
      ">> Epoch 440 finished \tANN training loss 0.144935\n",
      ">> Epoch 441 finished \tANN training loss 0.089470\n",
      ">> Epoch 442 finished \tANN training loss 0.092888\n",
      ">> Epoch 443 finished \tANN training loss 0.086672\n",
      ">> Epoch 444 finished \tANN training loss 0.093635\n",
      ">> Epoch 445 finished \tANN training loss 0.094782\n",
      ">> Epoch 446 finished \tANN training loss 0.080667\n",
      ">> Epoch 447 finished \tANN training loss 0.083299\n",
      ">> Epoch 448 finished \tANN training loss 0.101560\n",
      ">> Epoch 449 finished \tANN training loss 0.083694\n",
      ">> Epoch 450 finished \tANN training loss 0.135510\n",
      ">> Epoch 451 finished \tANN training loss 0.123145\n",
      ">> Epoch 452 finished \tANN training loss 0.083968\n",
      ">> Epoch 453 finished \tANN training loss 0.094284\n",
      ">> Epoch 454 finished \tANN training loss 0.087226\n",
      ">> Epoch 455 finished \tANN training loss 0.098128\n",
      ">> Epoch 456 finished \tANN training loss 0.085171\n",
      ">> Epoch 457 finished \tANN training loss 0.077544\n",
      ">> Epoch 458 finished \tANN training loss 0.076669\n",
      ">> Epoch 459 finished \tANN training loss 0.081938\n",
      ">> Epoch 460 finished \tANN training loss 0.080417\n",
      ">> Epoch 461 finished \tANN training loss 0.097287\n",
      ">> Epoch 462 finished \tANN training loss 0.079027\n",
      ">> Epoch 463 finished \tANN training loss 0.080692\n",
      ">> Epoch 464 finished \tANN training loss 0.082613\n",
      ">> Epoch 465 finished \tANN training loss 0.094696\n",
      ">> Epoch 466 finished \tANN training loss 0.094759\n",
      ">> Epoch 467 finished \tANN training loss 0.090437\n",
      ">> Epoch 468 finished \tANN training loss 0.073155\n",
      ">> Epoch 469 finished \tANN training loss 0.085640\n",
      ">> Epoch 470 finished \tANN training loss 0.080485\n",
      ">> Epoch 471 finished \tANN training loss 0.078474\n",
      ">> Epoch 472 finished \tANN training loss 0.081194\n",
      ">> Epoch 473 finished \tANN training loss 0.079170\n",
      ">> Epoch 474 finished \tANN training loss 0.104158\n",
      ">> Epoch 475 finished \tANN training loss 0.092213\n",
      ">> Epoch 476 finished \tANN training loss 0.081051\n",
      ">> Epoch 477 finished \tANN training loss 0.098754\n",
      ">> Epoch 478 finished \tANN training loss 0.084342\n",
      ">> Epoch 479 finished \tANN training loss 0.077928\n",
      ">> Epoch 480 finished \tANN training loss 0.083237\n",
      ">> Epoch 481 finished \tANN training loss 0.094449\n",
      ">> Epoch 482 finished \tANN training loss 0.087174\n",
      ">> Epoch 483 finished \tANN training loss 0.073678\n",
      ">> Epoch 484 finished \tANN training loss 0.072971\n",
      ">> Epoch 485 finished \tANN training loss 0.085656\n",
      ">> Epoch 486 finished \tANN training loss 0.073813\n",
      ">> Epoch 487 finished \tANN training loss 0.073262\n",
      ">> Epoch 488 finished \tANN training loss 0.068587\n",
      ">> Epoch 489 finished \tANN training loss 0.074822\n",
      ">> Epoch 490 finished \tANN training loss 0.076455\n",
      ">> Epoch 491 finished \tANN training loss 0.075821\n",
      ">> Epoch 492 finished \tANN training loss 0.082179\n",
      ">> Epoch 493 finished \tANN training loss 0.077743\n",
      ">> Epoch 494 finished \tANN training loss 0.080746\n",
      ">> Epoch 495 finished \tANN training loss 0.086624\n",
      ">> Epoch 496 finished \tANN training loss 0.075631\n",
      ">> Epoch 497 finished \tANN training loss 0.089620\n",
      ">> Epoch 498 finished \tANN training loss 0.080478\n",
      ">> Epoch 499 finished \tANN training loss 0.075091\n",
      ">> Epoch 500 finished \tANN training loss 0.093894\n",
      ">> Epoch 501 finished \tANN training loss 0.079054\n",
      ">> Epoch 502 finished \tANN training loss 0.076945\n",
      ">> Epoch 503 finished \tANN training loss 0.078218\n",
      ">> Epoch 504 finished \tANN training loss 0.105486\n",
      ">> Epoch 505 finished \tANN training loss 0.089058\n",
      ">> Epoch 506 finished \tANN training loss 0.080335\n",
      ">> Epoch 507 finished \tANN training loss 0.087896\n",
      ">> Epoch 508 finished \tANN training loss 0.078410\n",
      ">> Epoch 509 finished \tANN training loss 0.089577\n",
      ">> Epoch 510 finished \tANN training loss 0.074075\n",
      ">> Epoch 511 finished \tANN training loss 0.078659\n",
      ">> Epoch 512 finished \tANN training loss 0.077645\n",
      ">> Epoch 513 finished \tANN training loss 0.080461\n",
      ">> Epoch 514 finished \tANN training loss 0.079310\n",
      ">> Epoch 515 finished \tANN training loss 0.077338\n",
      ">> Epoch 516 finished \tANN training loss 0.091702\n",
      ">> Epoch 517 finished \tANN training loss 0.080305\n",
      ">> Epoch 518 finished \tANN training loss 0.081595\n",
      ">> Epoch 519 finished \tANN training loss 0.083413\n",
      ">> Epoch 520 finished \tANN training loss 0.077693\n",
      ">> Epoch 521 finished \tANN training loss 0.075407\n",
      ">> Epoch 522 finished \tANN training loss 0.082655\n",
      ">> Epoch 523 finished \tANN training loss 0.078021\n",
      ">> Epoch 524 finished \tANN training loss 0.074723\n",
      ">> Epoch 525 finished \tANN training loss 0.086996\n",
      ">> Epoch 526 finished \tANN training loss 0.083642\n",
      ">> Epoch 527 finished \tANN training loss 0.089215\n",
      ">> Epoch 528 finished \tANN training loss 0.076590\n",
      ">> Epoch 529 finished \tANN training loss 0.073635\n",
      ">> Epoch 530 finished \tANN training loss 0.084074\n",
      ">> Epoch 531 finished \tANN training loss 0.075152\n",
      ">> Epoch 532 finished \tANN training loss 0.072126\n",
      ">> Epoch 533 finished \tANN training loss 0.070429\n",
      ">> Epoch 534 finished \tANN training loss 0.083672\n",
      ">> Epoch 535 finished \tANN training loss 0.087957\n",
      ">> Epoch 536 finished \tANN training loss 0.079623\n",
      ">> Epoch 537 finished \tANN training loss 0.081134\n",
      ">> Epoch 538 finished \tANN training loss 0.075497\n",
      ">> Epoch 539 finished \tANN training loss 0.080732\n",
      ">> Epoch 540 finished \tANN training loss 0.078540\n",
      ">> Epoch 541 finished \tANN training loss 0.098622\n",
      ">> Epoch 542 finished \tANN training loss 0.090468\n",
      ">> Epoch 543 finished \tANN training loss 0.085314\n",
      ">> Epoch 544 finished \tANN training loss 0.073705\n",
      ">> Epoch 545 finished \tANN training loss 0.072735\n",
      ">> Epoch 546 finished \tANN training loss 0.076863\n",
      ">> Epoch 547 finished \tANN training loss 0.073094\n",
      ">> Epoch 548 finished \tANN training loss 0.074411\n",
      ">> Epoch 549 finished \tANN training loss 0.073363\n",
      ">> Epoch 550 finished \tANN training loss 0.071263\n",
      ">> Epoch 551 finished \tANN training loss 0.073678\n",
      ">> Epoch 552 finished \tANN training loss 0.069752\n",
      ">> Epoch 553 finished \tANN training loss 0.074573\n",
      ">> Epoch 554 finished \tANN training loss 0.084607\n",
      ">> Epoch 555 finished \tANN training loss 0.073228\n",
      ">> Epoch 556 finished \tANN training loss 0.092019\n",
      ">> Epoch 557 finished \tANN training loss 0.079739\n",
      ">> Epoch 558 finished \tANN training loss 0.081408\n",
      ">> Epoch 559 finished \tANN training loss 0.100510\n",
      ">> Epoch 560 finished \tANN training loss 0.082462\n",
      ">> Epoch 561 finished \tANN training loss 0.084070\n",
      ">> Epoch 562 finished \tANN training loss 0.071897\n",
      ">> Epoch 563 finished \tANN training loss 0.078700\n",
      ">> Epoch 564 finished \tANN training loss 0.073829\n",
      ">> Epoch 565 finished \tANN training loss 0.072829\n",
      ">> Epoch 566 finished \tANN training loss 0.077469\n",
      ">> Epoch 567 finished \tANN training loss 0.077487\n",
      ">> Epoch 568 finished \tANN training loss 0.099253\n",
      ">> Epoch 569 finished \tANN training loss 0.086638\n",
      ">> Epoch 570 finished \tANN training loss 0.091538\n",
      ">> Epoch 571 finished \tANN training loss 0.080629\n",
      ">> Epoch 572 finished \tANN training loss 0.079106\n",
      ">> Epoch 573 finished \tANN training loss 0.074984\n",
      ">> Epoch 574 finished \tANN training loss 0.075389\n",
      ">> Epoch 575 finished \tANN training loss 0.078733\n",
      ">> Epoch 576 finished \tANN training loss 0.080791\n",
      ">> Epoch 577 finished \tANN training loss 0.075184\n",
      ">> Epoch 578 finished \tANN training loss 0.072266\n",
      ">> Epoch 579 finished \tANN training loss 0.075970\n",
      ">> Epoch 580 finished \tANN training loss 0.083702\n",
      ">> Epoch 581 finished \tANN training loss 0.076734\n",
      ">> Epoch 582 finished \tANN training loss 0.082912\n",
      ">> Epoch 583 finished \tANN training loss 0.087711\n",
      ">> Epoch 584 finished \tANN training loss 0.133819\n",
      ">> Epoch 585 finished \tANN training loss 0.078348\n",
      ">> Epoch 586 finished \tANN training loss 0.072749\n",
      ">> Epoch 587 finished \tANN training loss 0.073715\n",
      ">> Epoch 588 finished \tANN training loss 0.068569\n",
      ">> Epoch 589 finished \tANN training loss 0.069349\n",
      ">> Epoch 590 finished \tANN training loss 0.086151\n",
      ">> Epoch 591 finished \tANN training loss 0.067199\n",
      ">> Epoch 592 finished \tANN training loss 0.066472\n",
      ">> Epoch 593 finished \tANN training loss 0.070963\n",
      ">> Epoch 594 finished \tANN training loss 0.069371\n",
      ">> Epoch 595 finished \tANN training loss 0.070164\n",
      ">> Epoch 596 finished \tANN training loss 0.078571\n",
      ">> Epoch 597 finished \tANN training loss 0.071266\n",
      ">> Epoch 598 finished \tANN training loss 0.066943\n",
      ">> Epoch 599 finished \tANN training loss 0.078657\n",
      ">> Epoch 600 finished \tANN training loss 0.068251\n",
      ">> Epoch 601 finished \tANN training loss 0.086305\n",
      ">> Epoch 602 finished \tANN training loss 0.067843\n",
      ">> Epoch 603 finished \tANN training loss 0.077018\n",
      ">> Epoch 604 finished \tANN training loss 0.068148\n",
      ">> Epoch 605 finished \tANN training loss 0.074512\n",
      ">> Epoch 606 finished \tANN training loss 0.078676\n",
      ">> Epoch 607 finished \tANN training loss 0.075952\n",
      ">> Epoch 608 finished \tANN training loss 0.065480\n",
      ">> Epoch 609 finished \tANN training loss 0.076365\n",
      ">> Epoch 610 finished \tANN training loss 0.071873\n",
      ">> Epoch 611 finished \tANN training loss 0.080584\n",
      ">> Epoch 612 finished \tANN training loss 0.075122\n",
      ">> Epoch 613 finished \tANN training loss 0.072749\n",
      ">> Epoch 614 finished \tANN training loss 0.081009\n",
      ">> Epoch 615 finished \tANN training loss 0.085310\n",
      ">> Epoch 616 finished \tANN training loss 0.078481\n",
      ">> Epoch 617 finished \tANN training loss 0.079108\n",
      ">> Epoch 618 finished \tANN training loss 0.083788\n",
      ">> Epoch 619 finished \tANN training loss 0.079318\n",
      ">> Epoch 620 finished \tANN training loss 0.082792\n",
      ">> Epoch 621 finished \tANN training loss 0.096104\n",
      ">> Epoch 622 finished \tANN training loss 0.093827\n",
      ">> Epoch 623 finished \tANN training loss 0.080479\n",
      ">> Epoch 624 finished \tANN training loss 0.080741\n",
      ">> Epoch 625 finished \tANN training loss 0.082969\n",
      ">> Epoch 626 finished \tANN training loss 0.076924\n",
      ">> Epoch 627 finished \tANN training loss 0.084552\n",
      ">> Epoch 628 finished \tANN training loss 0.082613\n",
      ">> Epoch 629 finished \tANN training loss 0.084502\n",
      ">> Epoch 630 finished \tANN training loss 0.083761\n",
      ">> Epoch 631 finished \tANN training loss 0.075821\n",
      ">> Epoch 632 finished \tANN training loss 0.079025\n",
      ">> Epoch 633 finished \tANN training loss 0.074482\n",
      ">> Epoch 634 finished \tANN training loss 0.077905\n",
      ">> Epoch 635 finished \tANN training loss 0.074022\n",
      ">> Epoch 636 finished \tANN training loss 0.094427\n",
      ">> Epoch 637 finished \tANN training loss 0.085665\n",
      ">> Epoch 638 finished \tANN training loss 0.075300\n",
      ">> Epoch 639 finished \tANN training loss 0.074199\n",
      ">> Epoch 640 finished \tANN training loss 0.072403\n",
      ">> Epoch 641 finished \tANN training loss 0.074947\n",
      ">> Epoch 642 finished \tANN training loss 0.082983\n",
      ">> Epoch 643 finished \tANN training loss 0.085235\n",
      ">> Epoch 644 finished \tANN training loss 0.078991\n",
      ">> Epoch 645 finished \tANN training loss 0.090858\n",
      ">> Epoch 646 finished \tANN training loss 0.091969\n",
      ">> Epoch 647 finished \tANN training loss 0.076100\n",
      ">> Epoch 648 finished \tANN training loss 0.082096\n",
      ">> Epoch 649 finished \tANN training loss 0.075056\n",
      ">> Epoch 650 finished \tANN training loss 0.076776\n",
      ">> Epoch 651 finished \tANN training loss 0.076786\n",
      ">> Epoch 652 finished \tANN training loss 0.076882\n",
      ">> Epoch 653 finished \tANN training loss 0.080856\n",
      ">> Epoch 654 finished \tANN training loss 0.072816\n",
      ">> Epoch 655 finished \tANN training loss 0.086559\n",
      ">> Epoch 656 finished \tANN training loss 0.069496\n",
      ">> Epoch 657 finished \tANN training loss 0.079968\n",
      ">> Epoch 658 finished \tANN training loss 0.081332\n",
      ">> Epoch 659 finished \tANN training loss 0.083269\n",
      ">> Epoch 660 finished \tANN training loss 0.075161\n",
      ">> Epoch 661 finished \tANN training loss 0.074067\n",
      ">> Epoch 662 finished \tANN training loss 0.072624\n",
      ">> Epoch 663 finished \tANN training loss 0.086830\n",
      ">> Epoch 664 finished \tANN training loss 0.076598\n",
      ">> Epoch 665 finished \tANN training loss 0.072487\n",
      ">> Epoch 666 finished \tANN training loss 0.085374\n",
      ">> Epoch 667 finished \tANN training loss 0.078120\n",
      ">> Epoch 668 finished \tANN training loss 0.070158\n",
      ">> Epoch 669 finished \tANN training loss 0.069615\n",
      ">> Epoch 670 finished \tANN training loss 0.074398\n",
      ">> Epoch 671 finished \tANN training loss 0.066951\n",
      ">> Epoch 672 finished \tANN training loss 0.065573\n",
      ">> Epoch 673 finished \tANN training loss 0.063298\n",
      ">> Epoch 674 finished \tANN training loss 0.063523\n",
      ">> Epoch 675 finished \tANN training loss 0.067142\n",
      ">> Epoch 676 finished \tANN training loss 0.075251\n",
      ">> Epoch 677 finished \tANN training loss 0.070681\n",
      ">> Epoch 678 finished \tANN training loss 0.074737\n",
      ">> Epoch 679 finished \tANN training loss 0.066667\n",
      ">> Epoch 680 finished \tANN training loss 0.077404\n",
      ">> Epoch 681 finished \tANN training loss 0.085452\n",
      ">> Epoch 682 finished \tANN training loss 0.075762\n",
      ">> Epoch 683 finished \tANN training loss 0.074469\n",
      ">> Epoch 684 finished \tANN training loss 0.074516\n",
      ">> Epoch 685 finished \tANN training loss 0.072351\n",
      ">> Epoch 686 finished \tANN training loss 0.066424\n",
      ">> Epoch 687 finished \tANN training loss 0.069130\n",
      ">> Epoch 688 finished \tANN training loss 0.063683\n",
      ">> Epoch 689 finished \tANN training loss 0.068139\n",
      ">> Epoch 690 finished \tANN training loss 0.075435\n",
      ">> Epoch 691 finished \tANN training loss 0.098216\n",
      ">> Epoch 692 finished \tANN training loss 0.067907\n",
      ">> Epoch 693 finished \tANN training loss 0.070164\n",
      ">> Epoch 694 finished \tANN training loss 0.067895\n",
      ">> Epoch 695 finished \tANN training loss 0.077138\n",
      ">> Epoch 696 finished \tANN training loss 0.077138\n",
      ">> Epoch 697 finished \tANN training loss 0.079012\n",
      ">> Epoch 698 finished \tANN training loss 0.066802\n",
      ">> Epoch 699 finished \tANN training loss 0.064200\n",
      ">> Epoch 700 finished \tANN training loss 0.061714\n",
      ">> Epoch 701 finished \tANN training loss 0.065281\n",
      ">> Epoch 702 finished \tANN training loss 0.075487\n",
      ">> Epoch 703 finished \tANN training loss 0.065224\n",
      ">> Epoch 704 finished \tANN training loss 0.074830\n",
      ">> Epoch 705 finished \tANN training loss 0.069818\n",
      ">> Epoch 706 finished \tANN training loss 0.077641\n",
      ">> Epoch 707 finished \tANN training loss 0.067412\n",
      ">> Epoch 708 finished \tANN training loss 0.091369\n",
      ">> Epoch 709 finished \tANN training loss 0.076250\n",
      ">> Epoch 710 finished \tANN training loss 0.069980\n",
      ">> Epoch 711 finished \tANN training loss 0.069185\n",
      ">> Epoch 712 finished \tANN training loss 0.080535\n",
      ">> Epoch 713 finished \tANN training loss 0.065764\n",
      ">> Epoch 714 finished \tANN training loss 0.067370\n",
      ">> Epoch 715 finished \tANN training loss 0.062491\n",
      ">> Epoch 716 finished \tANN training loss 0.078343\n",
      ">> Epoch 717 finished \tANN training loss 0.068570\n",
      ">> Epoch 718 finished \tANN training loss 0.068082\n",
      ">> Epoch 719 finished \tANN training loss 0.068164\n",
      ">> Epoch 720 finished \tANN training loss 0.072136\n",
      ">> Epoch 721 finished \tANN training loss 0.066840\n",
      ">> Epoch 722 finished \tANN training loss 0.068859\n",
      ">> Epoch 723 finished \tANN training loss 0.067075\n",
      ">> Epoch 724 finished \tANN training loss 0.062972\n",
      ">> Epoch 725 finished \tANN training loss 0.066737\n",
      ">> Epoch 726 finished \tANN training loss 0.073214\n",
      ">> Epoch 727 finished \tANN training loss 0.059755\n",
      ">> Epoch 728 finished \tANN training loss 0.062985\n",
      ">> Epoch 729 finished \tANN training loss 0.066229\n",
      ">> Epoch 730 finished \tANN training loss 0.062545\n",
      ">> Epoch 731 finished \tANN training loss 0.058784\n",
      ">> Epoch 732 finished \tANN training loss 0.070643\n",
      ">> Epoch 733 finished \tANN training loss 0.058048\n",
      ">> Epoch 734 finished \tANN training loss 0.063522\n",
      ">> Epoch 735 finished \tANN training loss 0.076838\n",
      ">> Epoch 736 finished \tANN training loss 0.072219\n",
      ">> Epoch 737 finished \tANN training loss 0.065585\n",
      ">> Epoch 738 finished \tANN training loss 0.067255\n",
      ">> Epoch 739 finished \tANN training loss 0.062787\n",
      ">> Epoch 740 finished \tANN training loss 0.063703\n",
      ">> Epoch 741 finished \tANN training loss 0.059509\n",
      ">> Epoch 742 finished \tANN training loss 0.056057\n",
      ">> Epoch 743 finished \tANN training loss 0.068981\n",
      ">> Epoch 744 finished \tANN training loss 0.064671\n",
      ">> Epoch 745 finished \tANN training loss 0.098399\n",
      ">> Epoch 746 finished \tANN training loss 0.089398\n",
      ">> Epoch 747 finished \tANN training loss 0.062060\n",
      ">> Epoch 748 finished \tANN training loss 0.061855\n",
      ">> Epoch 749 finished \tANN training loss 0.064945\n",
      ">> Epoch 750 finished \tANN training loss 0.069855\n",
      ">> Epoch 751 finished \tANN training loss 0.060188\n",
      ">> Epoch 752 finished \tANN training loss 0.058708\n",
      ">> Epoch 753 finished \tANN training loss 0.058340\n",
      ">> Epoch 754 finished \tANN training loss 0.059063\n",
      ">> Epoch 755 finished \tANN training loss 0.063261\n",
      ">> Epoch 756 finished \tANN training loss 0.063848\n",
      ">> Epoch 757 finished \tANN training loss 0.074126\n",
      ">> Epoch 758 finished \tANN training loss 0.074087\n",
      ">> Epoch 759 finished \tANN training loss 0.062462\n",
      ">> Epoch 760 finished \tANN training loss 0.060937\n",
      ">> Epoch 761 finished \tANN training loss 0.063070\n",
      ">> Epoch 762 finished \tANN training loss 0.058303\n",
      ">> Epoch 763 finished \tANN training loss 0.060415\n",
      ">> Epoch 764 finished \tANN training loss 0.062391\n",
      ">> Epoch 765 finished \tANN training loss 0.069783\n",
      ">> Epoch 766 finished \tANN training loss 0.062932\n",
      ">> Epoch 767 finished \tANN training loss 0.062397\n",
      ">> Epoch 768 finished \tANN training loss 0.065912\n",
      ">> Epoch 769 finished \tANN training loss 0.076919\n",
      ">> Epoch 770 finished \tANN training loss 0.073322\n",
      ">> Epoch 771 finished \tANN training loss 0.062627\n",
      ">> Epoch 772 finished \tANN training loss 0.081305\n",
      ">> Epoch 773 finished \tANN training loss 0.082194\n",
      ">> Epoch 774 finished \tANN training loss 0.066735\n",
      ">> Epoch 775 finished \tANN training loss 0.068540\n",
      ">> Epoch 776 finished \tANN training loss 0.070168\n",
      ">> Epoch 777 finished \tANN training loss 0.069899\n",
      ">> Epoch 778 finished \tANN training loss 0.063347\n",
      ">> Epoch 779 finished \tANN training loss 0.065855\n",
      ">> Epoch 780 finished \tANN training loss 0.063575\n",
      ">> Epoch 781 finished \tANN training loss 0.069866\n",
      ">> Epoch 782 finished \tANN training loss 0.062011\n",
      ">> Epoch 783 finished \tANN training loss 0.059098\n",
      ">> Epoch 784 finished \tANN training loss 0.062195\n",
      ">> Epoch 785 finished \tANN training loss 0.063319\n",
      ">> Epoch 786 finished \tANN training loss 0.063733\n",
      ">> Epoch 787 finished \tANN training loss 0.059738\n",
      ">> Epoch 788 finished \tANN training loss 0.058064\n",
      ">> Epoch 789 finished \tANN training loss 0.058550\n",
      ">> Epoch 790 finished \tANN training loss 0.061216\n",
      ">> Epoch 791 finished \tANN training loss 0.061429\n",
      ">> Epoch 792 finished \tANN training loss 0.071492\n",
      ">> Epoch 793 finished \tANN training loss 0.061746\n",
      ">> Epoch 794 finished \tANN training loss 0.060254\n",
      ">> Epoch 795 finished \tANN training loss 0.054815\n",
      ">> Epoch 796 finished \tANN training loss 0.053535\n",
      ">> Epoch 797 finished \tANN training loss 0.061698\n",
      ">> Epoch 798 finished \tANN training loss 0.055142\n",
      ">> Epoch 799 finished \tANN training loss 0.066035\n",
      ">> Epoch 800 finished \tANN training loss 0.058497\n",
      ">> Epoch 801 finished \tANN training loss 0.062795\n",
      ">> Epoch 802 finished \tANN training loss 0.053692\n",
      ">> Epoch 803 finished \tANN training loss 0.079020\n",
      ">> Epoch 804 finished \tANN training loss 0.069332\n",
      ">> Epoch 805 finished \tANN training loss 0.066536\n",
      ">> Epoch 806 finished \tANN training loss 0.053857\n",
      ">> Epoch 807 finished \tANN training loss 0.064110\n",
      ">> Epoch 808 finished \tANN training loss 0.060958\n",
      ">> Epoch 809 finished \tANN training loss 0.062170\n",
      ">> Epoch 810 finished \tANN training loss 0.061737\n",
      ">> Epoch 811 finished \tANN training loss 0.062797\n",
      ">> Epoch 812 finished \tANN training loss 0.061446\n",
      ">> Epoch 813 finished \tANN training loss 0.058526\n",
      ">> Epoch 814 finished \tANN training loss 0.056370\n",
      ">> Epoch 815 finished \tANN training loss 0.059437\n",
      ">> Epoch 816 finished \tANN training loss 0.060294\n",
      ">> Epoch 817 finished \tANN training loss 0.062468\n",
      ">> Epoch 818 finished \tANN training loss 0.060050\n",
      ">> Epoch 819 finished \tANN training loss 0.062994\n",
      ">> Epoch 820 finished \tANN training loss 0.061056\n",
      ">> Epoch 821 finished \tANN training loss 0.057870\n",
      ">> Epoch 822 finished \tANN training loss 0.056814\n",
      ">> Epoch 823 finished \tANN training loss 0.054516\n",
      ">> Epoch 824 finished \tANN training loss 0.055733\n",
      ">> Epoch 825 finished \tANN training loss 0.071802\n",
      ">> Epoch 826 finished \tANN training loss 0.065902\n",
      ">> Epoch 827 finished \tANN training loss 0.054155\n",
      ">> Epoch 828 finished \tANN training loss 0.060610\n",
      ">> Epoch 829 finished \tANN training loss 0.075831\n",
      ">> Epoch 830 finished \tANN training loss 0.066682\n",
      ">> Epoch 831 finished \tANN training loss 0.061512\n",
      ">> Epoch 832 finished \tANN training loss 0.067569\n",
      ">> Epoch 833 finished \tANN training loss 0.073672\n",
      ">> Epoch 834 finished \tANN training loss 0.077963\n",
      ">> Epoch 835 finished \tANN training loss 0.062119\n",
      ">> Epoch 836 finished \tANN training loss 0.071716\n",
      ">> Epoch 837 finished \tANN training loss 0.056843\n",
      ">> Epoch 838 finished \tANN training loss 0.057154\n",
      ">> Epoch 839 finished \tANN training loss 0.052719\n",
      ">> Epoch 840 finished \tANN training loss 0.060323\n",
      ">> Epoch 841 finished \tANN training loss 0.063019\n",
      ">> Epoch 842 finished \tANN training loss 0.056864\n",
      ">> Epoch 843 finished \tANN training loss 0.056017\n",
      ">> Epoch 844 finished \tANN training loss 0.052865\n",
      ">> Epoch 845 finished \tANN training loss 0.054477\n",
      ">> Epoch 846 finished \tANN training loss 0.059621\n",
      ">> Epoch 847 finished \tANN training loss 0.083223\n",
      ">> Epoch 848 finished \tANN training loss 0.060154\n",
      ">> Epoch 849 finished \tANN training loss 0.064258\n",
      ">> Epoch 850 finished \tANN training loss 0.060908\n",
      ">> Epoch 851 finished \tANN training loss 0.056198\n",
      ">> Epoch 852 finished \tANN training loss 0.071942\n",
      ">> Epoch 853 finished \tANN training loss 0.055927\n",
      ">> Epoch 854 finished \tANN training loss 0.063539\n",
      ">> Epoch 855 finished \tANN training loss 0.055463\n",
      ">> Epoch 856 finished \tANN training loss 0.057743\n",
      ">> Epoch 857 finished \tANN training loss 0.057220\n",
      ">> Epoch 858 finished \tANN training loss 0.061205\n",
      ">> Epoch 859 finished \tANN training loss 0.058846\n",
      ">> Epoch 860 finished \tANN training loss 0.064769\n",
      ">> Epoch 861 finished \tANN training loss 0.056576\n",
      ">> Epoch 862 finished \tANN training loss 0.066077\n",
      ">> Epoch 863 finished \tANN training loss 0.064309\n",
      ">> Epoch 864 finished \tANN training loss 0.059653\n",
      ">> Epoch 865 finished \tANN training loss 0.065229\n",
      ">> Epoch 866 finished \tANN training loss 0.066966\n",
      ">> Epoch 867 finished \tANN training loss 0.053542\n",
      ">> Epoch 868 finished \tANN training loss 0.054843\n",
      ">> Epoch 869 finished \tANN training loss 0.057741\n",
      ">> Epoch 870 finished \tANN training loss 0.063578\n",
      ">> Epoch 871 finished \tANN training loss 0.053217\n",
      ">> Epoch 872 finished \tANN training loss 0.062031\n",
      ">> Epoch 873 finished \tANN training loss 0.063281\n",
      ">> Epoch 874 finished \tANN training loss 0.058723\n",
      ">> Epoch 875 finished \tANN training loss 0.056253\n",
      ">> Epoch 876 finished \tANN training loss 0.060719\n",
      ">> Epoch 877 finished \tANN training loss 0.053941\n",
      ">> Epoch 878 finished \tANN training loss 0.061411\n",
      ">> Epoch 879 finished \tANN training loss 0.055378\n",
      ">> Epoch 880 finished \tANN training loss 0.058152\n",
      ">> Epoch 881 finished \tANN training loss 0.064402\n",
      ">> Epoch 882 finished \tANN training loss 0.058755\n",
      ">> Epoch 883 finished \tANN training loss 0.056230\n",
      ">> Epoch 884 finished \tANN training loss 0.066705\n",
      ">> Epoch 885 finished \tANN training loss 0.059427\n",
      ">> Epoch 886 finished \tANN training loss 0.061951\n",
      ">> Epoch 887 finished \tANN training loss 0.057073\n",
      ">> Epoch 888 finished \tANN training loss 0.070212\n",
      ">> Epoch 889 finished \tANN training loss 0.061221\n",
      ">> Epoch 890 finished \tANN training loss 0.056384\n",
      ">> Epoch 891 finished \tANN training loss 0.053490\n",
      ">> Epoch 892 finished \tANN training loss 0.055836\n",
      ">> Epoch 893 finished \tANN training loss 0.057673\n",
      ">> Epoch 894 finished \tANN training loss 0.051786\n",
      ">> Epoch 895 finished \tANN training loss 0.056146\n",
      ">> Epoch 896 finished \tANN training loss 0.050456\n",
      ">> Epoch 897 finished \tANN training loss 0.052469\n",
      ">> Epoch 898 finished \tANN training loss 0.056395\n",
      ">> Epoch 899 finished \tANN training loss 0.054478\n",
      ">> Epoch 900 finished \tANN training loss 0.056258\n",
      ">> Epoch 901 finished \tANN training loss 0.060972\n",
      ">> Epoch 902 finished \tANN training loss 0.057089\n",
      ">> Epoch 903 finished \tANN training loss 0.061531\n",
      ">> Epoch 904 finished \tANN training loss 0.054463\n",
      ">> Epoch 905 finished \tANN training loss 0.064554\n",
      ">> Epoch 906 finished \tANN training loss 0.057923\n",
      ">> Epoch 907 finished \tANN training loss 0.056559\n",
      ">> Epoch 908 finished \tANN training loss 0.055143\n",
      ">> Epoch 909 finished \tANN training loss 0.053859\n",
      ">> Epoch 910 finished \tANN training loss 0.057379\n",
      ">> Epoch 911 finished \tANN training loss 0.055494\n",
      ">> Epoch 912 finished \tANN training loss 0.056744\n",
      ">> Epoch 913 finished \tANN training loss 0.058022\n",
      ">> Epoch 914 finished \tANN training loss 0.058841\n",
      ">> Epoch 915 finished \tANN training loss 0.053429\n",
      ">> Epoch 916 finished \tANN training loss 0.053942\n",
      ">> Epoch 917 finished \tANN training loss 0.055747\n",
      ">> Epoch 918 finished \tANN training loss 0.052774\n",
      ">> Epoch 919 finished \tANN training loss 0.052395\n",
      ">> Epoch 920 finished \tANN training loss 0.051381\n",
      ">> Epoch 921 finished \tANN training loss 0.051017\n",
      ">> Epoch 922 finished \tANN training loss 0.056724\n",
      ">> Epoch 923 finished \tANN training loss 0.057405\n",
      ">> Epoch 924 finished \tANN training loss 0.067671\n",
      ">> Epoch 925 finished \tANN training loss 0.059872\n",
      ">> Epoch 926 finished \tANN training loss 0.054629\n",
      ">> Epoch 927 finished \tANN training loss 0.051526\n",
      ">> Epoch 928 finished \tANN training loss 0.051603\n",
      ">> Epoch 929 finished \tANN training loss 0.062149\n",
      ">> Epoch 930 finished \tANN training loss 0.059213\n",
      ">> Epoch 931 finished \tANN training loss 0.052359\n",
      ">> Epoch 932 finished \tANN training loss 0.055020\n",
      ">> Epoch 933 finished \tANN training loss 0.056728\n",
      ">> Epoch 934 finished \tANN training loss 0.056192\n",
      ">> Epoch 935 finished \tANN training loss 0.054259\n",
      ">> Epoch 936 finished \tANN training loss 0.054109\n",
      ">> Epoch 937 finished \tANN training loss 0.062837\n",
      ">> Epoch 938 finished \tANN training loss 0.052086\n",
      ">> Epoch 939 finished \tANN training loss 0.052596\n",
      ">> Epoch 940 finished \tANN training loss 0.052155\n",
      ">> Epoch 941 finished \tANN training loss 0.066538\n",
      ">> Epoch 942 finished \tANN training loss 0.054558\n",
      ">> Epoch 943 finished \tANN training loss 0.058969\n",
      ">> Epoch 944 finished \tANN training loss 0.057545\n",
      ">> Epoch 945 finished \tANN training loss 0.059556\n",
      ">> Epoch 946 finished \tANN training loss 0.053712\n",
      ">> Epoch 947 finished \tANN training loss 0.055316\n",
      ">> Epoch 948 finished \tANN training loss 0.056519\n",
      ">> Epoch 949 finished \tANN training loss 0.055745\n",
      ">> Epoch 950 finished \tANN training loss 0.052653\n",
      ">> Epoch 951 finished \tANN training loss 0.055422\n",
      ">> Epoch 952 finished \tANN training loss 0.056223\n",
      ">> Epoch 953 finished \tANN training loss 0.054677\n",
      ">> Epoch 954 finished \tANN training loss 0.056875\n",
      ">> Epoch 955 finished \tANN training loss 0.056840\n",
      ">> Epoch 956 finished \tANN training loss 0.055509\n",
      ">> Epoch 957 finished \tANN training loss 0.050805\n",
      ">> Epoch 958 finished \tANN training loss 0.052636\n",
      ">> Epoch 959 finished \tANN training loss 0.060217\n",
      ">> Epoch 960 finished \tANN training loss 0.053093\n",
      ">> Epoch 961 finished \tANN training loss 0.075712\n",
      ">> Epoch 962 finished \tANN training loss 0.049887\n",
      ">> Epoch 963 finished \tANN training loss 0.060185\n",
      ">> Epoch 964 finished \tANN training loss 0.055352\n",
      ">> Epoch 965 finished \tANN training loss 0.056082\n",
      ">> Epoch 966 finished \tANN training loss 0.055782\n",
      ">> Epoch 967 finished \tANN training loss 0.051764\n",
      ">> Epoch 968 finished \tANN training loss 0.046998\n",
      ">> Epoch 969 finished \tANN training loss 0.051287\n",
      ">> Epoch 970 finished \tANN training loss 0.058017\n",
      ">> Epoch 971 finished \tANN training loss 0.055714\n",
      ">> Epoch 972 finished \tANN training loss 0.055254\n",
      ">> Epoch 973 finished \tANN training loss 0.051108\n",
      ">> Epoch 974 finished \tANN training loss 0.062132\n",
      ">> Epoch 975 finished \tANN training loss 0.057860\n",
      ">> Epoch 976 finished \tANN training loss 0.052730\n",
      ">> Epoch 977 finished \tANN training loss 0.050629\n",
      ">> Epoch 978 finished \tANN training loss 0.060546\n",
      ">> Epoch 979 finished \tANN training loss 0.058994\n",
      ">> Epoch 980 finished \tANN training loss 0.054909\n",
      ">> Epoch 981 finished \tANN training loss 0.061501\n",
      ">> Epoch 982 finished \tANN training loss 0.052955\n",
      ">> Epoch 983 finished \tANN training loss 0.052394\n",
      ">> Epoch 984 finished \tANN training loss 0.056280\n",
      ">> Epoch 985 finished \tANN training loss 0.056879\n",
      ">> Epoch 986 finished \tANN training loss 0.054741\n",
      ">> Epoch 987 finished \tANN training loss 0.050113\n",
      ">> Epoch 988 finished \tANN training loss 0.056654\n",
      ">> Epoch 989 finished \tANN training loss 0.059541\n",
      ">> Epoch 990 finished \tANN training loss 0.059929\n",
      ">> Epoch 991 finished \tANN training loss 0.069844\n",
      ">> Epoch 992 finished \tANN training loss 0.059872\n",
      ">> Epoch 993 finished \tANN training loss 0.072665\n",
      ">> Epoch 994 finished \tANN training loss 0.059163\n",
      ">> Epoch 995 finished \tANN training loss 0.053212\n",
      ">> Epoch 996 finished \tANN training loss 0.073767\n",
      ">> Epoch 997 finished \tANN training loss 0.062861\n",
      ">> Epoch 998 finished \tANN training loss 0.058007\n",
      ">> Epoch 999 finished \tANN training loss 0.055924\n",
      "[END] Fine tuning step\n",
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 10.032800\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 13.166056\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 13.305145\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 12.503866\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 11.842634\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 11.879367\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 12.444973\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 11.452618\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 12.493850\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 11.382206\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 10.515911\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 10.391162\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 10.390066\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 10.026638\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 9.945005\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 8.791968\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 9.257922\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 9.314695\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 8.783711\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 9.764697\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 16.464731\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 19.473833\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 19.518686\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 17.632826\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 21.063391\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 19.495432\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 17.718208\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 18.851074\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 19.719761\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 19.043720\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 18.404055\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 19.263062\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 19.126789\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 18.047499\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 20.076647\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 18.224705\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 19.572336\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 19.712139\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 20.183605\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 19.028700\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.921770\n",
      ">> Epoch 1 finished \tANN training loss 0.855633\n",
      ">> Epoch 2 finished \tANN training loss 0.787456\n",
      ">> Epoch 3 finished \tANN training loss 0.764194\n",
      ">> Epoch 4 finished \tANN training loss 0.704448\n",
      ">> Epoch 5 finished \tANN training loss 0.692082\n",
      ">> Epoch 6 finished \tANN training loss 0.651149\n",
      ">> Epoch 7 finished \tANN training loss 0.623760\n",
      ">> Epoch 8 finished \tANN training loss 0.625189\n",
      ">> Epoch 9 finished \tANN training loss 0.570467\n",
      ">> Epoch 10 finished \tANN training loss 0.555130\n",
      ">> Epoch 11 finished \tANN training loss 0.521277\n",
      ">> Epoch 12 finished \tANN training loss 0.510160\n",
      ">> Epoch 13 finished \tANN training loss 0.499186\n",
      ">> Epoch 14 finished \tANN training loss 0.462523\n",
      ">> Epoch 15 finished \tANN training loss 0.466494\n",
      ">> Epoch 16 finished \tANN training loss 0.465128\n",
      ">> Epoch 17 finished \tANN training loss 0.444717\n",
      ">> Epoch 18 finished \tANN training loss 0.445053\n",
      ">> Epoch 19 finished \tANN training loss 0.424005\n",
      ">> Epoch 20 finished \tANN training loss 0.413969\n",
      ">> Epoch 21 finished \tANN training loss 0.413991\n",
      ">> Epoch 22 finished \tANN training loss 0.398617\n",
      ">> Epoch 23 finished \tANN training loss 0.399245\n",
      ">> Epoch 24 finished \tANN training loss 0.386517\n",
      ">> Epoch 25 finished \tANN training loss 0.388052\n",
      ">> Epoch 26 finished \tANN training loss 0.397100\n",
      ">> Epoch 27 finished \tANN training loss 0.362839\n",
      ">> Epoch 28 finished \tANN training loss 0.359596\n",
      ">> Epoch 29 finished \tANN training loss 0.363674\n",
      ">> Epoch 30 finished \tANN training loss 0.361140\n",
      ">> Epoch 31 finished \tANN training loss 0.351768\n",
      ">> Epoch 32 finished \tANN training loss 0.357607\n",
      ">> Epoch 33 finished \tANN training loss 0.344988\n",
      ">> Epoch 34 finished \tANN training loss 0.339089\n",
      ">> Epoch 35 finished \tANN training loss 0.325806\n",
      ">> Epoch 36 finished \tANN training loss 0.312452\n",
      ">> Epoch 37 finished \tANN training loss 0.303526\n",
      ">> Epoch 38 finished \tANN training loss 0.298606\n",
      ">> Epoch 39 finished \tANN training loss 0.301591\n",
      ">> Epoch 40 finished \tANN training loss 0.332760\n",
      ">> Epoch 41 finished \tANN training loss 0.282270\n",
      ">> Epoch 42 finished \tANN training loss 0.303597\n",
      ">> Epoch 43 finished \tANN training loss 0.290656\n",
      ">> Epoch 44 finished \tANN training loss 0.249338\n",
      ">> Epoch 45 finished \tANN training loss 0.261117\n",
      ">> Epoch 46 finished \tANN training loss 0.258454\n",
      ">> Epoch 47 finished \tANN training loss 0.267521\n",
      ">> Epoch 48 finished \tANN training loss 0.255146\n",
      ">> Epoch 49 finished \tANN training loss 0.245062\n",
      ">> Epoch 50 finished \tANN training loss 0.256969\n",
      ">> Epoch 51 finished \tANN training loss 0.249922\n",
      ">> Epoch 52 finished \tANN training loss 0.246493\n",
      ">> Epoch 53 finished \tANN training loss 0.241823\n",
      ">> Epoch 54 finished \tANN training loss 0.269984\n",
      ">> Epoch 55 finished \tANN training loss 0.243125\n",
      ">> Epoch 56 finished \tANN training loss 0.245144\n",
      ">> Epoch 57 finished \tANN training loss 0.231202\n",
      ">> Epoch 58 finished \tANN training loss 0.214457\n",
      ">> Epoch 59 finished \tANN training loss 0.220838\n",
      ">> Epoch 60 finished \tANN training loss 0.231391\n",
      ">> Epoch 61 finished \tANN training loss 0.207643\n",
      ">> Epoch 62 finished \tANN training loss 0.198505\n",
      ">> Epoch 63 finished \tANN training loss 0.223152\n",
      ">> Epoch 64 finished \tANN training loss 0.222234\n",
      ">> Epoch 65 finished \tANN training loss 0.214558\n",
      ">> Epoch 66 finished \tANN training loss 0.178017\n",
      ">> Epoch 67 finished \tANN training loss 0.219209\n",
      ">> Epoch 68 finished \tANN training loss 0.194318\n",
      ">> Epoch 69 finished \tANN training loss 0.234704\n",
      ">> Epoch 70 finished \tANN training loss 0.203609\n",
      ">> Epoch 71 finished \tANN training loss 0.204016\n",
      ">> Epoch 72 finished \tANN training loss 0.186648\n",
      ">> Epoch 73 finished \tANN training loss 0.201064\n",
      ">> Epoch 74 finished \tANN training loss 0.173212\n",
      ">> Epoch 75 finished \tANN training loss 0.223396\n",
      ">> Epoch 76 finished \tANN training loss 0.184616\n",
      ">> Epoch 77 finished \tANN training loss 0.194110\n",
      ">> Epoch 78 finished \tANN training loss 0.192254\n",
      ">> Epoch 79 finished \tANN training loss 0.184710\n",
      ">> Epoch 80 finished \tANN training loss 0.181103\n",
      ">> Epoch 81 finished \tANN training loss 0.202033\n",
      ">> Epoch 82 finished \tANN training loss 0.173750\n",
      ">> Epoch 83 finished \tANN training loss 0.164655\n",
      ">> Epoch 84 finished \tANN training loss 0.170754\n",
      ">> Epoch 85 finished \tANN training loss 0.165532\n",
      ">> Epoch 86 finished \tANN training loss 0.159764\n",
      ">> Epoch 87 finished \tANN training loss 0.198186\n",
      ">> Epoch 88 finished \tANN training loss 0.171232\n",
      ">> Epoch 89 finished \tANN training loss 0.175092\n",
      ">> Epoch 90 finished \tANN training loss 0.166601\n",
      ">> Epoch 91 finished \tANN training loss 0.156715\n",
      ">> Epoch 92 finished \tANN training loss 0.191511\n",
      ">> Epoch 93 finished \tANN training loss 0.168209\n",
      ">> Epoch 94 finished \tANN training loss 0.187699\n",
      ">> Epoch 95 finished \tANN training loss 0.160687\n",
      ">> Epoch 96 finished \tANN training loss 0.193116\n",
      ">> Epoch 97 finished \tANN training loss 0.173453\n",
      ">> Epoch 98 finished \tANN training loss 0.192215\n",
      ">> Epoch 99 finished \tANN training loss 0.166431\n",
      ">> Epoch 100 finished \tANN training loss 0.161331\n",
      ">> Epoch 101 finished \tANN training loss 0.175067\n",
      ">> Epoch 102 finished \tANN training loss 0.157239\n",
      ">> Epoch 103 finished \tANN training loss 0.152185\n",
      ">> Epoch 104 finished \tANN training loss 0.146614\n",
      ">> Epoch 105 finished \tANN training loss 0.164700\n",
      ">> Epoch 106 finished \tANN training loss 0.159642\n",
      ">> Epoch 107 finished \tANN training loss 0.155320\n",
      ">> Epoch 108 finished \tANN training loss 0.167917\n",
      ">> Epoch 109 finished \tANN training loss 0.147047\n",
      ">> Epoch 110 finished \tANN training loss 0.168942\n",
      ">> Epoch 111 finished \tANN training loss 0.148958\n",
      ">> Epoch 112 finished \tANN training loss 0.141189\n",
      ">> Epoch 113 finished \tANN training loss 0.147062\n",
      ">> Epoch 114 finished \tANN training loss 0.143766\n",
      ">> Epoch 115 finished \tANN training loss 0.166553\n",
      ">> Epoch 116 finished \tANN training loss 0.166674\n",
      ">> Epoch 117 finished \tANN training loss 0.141050\n",
      ">> Epoch 118 finished \tANN training loss 0.132766\n",
      ">> Epoch 119 finished \tANN training loss 0.148212\n",
      ">> Epoch 120 finished \tANN training loss 0.169275\n",
      ">> Epoch 121 finished \tANN training loss 0.154376\n",
      ">> Epoch 122 finished \tANN training loss 0.178626\n",
      ">> Epoch 123 finished \tANN training loss 0.166620\n",
      ">> Epoch 124 finished \tANN training loss 0.142174\n",
      ">> Epoch 125 finished \tANN training loss 0.140264\n",
      ">> Epoch 126 finished \tANN training loss 0.151512\n",
      ">> Epoch 127 finished \tANN training loss 0.164790\n",
      ">> Epoch 128 finished \tANN training loss 0.157437\n",
      ">> Epoch 129 finished \tANN training loss 0.145088\n",
      ">> Epoch 130 finished \tANN training loss 0.135883\n",
      ">> Epoch 131 finished \tANN training loss 0.144401\n",
      ">> Epoch 132 finished \tANN training loss 0.179859\n",
      ">> Epoch 133 finished \tANN training loss 0.132677\n",
      ">> Epoch 134 finished \tANN training loss 0.122916\n",
      ">> Epoch 135 finished \tANN training loss 0.153779\n",
      ">> Epoch 136 finished \tANN training loss 0.130884\n",
      ">> Epoch 137 finished \tANN training loss 0.151889\n",
      ">> Epoch 138 finished \tANN training loss 0.147088\n",
      ">> Epoch 139 finished \tANN training loss 0.121023\n",
      ">> Epoch 140 finished \tANN training loss 0.135853\n",
      ">> Epoch 141 finished \tANN training loss 0.138846\n",
      ">> Epoch 142 finished \tANN training loss 0.150568\n",
      ">> Epoch 143 finished \tANN training loss 0.137007\n",
      ">> Epoch 144 finished \tANN training loss 0.136980\n",
      ">> Epoch 145 finished \tANN training loss 0.128559\n",
      ">> Epoch 146 finished \tANN training loss 0.130875\n",
      ">> Epoch 147 finished \tANN training loss 0.131201\n",
      ">> Epoch 148 finished \tANN training loss 0.133681\n",
      ">> Epoch 149 finished \tANN training loss 0.125948\n",
      ">> Epoch 150 finished \tANN training loss 0.155620\n",
      ">> Epoch 151 finished \tANN training loss 0.143169\n",
      ">> Epoch 152 finished \tANN training loss 0.133471\n",
      ">> Epoch 153 finished \tANN training loss 0.134442\n",
      ">> Epoch 154 finished \tANN training loss 0.126005\n",
      ">> Epoch 155 finished \tANN training loss 0.137517\n",
      ">> Epoch 156 finished \tANN training loss 0.132603\n",
      ">> Epoch 157 finished \tANN training loss 0.110876\n",
      ">> Epoch 158 finished \tANN training loss 0.126254\n",
      ">> Epoch 159 finished \tANN training loss 0.126998\n",
      ">> Epoch 160 finished \tANN training loss 0.161381\n",
      ">> Epoch 161 finished \tANN training loss 0.125493\n",
      ">> Epoch 162 finished \tANN training loss 0.123609\n",
      ">> Epoch 163 finished \tANN training loss 0.154829\n",
      ">> Epoch 164 finished \tANN training loss 0.195984\n",
      ">> Epoch 165 finished \tANN training loss 0.122907\n",
      ">> Epoch 166 finished \tANN training loss 0.122857\n",
      ">> Epoch 167 finished \tANN training loss 0.118468\n",
      ">> Epoch 168 finished \tANN training loss 0.112148\n",
      ">> Epoch 169 finished \tANN training loss 0.128286\n",
      ">> Epoch 170 finished \tANN training loss 0.113374\n",
      ">> Epoch 171 finished \tANN training loss 0.131754\n",
      ">> Epoch 172 finished \tANN training loss 0.123866\n",
      ">> Epoch 173 finished \tANN training loss 0.115665\n",
      ">> Epoch 174 finished \tANN training loss 0.114993\n",
      ">> Epoch 175 finished \tANN training loss 0.129403\n",
      ">> Epoch 176 finished \tANN training loss 0.120094\n",
      ">> Epoch 177 finished \tANN training loss 0.112965\n",
      ">> Epoch 178 finished \tANN training loss 0.130224\n",
      ">> Epoch 179 finished \tANN training loss 0.120531\n",
      ">> Epoch 180 finished \tANN training loss 0.124685\n",
      ">> Epoch 181 finished \tANN training loss 0.134679\n",
      ">> Epoch 182 finished \tANN training loss 0.112968\n",
      ">> Epoch 183 finished \tANN training loss 0.124007\n",
      ">> Epoch 184 finished \tANN training loss 0.129866\n",
      ">> Epoch 185 finished \tANN training loss 0.117750\n",
      ">> Epoch 186 finished \tANN training loss 0.113444\n",
      ">> Epoch 187 finished \tANN training loss 0.110174\n",
      ">> Epoch 188 finished \tANN training loss 0.105476\n",
      ">> Epoch 189 finished \tANN training loss 0.134834\n",
      ">> Epoch 190 finished \tANN training loss 0.124769\n",
      ">> Epoch 191 finished \tANN training loss 0.130830\n",
      ">> Epoch 192 finished \tANN training loss 0.114704\n",
      ">> Epoch 193 finished \tANN training loss 0.115567\n",
      ">> Epoch 194 finished \tANN training loss 0.107816\n",
      ">> Epoch 195 finished \tANN training loss 0.114110\n",
      ">> Epoch 196 finished \tANN training loss 0.106469\n",
      ">> Epoch 197 finished \tANN training loss 0.113332\n",
      ">> Epoch 198 finished \tANN training loss 0.129646\n",
      ">> Epoch 199 finished \tANN training loss 0.101643\n",
      ">> Epoch 200 finished \tANN training loss 0.131402\n",
      ">> Epoch 201 finished \tANN training loss 0.095368\n",
      ">> Epoch 202 finished \tANN training loss 0.100117\n",
      ">> Epoch 203 finished \tANN training loss 0.113071\n",
      ">> Epoch 204 finished \tANN training loss 0.113482\n",
      ">> Epoch 205 finished \tANN training loss 0.102639\n",
      ">> Epoch 206 finished \tANN training loss 0.105436\n",
      ">> Epoch 207 finished \tANN training loss 0.110755\n",
      ">> Epoch 208 finished \tANN training loss 0.111394\n",
      ">> Epoch 209 finished \tANN training loss 0.110349\n",
      ">> Epoch 210 finished \tANN training loss 0.123207\n",
      ">> Epoch 211 finished \tANN training loss 0.140859\n",
      ">> Epoch 212 finished \tANN training loss 0.114131\n",
      ">> Epoch 213 finished \tANN training loss 0.131201\n",
      ">> Epoch 214 finished \tANN training loss 0.120131\n",
      ">> Epoch 215 finished \tANN training loss 0.109402\n",
      ">> Epoch 216 finished \tANN training loss 0.114607\n",
      ">> Epoch 217 finished \tANN training loss 0.113016\n",
      ">> Epoch 218 finished \tANN training loss 0.105808\n",
      ">> Epoch 219 finished \tANN training loss 0.117140\n",
      ">> Epoch 220 finished \tANN training loss 0.130399\n",
      ">> Epoch 221 finished \tANN training loss 0.113803\n",
      ">> Epoch 222 finished \tANN training loss 0.129359\n",
      ">> Epoch 223 finished \tANN training loss 0.104643\n",
      ">> Epoch 224 finished \tANN training loss 0.140088\n",
      ">> Epoch 225 finished \tANN training loss 0.110661\n",
      ">> Epoch 226 finished \tANN training loss 0.106755\n",
      ">> Epoch 227 finished \tANN training loss 0.106589\n",
      ">> Epoch 228 finished \tANN training loss 0.121917\n",
      ">> Epoch 229 finished \tANN training loss 0.105759\n",
      ">> Epoch 230 finished \tANN training loss 0.097548\n",
      ">> Epoch 231 finished \tANN training loss 0.104108\n",
      ">> Epoch 232 finished \tANN training loss 0.103155\n",
      ">> Epoch 233 finished \tANN training loss 0.113870\n",
      ">> Epoch 234 finished \tANN training loss 0.113349\n",
      ">> Epoch 235 finished \tANN training loss 0.112383\n",
      ">> Epoch 236 finished \tANN training loss 0.101427\n",
      ">> Epoch 237 finished \tANN training loss 0.095892\n",
      ">> Epoch 238 finished \tANN training loss 0.104769\n",
      ">> Epoch 239 finished \tANN training loss 0.092350\n",
      ">> Epoch 240 finished \tANN training loss 0.100832\n",
      ">> Epoch 241 finished \tANN training loss 0.093660\n",
      ">> Epoch 242 finished \tANN training loss 0.102998\n",
      ">> Epoch 243 finished \tANN training loss 0.114256\n",
      ">> Epoch 244 finished \tANN training loss 0.097778\n",
      ">> Epoch 245 finished \tANN training loss 0.093678\n",
      ">> Epoch 246 finished \tANN training loss 0.093144\n",
      ">> Epoch 247 finished \tANN training loss 0.091941\n",
      ">> Epoch 248 finished \tANN training loss 0.095017\n",
      ">> Epoch 249 finished \tANN training loss 0.088350\n",
      ">> Epoch 250 finished \tANN training loss 0.101717\n",
      ">> Epoch 251 finished \tANN training loss 0.114133\n",
      ">> Epoch 252 finished \tANN training loss 0.125496\n",
      ">> Epoch 253 finished \tANN training loss 0.105967\n",
      ">> Epoch 254 finished \tANN training loss 0.105439\n",
      ">> Epoch 255 finished \tANN training loss 0.106500\n",
      ">> Epoch 256 finished \tANN training loss 0.112936\n",
      ">> Epoch 257 finished \tANN training loss 0.103026\n",
      ">> Epoch 258 finished \tANN training loss 0.099586\n",
      ">> Epoch 259 finished \tANN training loss 0.121121\n",
      ">> Epoch 260 finished \tANN training loss 0.106601\n",
      ">> Epoch 261 finished \tANN training loss 0.126509\n",
      ">> Epoch 262 finished \tANN training loss 0.092710\n",
      ">> Epoch 263 finished \tANN training loss 0.122424\n",
      ">> Epoch 264 finished \tANN training loss 0.098301\n",
      ">> Epoch 265 finished \tANN training loss 0.110174\n",
      ">> Epoch 266 finished \tANN training loss 0.122086\n",
      ">> Epoch 267 finished \tANN training loss 0.108543\n",
      ">> Epoch 268 finished \tANN training loss 0.148189\n",
      ">> Epoch 269 finished \tANN training loss 0.102154\n",
      ">> Epoch 270 finished \tANN training loss 0.100341\n",
      ">> Epoch 271 finished \tANN training loss 0.101827\n",
      ">> Epoch 272 finished \tANN training loss 0.092655\n",
      ">> Epoch 273 finished \tANN training loss 0.096108\n",
      ">> Epoch 274 finished \tANN training loss 0.082460\n",
      ">> Epoch 275 finished \tANN training loss 0.092675\n",
      ">> Epoch 276 finished \tANN training loss 0.112075\n",
      ">> Epoch 277 finished \tANN training loss 0.125553\n",
      ">> Epoch 278 finished \tANN training loss 0.097452\n",
      ">> Epoch 279 finished \tANN training loss 0.106272\n",
      ">> Epoch 280 finished \tANN training loss 0.102947\n",
      ">> Epoch 281 finished \tANN training loss 0.096239\n",
      ">> Epoch 282 finished \tANN training loss 0.102714\n",
      ">> Epoch 283 finished \tANN training loss 0.095149\n",
      ">> Epoch 284 finished \tANN training loss 0.091862\n",
      ">> Epoch 285 finished \tANN training loss 0.104657\n",
      ">> Epoch 286 finished \tANN training loss 0.095253\n",
      ">> Epoch 287 finished \tANN training loss 0.093676\n",
      ">> Epoch 288 finished \tANN training loss 0.087240\n",
      ">> Epoch 289 finished \tANN training loss 0.095032\n",
      ">> Epoch 290 finished \tANN training loss 0.106803\n",
      ">> Epoch 291 finished \tANN training loss 0.092346\n",
      ">> Epoch 292 finished \tANN training loss 0.105605\n",
      ">> Epoch 293 finished \tANN training loss 0.109978\n",
      ">> Epoch 294 finished \tANN training loss 0.105658\n",
      ">> Epoch 295 finished \tANN training loss 0.100721\n",
      ">> Epoch 296 finished \tANN training loss 0.093419\n",
      ">> Epoch 297 finished \tANN training loss 0.095524\n",
      ">> Epoch 298 finished \tANN training loss 0.083004\n",
      ">> Epoch 299 finished \tANN training loss 0.084153\n",
      ">> Epoch 300 finished \tANN training loss 0.089647\n",
      ">> Epoch 301 finished \tANN training loss 0.089564\n",
      ">> Epoch 302 finished \tANN training loss 0.094591\n",
      ">> Epoch 303 finished \tANN training loss 0.101618\n",
      ">> Epoch 304 finished \tANN training loss 0.094456\n",
      ">> Epoch 305 finished \tANN training loss 0.097448\n",
      ">> Epoch 306 finished \tANN training loss 0.103118\n",
      ">> Epoch 307 finished \tANN training loss 0.096507\n",
      ">> Epoch 308 finished \tANN training loss 0.112350\n",
      ">> Epoch 309 finished \tANN training loss 0.097039\n",
      ">> Epoch 310 finished \tANN training loss 0.096603\n",
      ">> Epoch 311 finished \tANN training loss 0.098011\n",
      ">> Epoch 312 finished \tANN training loss 0.091757\n",
      ">> Epoch 313 finished \tANN training loss 0.092493\n",
      ">> Epoch 314 finished \tANN training loss 0.088742\n",
      ">> Epoch 315 finished \tANN training loss 0.083497\n",
      ">> Epoch 316 finished \tANN training loss 0.100700\n",
      ">> Epoch 317 finished \tANN training loss 0.094640\n",
      ">> Epoch 318 finished \tANN training loss 0.091470\n",
      ">> Epoch 319 finished \tANN training loss 0.098597\n",
      ">> Epoch 320 finished \tANN training loss 0.083401\n",
      ">> Epoch 321 finished \tANN training loss 0.091715\n",
      ">> Epoch 322 finished \tANN training loss 0.097391\n",
      ">> Epoch 323 finished \tANN training loss 0.089250\n",
      ">> Epoch 324 finished \tANN training loss 0.101781\n",
      ">> Epoch 325 finished \tANN training loss 0.092203\n",
      ">> Epoch 326 finished \tANN training loss 0.106852\n",
      ">> Epoch 327 finished \tANN training loss 0.105740\n",
      ">> Epoch 328 finished \tANN training loss 0.080235\n",
      ">> Epoch 329 finished \tANN training loss 0.097113\n",
      ">> Epoch 330 finished \tANN training loss 0.084347\n",
      ">> Epoch 331 finished \tANN training loss 0.081193\n",
      ">> Epoch 332 finished \tANN training loss 0.083615\n",
      ">> Epoch 333 finished \tANN training loss 0.082318\n",
      ">> Epoch 334 finished \tANN training loss 0.098183\n",
      ">> Epoch 335 finished \tANN training loss 0.088877\n",
      ">> Epoch 336 finished \tANN training loss 0.128983\n",
      ">> Epoch 337 finished \tANN training loss 0.123388\n",
      ">> Epoch 338 finished \tANN training loss 0.108226\n",
      ">> Epoch 339 finished \tANN training loss 0.102750\n",
      ">> Epoch 340 finished \tANN training loss 0.105720\n",
      ">> Epoch 341 finished \tANN training loss 0.090765\n",
      ">> Epoch 342 finished \tANN training loss 0.097011\n",
      ">> Epoch 343 finished \tANN training loss 0.092458\n",
      ">> Epoch 344 finished \tANN training loss 0.099284\n",
      ">> Epoch 345 finished \tANN training loss 0.101912\n",
      ">> Epoch 346 finished \tANN training loss 0.102599\n",
      ">> Epoch 347 finished \tANN training loss 0.093930\n",
      ">> Epoch 348 finished \tANN training loss 0.102061\n",
      ">> Epoch 349 finished \tANN training loss 0.121207\n",
      ">> Epoch 350 finished \tANN training loss 0.088572\n",
      ">> Epoch 351 finished \tANN training loss 0.103158\n",
      ">> Epoch 352 finished \tANN training loss 0.085455\n",
      ">> Epoch 353 finished \tANN training loss 0.089646\n",
      ">> Epoch 354 finished \tANN training loss 0.096571\n",
      ">> Epoch 355 finished \tANN training loss 0.086945\n",
      ">> Epoch 356 finished \tANN training loss 0.085777\n",
      ">> Epoch 357 finished \tANN training loss 0.087723\n",
      ">> Epoch 358 finished \tANN training loss 0.114877\n",
      ">> Epoch 359 finished \tANN training loss 0.083612\n",
      ">> Epoch 360 finished \tANN training loss 0.097083\n",
      ">> Epoch 361 finished \tANN training loss 0.089950\n",
      ">> Epoch 362 finished \tANN training loss 0.102334\n",
      ">> Epoch 363 finished \tANN training loss 0.110252\n",
      ">> Epoch 364 finished \tANN training loss 0.109875\n",
      ">> Epoch 365 finished \tANN training loss 0.095978\n",
      ">> Epoch 366 finished \tANN training loss 0.090995\n",
      ">> Epoch 367 finished \tANN training loss 0.091135\n",
      ">> Epoch 368 finished \tANN training loss 0.098942\n",
      ">> Epoch 369 finished \tANN training loss 0.086277\n",
      ">> Epoch 370 finished \tANN training loss 0.101351\n",
      ">> Epoch 371 finished \tANN training loss 0.091418\n",
      ">> Epoch 372 finished \tANN training loss 0.093123\n",
      ">> Epoch 373 finished \tANN training loss 0.096121\n",
      ">> Epoch 374 finished \tANN training loss 0.089908\n",
      ">> Epoch 375 finished \tANN training loss 0.087220\n",
      ">> Epoch 376 finished \tANN training loss 0.120093\n",
      ">> Epoch 377 finished \tANN training loss 0.089224\n",
      ">> Epoch 378 finished \tANN training loss 0.090827\n",
      ">> Epoch 379 finished \tANN training loss 0.073247\n",
      ">> Epoch 380 finished \tANN training loss 0.087642\n",
      ">> Epoch 381 finished \tANN training loss 0.084820\n",
      ">> Epoch 382 finished \tANN training loss 0.092735\n",
      ">> Epoch 383 finished \tANN training loss 0.080723\n",
      ">> Epoch 384 finished \tANN training loss 0.081457\n",
      ">> Epoch 385 finished \tANN training loss 0.088135\n",
      ">> Epoch 386 finished \tANN training loss 0.089571\n",
      ">> Epoch 387 finished \tANN training loss 0.083131\n",
      ">> Epoch 388 finished \tANN training loss 0.087615\n",
      ">> Epoch 389 finished \tANN training loss 0.097875\n",
      ">> Epoch 390 finished \tANN training loss 0.083867\n",
      ">> Epoch 391 finished \tANN training loss 0.100386\n",
      ">> Epoch 392 finished \tANN training loss 0.094533\n",
      ">> Epoch 393 finished \tANN training loss 0.100391\n",
      ">> Epoch 394 finished \tANN training loss 0.103046\n",
      ">> Epoch 395 finished \tANN training loss 0.081568\n",
      ">> Epoch 396 finished \tANN training loss 0.087502\n",
      ">> Epoch 397 finished \tANN training loss 0.077426\n",
      ">> Epoch 398 finished \tANN training loss 0.079622\n",
      ">> Epoch 399 finished \tANN training loss 0.087766\n",
      ">> Epoch 400 finished \tANN training loss 0.094910\n",
      ">> Epoch 401 finished \tANN training loss 0.106626\n",
      ">> Epoch 402 finished \tANN training loss 0.089529\n",
      ">> Epoch 403 finished \tANN training loss 0.079703\n",
      ">> Epoch 404 finished \tANN training loss 0.091126\n",
      ">> Epoch 405 finished \tANN training loss 0.091864\n",
      ">> Epoch 406 finished \tANN training loss 0.089987\n",
      ">> Epoch 407 finished \tANN training loss 0.083958\n",
      ">> Epoch 408 finished \tANN training loss 0.084406\n",
      ">> Epoch 409 finished \tANN training loss 0.083855\n",
      ">> Epoch 410 finished \tANN training loss 0.082620\n",
      ">> Epoch 411 finished \tANN training loss 0.083421\n",
      ">> Epoch 412 finished \tANN training loss 0.090418\n",
      ">> Epoch 413 finished \tANN training loss 0.082268\n",
      ">> Epoch 414 finished \tANN training loss 0.090049\n",
      ">> Epoch 415 finished \tANN training loss 0.078330\n",
      ">> Epoch 416 finished \tANN training loss 0.087520\n",
      ">> Epoch 417 finished \tANN training loss 0.084958\n",
      ">> Epoch 418 finished \tANN training loss 0.087788\n",
      ">> Epoch 419 finished \tANN training loss 0.087796\n",
      ">> Epoch 420 finished \tANN training loss 0.085492\n",
      ">> Epoch 421 finished \tANN training loss 0.084518\n",
      ">> Epoch 422 finished \tANN training loss 0.085190\n",
      ">> Epoch 423 finished \tANN training loss 0.084827\n",
      ">> Epoch 424 finished \tANN training loss 0.072323\n",
      ">> Epoch 425 finished \tANN training loss 0.098242\n",
      ">> Epoch 426 finished \tANN training loss 0.094289\n",
      ">> Epoch 427 finished \tANN training loss 0.090594\n",
      ">> Epoch 428 finished \tANN training loss 0.086291\n",
      ">> Epoch 429 finished \tANN training loss 0.102612\n",
      ">> Epoch 430 finished \tANN training loss 0.079620\n",
      ">> Epoch 431 finished \tANN training loss 0.096416\n",
      ">> Epoch 432 finished \tANN training loss 0.108229\n",
      ">> Epoch 433 finished \tANN training loss 0.085693\n",
      ">> Epoch 434 finished \tANN training loss 0.078984\n",
      ">> Epoch 435 finished \tANN training loss 0.093360\n",
      ">> Epoch 436 finished \tANN training loss 0.075861\n",
      ">> Epoch 437 finished \tANN training loss 0.068236\n",
      ">> Epoch 438 finished \tANN training loss 0.069754\n",
      ">> Epoch 439 finished \tANN training loss 0.082470\n",
      ">> Epoch 440 finished \tANN training loss 0.070915\n",
      ">> Epoch 441 finished \tANN training loss 0.068906\n",
      ">> Epoch 442 finished \tANN training loss 0.094907\n",
      ">> Epoch 443 finished \tANN training loss 0.077328\n",
      ">> Epoch 444 finished \tANN training loss 0.082264\n",
      ">> Epoch 445 finished \tANN training loss 0.078654\n",
      ">> Epoch 446 finished \tANN training loss 0.079614\n",
      ">> Epoch 447 finished \tANN training loss 0.070329\n",
      ">> Epoch 448 finished \tANN training loss 0.073092\n",
      ">> Epoch 449 finished \tANN training loss 0.086446\n",
      ">> Epoch 450 finished \tANN training loss 0.081490\n",
      ">> Epoch 451 finished \tANN training loss 0.085923\n",
      ">> Epoch 452 finished \tANN training loss 0.068052\n",
      ">> Epoch 453 finished \tANN training loss 0.076233\n",
      ">> Epoch 454 finished \tANN training loss 0.077710\n",
      ">> Epoch 455 finished \tANN training loss 0.075290\n",
      ">> Epoch 456 finished \tANN training loss 0.073133\n",
      ">> Epoch 457 finished \tANN training loss 0.087246\n",
      ">> Epoch 458 finished \tANN training loss 0.086128\n",
      ">> Epoch 459 finished \tANN training loss 0.087649\n",
      ">> Epoch 460 finished \tANN training loss 0.092318\n",
      ">> Epoch 461 finished \tANN training loss 0.084715\n",
      ">> Epoch 462 finished \tANN training loss 0.082740\n",
      ">> Epoch 463 finished \tANN training loss 0.082337\n",
      ">> Epoch 464 finished \tANN training loss 0.075812\n",
      ">> Epoch 465 finished \tANN training loss 0.082102\n",
      ">> Epoch 466 finished \tANN training loss 0.081777\n",
      ">> Epoch 467 finished \tANN training loss 0.103163\n",
      ">> Epoch 468 finished \tANN training loss 0.093521\n",
      ">> Epoch 469 finished \tANN training loss 0.085037\n",
      ">> Epoch 470 finished \tANN training loss 0.080087\n",
      ">> Epoch 471 finished \tANN training loss 0.087001\n",
      ">> Epoch 472 finished \tANN training loss 0.092072\n",
      ">> Epoch 473 finished \tANN training loss 0.079397\n",
      ">> Epoch 474 finished \tANN training loss 0.084211\n",
      ">> Epoch 475 finished \tANN training loss 0.075587\n",
      ">> Epoch 476 finished \tANN training loss 0.084119\n",
      ">> Epoch 477 finished \tANN training loss 0.080092\n",
      ">> Epoch 478 finished \tANN training loss 0.096810\n",
      ">> Epoch 479 finished \tANN training loss 0.078660\n",
      ">> Epoch 480 finished \tANN training loss 0.088588\n",
      ">> Epoch 481 finished \tANN training loss 0.072551\n",
      ">> Epoch 482 finished \tANN training loss 0.076717\n",
      ">> Epoch 483 finished \tANN training loss 0.093446\n",
      ">> Epoch 484 finished \tANN training loss 0.095847\n",
      ">> Epoch 485 finished \tANN training loss 0.111383\n",
      ">> Epoch 486 finished \tANN training loss 0.088404\n",
      ">> Epoch 487 finished \tANN training loss 0.089626\n",
      ">> Epoch 488 finished \tANN training loss 0.074515\n",
      ">> Epoch 489 finished \tANN training loss 0.073351\n",
      ">> Epoch 490 finished \tANN training loss 0.077699\n",
      ">> Epoch 491 finished \tANN training loss 0.093391\n",
      ">> Epoch 492 finished \tANN training loss 0.083457\n",
      ">> Epoch 493 finished \tANN training loss 0.093806\n",
      ">> Epoch 494 finished \tANN training loss 0.089687\n",
      ">> Epoch 495 finished \tANN training loss 0.090120\n",
      ">> Epoch 496 finished \tANN training loss 0.086639\n",
      ">> Epoch 497 finished \tANN training loss 0.081830\n",
      ">> Epoch 498 finished \tANN training loss 0.078155\n",
      ">> Epoch 499 finished \tANN training loss 0.077867\n",
      ">> Epoch 500 finished \tANN training loss 0.074131\n",
      ">> Epoch 501 finished \tANN training loss 0.085898\n",
      ">> Epoch 502 finished \tANN training loss 0.074904\n",
      ">> Epoch 503 finished \tANN training loss 0.076671\n",
      ">> Epoch 504 finished \tANN training loss 0.075114\n",
      ">> Epoch 505 finished \tANN training loss 0.067561\n",
      ">> Epoch 506 finished \tANN training loss 0.067976\n",
      ">> Epoch 507 finished \tANN training loss 0.089717\n",
      ">> Epoch 508 finished \tANN training loss 0.085420\n",
      ">> Epoch 509 finished \tANN training loss 0.078647\n",
      ">> Epoch 510 finished \tANN training loss 0.087467\n",
      ">> Epoch 511 finished \tANN training loss 0.080738\n",
      ">> Epoch 512 finished \tANN training loss 0.086925\n",
      ">> Epoch 513 finished \tANN training loss 0.075919\n",
      ">> Epoch 514 finished \tANN training loss 0.082659\n",
      ">> Epoch 515 finished \tANN training loss 0.086971\n",
      ">> Epoch 516 finished \tANN training loss 0.084540\n",
      ">> Epoch 517 finished \tANN training loss 0.086206\n",
      ">> Epoch 518 finished \tANN training loss 0.085316\n",
      ">> Epoch 519 finished \tANN training loss 0.079064\n",
      ">> Epoch 520 finished \tANN training loss 0.065749\n",
      ">> Epoch 521 finished \tANN training loss 0.074895\n",
      ">> Epoch 522 finished \tANN training loss 0.076897\n",
      ">> Epoch 523 finished \tANN training loss 0.077575\n",
      ">> Epoch 524 finished \tANN training loss 0.080785\n",
      ">> Epoch 525 finished \tANN training loss 0.076849\n",
      ">> Epoch 526 finished \tANN training loss 0.079273\n",
      ">> Epoch 527 finished \tANN training loss 0.071825\n",
      ">> Epoch 528 finished \tANN training loss 0.086122\n",
      ">> Epoch 529 finished \tANN training loss 0.082363\n",
      ">> Epoch 530 finished \tANN training loss 0.079342\n",
      ">> Epoch 531 finished \tANN training loss 0.091066\n",
      ">> Epoch 532 finished \tANN training loss 0.072876\n",
      ">> Epoch 533 finished \tANN training loss 0.069872\n",
      ">> Epoch 534 finished \tANN training loss 0.072739\n",
      ">> Epoch 535 finished \tANN training loss 0.068409\n",
      ">> Epoch 536 finished \tANN training loss 0.086685\n",
      ">> Epoch 537 finished \tANN training loss 0.102515\n",
      ">> Epoch 538 finished \tANN training loss 0.078658\n",
      ">> Epoch 539 finished \tANN training loss 0.077711\n",
      ">> Epoch 540 finished \tANN training loss 0.101687\n",
      ">> Epoch 541 finished \tANN training loss 0.094806\n",
      ">> Epoch 542 finished \tANN training loss 0.101442\n",
      ">> Epoch 543 finished \tANN training loss 0.086574\n",
      ">> Epoch 544 finished \tANN training loss 0.084359\n",
      ">> Epoch 545 finished \tANN training loss 0.077088\n",
      ">> Epoch 546 finished \tANN training loss 0.078703\n",
      ">> Epoch 547 finished \tANN training loss 0.082590\n",
      ">> Epoch 548 finished \tANN training loss 0.079656\n",
      ">> Epoch 549 finished \tANN training loss 0.073740\n",
      ">> Epoch 550 finished \tANN training loss 0.082513\n",
      ">> Epoch 551 finished \tANN training loss 0.076353\n",
      ">> Epoch 552 finished \tANN training loss 0.069952\n",
      ">> Epoch 553 finished \tANN training loss 0.068231\n",
      ">> Epoch 554 finished \tANN training loss 0.067454\n",
      ">> Epoch 555 finished \tANN training loss 0.076729\n",
      ">> Epoch 556 finished \tANN training loss 0.082247\n",
      ">> Epoch 557 finished \tANN training loss 0.069749\n",
      ">> Epoch 558 finished \tANN training loss 0.086676\n",
      ">> Epoch 559 finished \tANN training loss 0.072397\n",
      ">> Epoch 560 finished \tANN training loss 0.086222\n",
      ">> Epoch 561 finished \tANN training loss 0.070435\n",
      ">> Epoch 562 finished \tANN training loss 0.070205\n",
      ">> Epoch 563 finished \tANN training loss 0.073366\n",
      ">> Epoch 564 finished \tANN training loss 0.076748\n",
      ">> Epoch 565 finished \tANN training loss 0.077363\n",
      ">> Epoch 566 finished \tANN training loss 0.069006\n",
      ">> Epoch 567 finished \tANN training loss 0.074578\n",
      ">> Epoch 568 finished \tANN training loss 0.074564\n",
      ">> Epoch 569 finished \tANN training loss 0.082453\n",
      ">> Epoch 570 finished \tANN training loss 0.074404\n",
      ">> Epoch 571 finished \tANN training loss 0.081307\n",
      ">> Epoch 572 finished \tANN training loss 0.080658\n",
      ">> Epoch 573 finished \tANN training loss 0.062346\n",
      ">> Epoch 574 finished \tANN training loss 0.081024\n",
      ">> Epoch 575 finished \tANN training loss 0.070024\n",
      ">> Epoch 576 finished \tANN training loss 0.072273\n",
      ">> Epoch 577 finished \tANN training loss 0.079052\n",
      ">> Epoch 578 finished \tANN training loss 0.073365\n",
      ">> Epoch 579 finished \tANN training loss 0.089863\n",
      ">> Epoch 580 finished \tANN training loss 0.066475\n",
      ">> Epoch 581 finished \tANN training loss 0.077179\n",
      ">> Epoch 582 finished \tANN training loss 0.079877\n",
      ">> Epoch 583 finished \tANN training loss 0.081163\n",
      ">> Epoch 584 finished \tANN training loss 0.080678\n",
      ">> Epoch 585 finished \tANN training loss 0.073920\n",
      ">> Epoch 586 finished \tANN training loss 0.072229\n",
      ">> Epoch 587 finished \tANN training loss 0.070238\n",
      ">> Epoch 588 finished \tANN training loss 0.089302\n",
      ">> Epoch 589 finished \tANN training loss 0.058922\n",
      ">> Epoch 590 finished \tANN training loss 0.065375\n",
      ">> Epoch 591 finished \tANN training loss 0.070299\n",
      ">> Epoch 592 finished \tANN training loss 0.074672\n",
      ">> Epoch 593 finished \tANN training loss 0.067277\n",
      ">> Epoch 594 finished \tANN training loss 0.068452\n",
      ">> Epoch 595 finished \tANN training loss 0.091011\n",
      ">> Epoch 596 finished \tANN training loss 0.071958\n",
      ">> Epoch 597 finished \tANN training loss 0.085637\n",
      ">> Epoch 598 finished \tANN training loss 0.083549\n",
      ">> Epoch 599 finished \tANN training loss 0.068955\n",
      ">> Epoch 600 finished \tANN training loss 0.060700\n",
      ">> Epoch 601 finished \tANN training loss 0.060442\n",
      ">> Epoch 602 finished \tANN training loss 0.084028\n",
      ">> Epoch 603 finished \tANN training loss 0.085293\n",
      ">> Epoch 604 finished \tANN training loss 0.082685\n",
      ">> Epoch 605 finished \tANN training loss 0.072511\n",
      ">> Epoch 606 finished \tANN training loss 0.067816\n",
      ">> Epoch 607 finished \tANN training loss 0.065189\n",
      ">> Epoch 608 finished \tANN training loss 0.060080\n",
      ">> Epoch 609 finished \tANN training loss 0.066163\n",
      ">> Epoch 610 finished \tANN training loss 0.072316\n",
      ">> Epoch 611 finished \tANN training loss 0.066901\n",
      ">> Epoch 612 finished \tANN training loss 0.072237\n",
      ">> Epoch 613 finished \tANN training loss 0.066401\n",
      ">> Epoch 614 finished \tANN training loss 0.078243\n",
      ">> Epoch 615 finished \tANN training loss 0.086012\n",
      ">> Epoch 616 finished \tANN training loss 0.074177\n",
      ">> Epoch 617 finished \tANN training loss 0.072900\n",
      ">> Epoch 618 finished \tANN training loss 0.073788\n",
      ">> Epoch 619 finished \tANN training loss 0.084642\n",
      ">> Epoch 620 finished \tANN training loss 0.070775\n",
      ">> Epoch 621 finished \tANN training loss 0.068764\n",
      ">> Epoch 622 finished \tANN training loss 0.065085\n",
      ">> Epoch 623 finished \tANN training loss 0.065852\n",
      ">> Epoch 624 finished \tANN training loss 0.074943\n",
      ">> Epoch 625 finished \tANN training loss 0.075507\n",
      ">> Epoch 626 finished \tANN training loss 0.072108\n",
      ">> Epoch 627 finished \tANN training loss 0.069032\n",
      ">> Epoch 628 finished \tANN training loss 0.068104\n",
      ">> Epoch 629 finished \tANN training loss 0.075582\n",
      ">> Epoch 630 finished \tANN training loss 0.068341\n",
      ">> Epoch 631 finished \tANN training loss 0.072329\n",
      ">> Epoch 632 finished \tANN training loss 0.059025\n",
      ">> Epoch 633 finished \tANN training loss 0.061990\n",
      ">> Epoch 634 finished \tANN training loss 0.066242\n",
      ">> Epoch 635 finished \tANN training loss 0.081369\n",
      ">> Epoch 636 finished \tANN training loss 0.063445\n",
      ">> Epoch 637 finished \tANN training loss 0.064536\n",
      ">> Epoch 638 finished \tANN training loss 0.073348\n",
      ">> Epoch 639 finished \tANN training loss 0.067150\n",
      ">> Epoch 640 finished \tANN training loss 0.074884\n",
      ">> Epoch 641 finished \tANN training loss 0.077714\n",
      ">> Epoch 642 finished \tANN training loss 0.076771\n",
      ">> Epoch 643 finished \tANN training loss 0.064075\n",
      ">> Epoch 644 finished \tANN training loss 0.063377\n",
      ">> Epoch 645 finished \tANN training loss 0.064317\n",
      ">> Epoch 646 finished \tANN training loss 0.076658\n",
      ">> Epoch 647 finished \tANN training loss 0.070812\n",
      ">> Epoch 648 finished \tANN training loss 0.076797\n",
      ">> Epoch 649 finished \tANN training loss 0.070179\n",
      ">> Epoch 650 finished \tANN training loss 0.080048\n",
      ">> Epoch 651 finished \tANN training loss 0.071950\n",
      ">> Epoch 652 finished \tANN training loss 0.084810\n",
      ">> Epoch 653 finished \tANN training loss 0.077673\n",
      ">> Epoch 654 finished \tANN training loss 0.073860\n",
      ">> Epoch 655 finished \tANN training loss 0.068268\n",
      ">> Epoch 656 finished \tANN training loss 0.068281\n",
      ">> Epoch 657 finished \tANN training loss 0.084104\n",
      ">> Epoch 658 finished \tANN training loss 0.083824\n",
      ">> Epoch 659 finished \tANN training loss 0.065710\n",
      ">> Epoch 660 finished \tANN training loss 0.063500\n",
      ">> Epoch 661 finished \tANN training loss 0.092838\n",
      ">> Epoch 662 finished \tANN training loss 0.065540\n",
      ">> Epoch 663 finished \tANN training loss 0.071850\n",
      ">> Epoch 664 finished \tANN training loss 0.066918\n",
      ">> Epoch 665 finished \tANN training loss 0.082813\n",
      ">> Epoch 666 finished \tANN training loss 0.061515\n",
      ">> Epoch 667 finished \tANN training loss 0.058378\n",
      ">> Epoch 668 finished \tANN training loss 0.057553\n",
      ">> Epoch 669 finished \tANN training loss 0.073512\n",
      ">> Epoch 670 finished \tANN training loss 0.054971\n",
      ">> Epoch 671 finished \tANN training loss 0.060286\n",
      ">> Epoch 672 finished \tANN training loss 0.062748\n",
      ">> Epoch 673 finished \tANN training loss 0.061512\n",
      ">> Epoch 674 finished \tANN training loss 0.059250\n",
      ">> Epoch 675 finished \tANN training loss 0.061824\n",
      ">> Epoch 676 finished \tANN training loss 0.063283\n",
      ">> Epoch 677 finished \tANN training loss 0.074652\n",
      ">> Epoch 678 finished \tANN training loss 0.084780\n",
      ">> Epoch 679 finished \tANN training loss 0.068277\n",
      ">> Epoch 680 finished \tANN training loss 0.072765\n",
      ">> Epoch 681 finished \tANN training loss 0.079738\n",
      ">> Epoch 682 finished \tANN training loss 0.069179\n",
      ">> Epoch 683 finished \tANN training loss 0.077717\n",
      ">> Epoch 684 finished \tANN training loss 0.060465\n",
      ">> Epoch 685 finished \tANN training loss 0.059254\n",
      ">> Epoch 686 finished \tANN training loss 0.068717\n",
      ">> Epoch 687 finished \tANN training loss 0.066435\n",
      ">> Epoch 688 finished \tANN training loss 0.070718\n",
      ">> Epoch 689 finished \tANN training loss 0.074746\n",
      ">> Epoch 690 finished \tANN training loss 0.071344\n",
      ">> Epoch 691 finished \tANN training loss 0.070595\n",
      ">> Epoch 692 finished \tANN training loss 0.071962\n",
      ">> Epoch 693 finished \tANN training loss 0.062124\n",
      ">> Epoch 694 finished \tANN training loss 0.057425\n",
      ">> Epoch 695 finished \tANN training loss 0.065264\n",
      ">> Epoch 696 finished \tANN training loss 0.069638\n",
      ">> Epoch 697 finished \tANN training loss 0.073797\n",
      ">> Epoch 698 finished \tANN training loss 0.072742\n",
      ">> Epoch 699 finished \tANN training loss 0.076686\n",
      ">> Epoch 700 finished \tANN training loss 0.067771\n",
      ">> Epoch 701 finished \tANN training loss 0.061239\n",
      ">> Epoch 702 finished \tANN training loss 0.062603\n",
      ">> Epoch 703 finished \tANN training loss 0.066729\n",
      ">> Epoch 704 finished \tANN training loss 0.071372\n",
      ">> Epoch 705 finished \tANN training loss 0.064764\n",
      ">> Epoch 706 finished \tANN training loss 0.054198\n",
      ">> Epoch 707 finished \tANN training loss 0.075075\n",
      ">> Epoch 708 finished \tANN training loss 0.065155\n",
      ">> Epoch 709 finished \tANN training loss 0.052790\n",
      ">> Epoch 710 finished \tANN training loss 0.062621\n",
      ">> Epoch 711 finished \tANN training loss 0.074191\n",
      ">> Epoch 712 finished \tANN training loss 0.065641\n",
      ">> Epoch 713 finished \tANN training loss 0.062513\n",
      ">> Epoch 714 finished \tANN training loss 0.066586\n",
      ">> Epoch 715 finished \tANN training loss 0.054784\n",
      ">> Epoch 716 finished \tANN training loss 0.072618\n",
      ">> Epoch 717 finished \tANN training loss 0.064294\n",
      ">> Epoch 718 finished \tANN training loss 0.060559\n",
      ">> Epoch 719 finished \tANN training loss 0.063907\n",
      ">> Epoch 720 finished \tANN training loss 0.064194\n",
      ">> Epoch 721 finished \tANN training loss 0.070547\n",
      ">> Epoch 722 finished \tANN training loss 0.066932\n",
      ">> Epoch 723 finished \tANN training loss 0.071991\n",
      ">> Epoch 724 finished \tANN training loss 0.075586\n",
      ">> Epoch 725 finished \tANN training loss 0.063575\n",
      ">> Epoch 726 finished \tANN training loss 0.071108\n",
      ">> Epoch 727 finished \tANN training loss 0.068960\n",
      ">> Epoch 728 finished \tANN training loss 0.058359\n",
      ">> Epoch 729 finished \tANN training loss 0.069392\n",
      ">> Epoch 730 finished \tANN training loss 0.071176\n",
      ">> Epoch 731 finished \tANN training loss 0.054908\n",
      ">> Epoch 732 finished \tANN training loss 0.060131\n",
      ">> Epoch 733 finished \tANN training loss 0.061098\n",
      ">> Epoch 734 finished \tANN training loss 0.064589\n",
      ">> Epoch 735 finished \tANN training loss 0.059714\n",
      ">> Epoch 736 finished \tANN training loss 0.061888\n",
      ">> Epoch 737 finished \tANN training loss 0.060437\n",
      ">> Epoch 738 finished \tANN training loss 0.069128\n",
      ">> Epoch 739 finished \tANN training loss 0.079129\n",
      ">> Epoch 740 finished \tANN training loss 0.071166\n",
      ">> Epoch 741 finished \tANN training loss 0.071190\n",
      ">> Epoch 742 finished \tANN training loss 0.069184\n",
      ">> Epoch 743 finished \tANN training loss 0.070725\n",
      ">> Epoch 744 finished \tANN training loss 0.063890\n",
      ">> Epoch 745 finished \tANN training loss 0.065537\n",
      ">> Epoch 746 finished \tANN training loss 0.060689\n",
      ">> Epoch 747 finished \tANN training loss 0.066261\n",
      ">> Epoch 748 finished \tANN training loss 0.075789\n",
      ">> Epoch 749 finished \tANN training loss 0.079150\n",
      ">> Epoch 750 finished \tANN training loss 0.068841\n",
      ">> Epoch 751 finished \tANN training loss 0.066114\n",
      ">> Epoch 752 finished \tANN training loss 0.073500\n",
      ">> Epoch 753 finished \tANN training loss 0.065062\n",
      ">> Epoch 754 finished \tANN training loss 0.067816\n",
      ">> Epoch 755 finished \tANN training loss 0.073713\n",
      ">> Epoch 756 finished \tANN training loss 0.066317\n",
      ">> Epoch 757 finished \tANN training loss 0.065426\n",
      ">> Epoch 758 finished \tANN training loss 0.066052\n",
      ">> Epoch 759 finished \tANN training loss 0.061207\n",
      ">> Epoch 760 finished \tANN training loss 0.058442\n",
      ">> Epoch 761 finished \tANN training loss 0.066670\n",
      ">> Epoch 762 finished \tANN training loss 0.067979\n",
      ">> Epoch 763 finished \tANN training loss 0.068259\n",
      ">> Epoch 764 finished \tANN training loss 0.062099\n",
      ">> Epoch 765 finished \tANN training loss 0.065932\n",
      ">> Epoch 766 finished \tANN training loss 0.071551\n",
      ">> Epoch 767 finished \tANN training loss 0.058830\n",
      ">> Epoch 768 finished \tANN training loss 0.060319\n",
      ">> Epoch 769 finished \tANN training loss 0.059115\n",
      ">> Epoch 770 finished \tANN training loss 0.066226\n",
      ">> Epoch 771 finished \tANN training loss 0.061746\n",
      ">> Epoch 772 finished \tANN training loss 0.066431\n",
      ">> Epoch 773 finished \tANN training loss 0.063498\n",
      ">> Epoch 774 finished \tANN training loss 0.064004\n",
      ">> Epoch 775 finished \tANN training loss 0.054465\n",
      ">> Epoch 776 finished \tANN training loss 0.058871\n",
      ">> Epoch 777 finished \tANN training loss 0.055340\n",
      ">> Epoch 778 finished \tANN training loss 0.053598\n",
      ">> Epoch 779 finished \tANN training loss 0.070910\n",
      ">> Epoch 780 finished \tANN training loss 0.066746\n",
      ">> Epoch 781 finished \tANN training loss 0.057958\n",
      ">> Epoch 782 finished \tANN training loss 0.053100\n",
      ">> Epoch 783 finished \tANN training loss 0.060847\n",
      ">> Epoch 784 finished \tANN training loss 0.066170\n",
      ">> Epoch 785 finished \tANN training loss 0.064900\n",
      ">> Epoch 786 finished \tANN training loss 0.071202\n",
      ">> Epoch 787 finished \tANN training loss 0.064411\n",
      ">> Epoch 788 finished \tANN training loss 0.059426\n",
      ">> Epoch 789 finished \tANN training loss 0.074425\n",
      ">> Epoch 790 finished \tANN training loss 0.060950\n",
      ">> Epoch 791 finished \tANN training loss 0.069288\n",
      ">> Epoch 792 finished \tANN training loss 0.081316\n",
      ">> Epoch 793 finished \tANN training loss 0.065087\n",
      ">> Epoch 794 finished \tANN training loss 0.072299\n",
      ">> Epoch 795 finished \tANN training loss 0.065300\n",
      ">> Epoch 796 finished \tANN training loss 0.073926\n",
      ">> Epoch 797 finished \tANN training loss 0.071909\n",
      ">> Epoch 798 finished \tANN training loss 0.062451\n",
      ">> Epoch 799 finished \tANN training loss 0.070095\n",
      ">> Epoch 800 finished \tANN training loss 0.066118\n",
      ">> Epoch 801 finished \tANN training loss 0.070803\n",
      ">> Epoch 802 finished \tANN training loss 0.073765\n",
      ">> Epoch 803 finished \tANN training loss 0.070227\n",
      ">> Epoch 804 finished \tANN training loss 0.076002\n",
      ">> Epoch 805 finished \tANN training loss 0.073120\n",
      ">> Epoch 806 finished \tANN training loss 0.065638\n",
      ">> Epoch 807 finished \tANN training loss 0.072198\n",
      ">> Epoch 808 finished \tANN training loss 0.055898\n",
      ">> Epoch 809 finished \tANN training loss 0.066604\n",
      ">> Epoch 810 finished \tANN training loss 0.064440\n",
      ">> Epoch 811 finished \tANN training loss 0.064194\n",
      ">> Epoch 812 finished \tANN training loss 0.058419\n",
      ">> Epoch 813 finished \tANN training loss 0.055356\n",
      ">> Epoch 814 finished \tANN training loss 0.089830\n",
      ">> Epoch 815 finished \tANN training loss 0.075648\n",
      ">> Epoch 816 finished \tANN training loss 0.062211\n",
      ">> Epoch 817 finished \tANN training loss 0.061602\n",
      ">> Epoch 818 finished \tANN training loss 0.059316\n",
      ">> Epoch 819 finished \tANN training loss 0.081328\n",
      ">> Epoch 820 finished \tANN training loss 0.060174\n",
      ">> Epoch 821 finished \tANN training loss 0.057773\n",
      ">> Epoch 822 finished \tANN training loss 0.057989\n",
      ">> Epoch 823 finished \tANN training loss 0.059108\n",
      ">> Epoch 824 finished \tANN training loss 0.069239\n",
      ">> Epoch 825 finished \tANN training loss 0.068486\n",
      ">> Epoch 826 finished \tANN training loss 0.054889\n",
      ">> Epoch 827 finished \tANN training loss 0.055831\n",
      ">> Epoch 828 finished \tANN training loss 0.054539\n",
      ">> Epoch 829 finished \tANN training loss 0.053149\n",
      ">> Epoch 830 finished \tANN training loss 0.065093\n",
      ">> Epoch 831 finished \tANN training loss 0.069314\n",
      ">> Epoch 832 finished \tANN training loss 0.064904\n",
      ">> Epoch 833 finished \tANN training loss 0.064506\n",
      ">> Epoch 834 finished \tANN training loss 0.068567\n",
      ">> Epoch 835 finished \tANN training loss 0.071510\n",
      ">> Epoch 836 finished \tANN training loss 0.068216\n",
      ">> Epoch 837 finished \tANN training loss 0.060056\n",
      ">> Epoch 838 finished \tANN training loss 0.060998\n",
      ">> Epoch 839 finished \tANN training loss 0.070678\n",
      ">> Epoch 840 finished \tANN training loss 0.057660\n",
      ">> Epoch 841 finished \tANN training loss 0.077888\n",
      ">> Epoch 842 finished \tANN training loss 0.057779\n",
      ">> Epoch 843 finished \tANN training loss 0.057305\n",
      ">> Epoch 844 finished \tANN training loss 0.052031\n",
      ">> Epoch 845 finished \tANN training loss 0.052925\n",
      ">> Epoch 846 finished \tANN training loss 0.055967\n",
      ">> Epoch 847 finished \tANN training loss 0.055292\n",
      ">> Epoch 848 finished \tANN training loss 0.068759\n",
      ">> Epoch 849 finished \tANN training loss 0.061972\n",
      ">> Epoch 850 finished \tANN training loss 0.064410\n",
      ">> Epoch 851 finished \tANN training loss 0.086541\n",
      ">> Epoch 852 finished \tANN training loss 0.069027\n",
      ">> Epoch 853 finished \tANN training loss 0.062836\n",
      ">> Epoch 854 finished \tANN training loss 0.073248\n",
      ">> Epoch 855 finished \tANN training loss 0.084769\n",
      ">> Epoch 856 finished \tANN training loss 0.072702\n",
      ">> Epoch 857 finished \tANN training loss 0.070156\n",
      ">> Epoch 858 finished \tANN training loss 0.080991\n",
      ">> Epoch 859 finished \tANN training loss 0.071917\n",
      ">> Epoch 860 finished \tANN training loss 0.067819\n",
      ">> Epoch 861 finished \tANN training loss 0.062457\n",
      ">> Epoch 862 finished \tANN training loss 0.064455\n",
      ">> Epoch 863 finished \tANN training loss 0.073376\n",
      ">> Epoch 864 finished \tANN training loss 0.075049\n",
      ">> Epoch 865 finished \tANN training loss 0.067153\n",
      ">> Epoch 866 finished \tANN training loss 0.065128\n",
      ">> Epoch 867 finished \tANN training loss 0.062102\n",
      ">> Epoch 868 finished \tANN training loss 0.065708\n",
      ">> Epoch 869 finished \tANN training loss 0.064077\n",
      ">> Epoch 870 finished \tANN training loss 0.062196\n",
      ">> Epoch 871 finished \tANN training loss 0.069051\n",
      ">> Epoch 872 finished \tANN training loss 0.068475\n",
      ">> Epoch 873 finished \tANN training loss 0.063335\n",
      ">> Epoch 874 finished \tANN training loss 0.065138\n",
      ">> Epoch 875 finished \tANN training loss 0.061004\n",
      ">> Epoch 876 finished \tANN training loss 0.070312\n",
      ">> Epoch 877 finished \tANN training loss 0.061274\n",
      ">> Epoch 878 finished \tANN training loss 0.070766\n",
      ">> Epoch 879 finished \tANN training loss 0.080119\n",
      ">> Epoch 880 finished \tANN training loss 0.061079\n",
      ">> Epoch 881 finished \tANN training loss 0.060162\n",
      ">> Epoch 882 finished \tANN training loss 0.056312\n",
      ">> Epoch 883 finished \tANN training loss 0.068919\n",
      ">> Epoch 884 finished \tANN training loss 0.061798\n",
      ">> Epoch 885 finished \tANN training loss 0.056773\n",
      ">> Epoch 886 finished \tANN training loss 0.059117\n",
      ">> Epoch 887 finished \tANN training loss 0.064116\n",
      ">> Epoch 888 finished \tANN training loss 0.057519\n",
      ">> Epoch 889 finished \tANN training loss 0.060266\n",
      ">> Epoch 890 finished \tANN training loss 0.076320\n",
      ">> Epoch 891 finished \tANN training loss 0.061900\n",
      ">> Epoch 892 finished \tANN training loss 0.081105\n",
      ">> Epoch 893 finished \tANN training loss 0.063148\n",
      ">> Epoch 894 finished \tANN training loss 0.055401\n",
      ">> Epoch 895 finished \tANN training loss 0.055222\n",
      ">> Epoch 896 finished \tANN training loss 0.056718\n",
      ">> Epoch 897 finished \tANN training loss 0.054988\n",
      ">> Epoch 898 finished \tANN training loss 0.059535\n",
      ">> Epoch 899 finished \tANN training loss 0.055435\n",
      ">> Epoch 900 finished \tANN training loss 0.058384\n",
      ">> Epoch 901 finished \tANN training loss 0.050462\n",
      ">> Epoch 902 finished \tANN training loss 0.055900\n",
      ">> Epoch 903 finished \tANN training loss 0.055223\n",
      ">> Epoch 904 finished \tANN training loss 0.062542\n",
      ">> Epoch 905 finished \tANN training loss 0.061965\n",
      ">> Epoch 906 finished \tANN training loss 0.058678\n",
      ">> Epoch 907 finished \tANN training loss 0.065298\n",
      ">> Epoch 908 finished \tANN training loss 0.063566\n",
      ">> Epoch 909 finished \tANN training loss 0.065633\n",
      ">> Epoch 910 finished \tANN training loss 0.060776\n",
      ">> Epoch 911 finished \tANN training loss 0.064908\n",
      ">> Epoch 912 finished \tANN training loss 0.063479\n",
      ">> Epoch 913 finished \tANN training loss 0.063172\n",
      ">> Epoch 914 finished \tANN training loss 0.080070\n",
      ">> Epoch 915 finished \tANN training loss 0.062408\n",
      ">> Epoch 916 finished \tANN training loss 0.058250\n",
      ">> Epoch 917 finished \tANN training loss 0.057343\n",
      ">> Epoch 918 finished \tANN training loss 0.057695\n",
      ">> Epoch 919 finished \tANN training loss 0.056620\n",
      ">> Epoch 920 finished \tANN training loss 0.057734\n",
      ">> Epoch 921 finished \tANN training loss 0.069232\n",
      ">> Epoch 922 finished \tANN training loss 0.061927\n",
      ">> Epoch 923 finished \tANN training loss 0.058636\n",
      ">> Epoch 924 finished \tANN training loss 0.056246\n",
      ">> Epoch 925 finished \tANN training loss 0.059388\n",
      ">> Epoch 926 finished \tANN training loss 0.060365\n",
      ">> Epoch 927 finished \tANN training loss 0.057104\n",
      ">> Epoch 928 finished \tANN training loss 0.064531\n",
      ">> Epoch 929 finished \tANN training loss 0.064305\n",
      ">> Epoch 930 finished \tANN training loss 0.073812\n",
      ">> Epoch 931 finished \tANN training loss 0.059209\n",
      ">> Epoch 932 finished \tANN training loss 0.064231\n",
      ">> Epoch 933 finished \tANN training loss 0.062722\n",
      ">> Epoch 934 finished \tANN training loss 0.064204\n",
      ">> Epoch 935 finished \tANN training loss 0.062806\n",
      ">> Epoch 936 finished \tANN training loss 0.064010\n",
      ">> Epoch 937 finished \tANN training loss 0.068707\n",
      ">> Epoch 938 finished \tANN training loss 0.060570\n",
      ">> Epoch 939 finished \tANN training loss 0.060553\n",
      ">> Epoch 940 finished \tANN training loss 0.070676\n",
      ">> Epoch 941 finished \tANN training loss 0.062020\n",
      ">> Epoch 942 finished \tANN training loss 0.057434\n",
      ">> Epoch 943 finished \tANN training loss 0.063665\n",
      ">> Epoch 944 finished \tANN training loss 0.059916\n",
      ">> Epoch 945 finished \tANN training loss 0.050070\n",
      ">> Epoch 946 finished \tANN training loss 0.068640\n",
      ">> Epoch 947 finished \tANN training loss 0.065822\n",
      ">> Epoch 948 finished \tANN training loss 0.057194\n",
      ">> Epoch 949 finished \tANN training loss 0.054443\n",
      ">> Epoch 950 finished \tANN training loss 0.066754\n",
      ">> Epoch 951 finished \tANN training loss 0.066180\n",
      ">> Epoch 952 finished \tANN training loss 0.063049\n",
      ">> Epoch 953 finished \tANN training loss 0.072663\n",
      ">> Epoch 954 finished \tANN training loss 0.066658\n",
      ">> Epoch 955 finished \tANN training loss 0.074837\n",
      ">> Epoch 956 finished \tANN training loss 0.066177\n",
      ">> Epoch 957 finished \tANN training loss 0.055489\n",
      ">> Epoch 958 finished \tANN training loss 0.061993\n",
      ">> Epoch 959 finished \tANN training loss 0.057705\n",
      ">> Epoch 960 finished \tANN training loss 0.060841\n",
      ">> Epoch 961 finished \tANN training loss 0.071271\n",
      ">> Epoch 962 finished \tANN training loss 0.072458\n",
      ">> Epoch 963 finished \tANN training loss 0.058646\n",
      ">> Epoch 964 finished \tANN training loss 0.049217\n",
      ">> Epoch 965 finished \tANN training loss 0.052972\n",
      ">> Epoch 966 finished \tANN training loss 0.063772\n",
      ">> Epoch 967 finished \tANN training loss 0.073906\n",
      ">> Epoch 968 finished \tANN training loss 0.060678\n",
      ">> Epoch 969 finished \tANN training loss 0.054649\n",
      ">> Epoch 970 finished \tANN training loss 0.065622\n",
      ">> Epoch 971 finished \tANN training loss 0.071695\n",
      ">> Epoch 972 finished \tANN training loss 0.071418\n",
      ">> Epoch 973 finished \tANN training loss 0.063083\n",
      ">> Epoch 974 finished \tANN training loss 0.058249\n",
      ">> Epoch 975 finished \tANN training loss 0.071515\n",
      ">> Epoch 976 finished \tANN training loss 0.060966\n",
      ">> Epoch 977 finished \tANN training loss 0.062655\n",
      ">> Epoch 978 finished \tANN training loss 0.067557\n",
      ">> Epoch 979 finished \tANN training loss 0.064856\n",
      ">> Epoch 980 finished \tANN training loss 0.068571\n",
      ">> Epoch 981 finished \tANN training loss 0.055511\n",
      ">> Epoch 982 finished \tANN training loss 0.073521\n",
      ">> Epoch 983 finished \tANN training loss 0.062202\n",
      ">> Epoch 984 finished \tANN training loss 0.060169\n",
      ">> Epoch 985 finished \tANN training loss 0.067812\n",
      ">> Epoch 986 finished \tANN training loss 0.058575\n",
      ">> Epoch 987 finished \tANN training loss 0.058567\n",
      ">> Epoch 988 finished \tANN training loss 0.066237\n",
      ">> Epoch 989 finished \tANN training loss 0.066794\n",
      ">> Epoch 990 finished \tANN training loss 0.073351\n",
      ">> Epoch 991 finished \tANN training loss 0.067305\n",
      ">> Epoch 992 finished \tANN training loss 0.057971\n",
      ">> Epoch 993 finished \tANN training loss 0.059702\n",
      ">> Epoch 994 finished \tANN training loss 0.059153\n",
      ">> Epoch 995 finished \tANN training loss 0.063900\n",
      ">> Epoch 996 finished \tANN training loss 0.059504\n",
      ">> Epoch 997 finished \tANN training loss 0.055543\n",
      ">> Epoch 998 finished \tANN training loss 0.064985\n",
      ">> Epoch 999 finished \tANN training loss 0.058824\n",
      "[END] Fine tuning step\n",
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 9.870491\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 11.332423\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 11.202591\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 10.797882\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 10.370235\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 10.433528\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 10.679785\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 10.184961\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 10.090860\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 10.064258\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 10.424543\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 10.118233\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 10.603822\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 10.551282\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 10.751033\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 10.938550\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 10.865404\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 9.954369\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 10.118990\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 10.039604\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 17.512068\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 16.460705\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 17.444149\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 15.527826\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 17.311890\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 19.471682\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 17.674963\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 19.900352\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 17.286392\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 18.292770\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 18.887281\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 18.251427\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 17.328009\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 16.999277\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 18.322927\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 18.271376\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 17.738735\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 18.628281\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 17.961571\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 20.188515\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 1.060245\n",
      ">> Epoch 1 finished \tANN training loss 0.997877\n",
      ">> Epoch 2 finished \tANN training loss 1.001357\n",
      ">> Epoch 3 finished \tANN training loss 0.962348\n",
      ">> Epoch 4 finished \tANN training loss 0.927913\n",
      ">> Epoch 5 finished \tANN training loss 0.856221\n",
      ">> Epoch 6 finished \tANN training loss 0.834780\n",
      ">> Epoch 7 finished \tANN training loss 0.794600\n",
      ">> Epoch 8 finished \tANN training loss 0.770696\n",
      ">> Epoch 9 finished \tANN training loss 0.741992\n",
      ">> Epoch 10 finished \tANN training loss 0.699935\n",
      ">> Epoch 11 finished \tANN training loss 0.669275\n",
      ">> Epoch 12 finished \tANN training loss 0.638687\n",
      ">> Epoch 13 finished \tANN training loss 0.614477\n",
      ">> Epoch 14 finished \tANN training loss 0.600177\n",
      ">> Epoch 15 finished \tANN training loss 0.571788\n",
      ">> Epoch 16 finished \tANN training loss 0.552933\n",
      ">> Epoch 17 finished \tANN training loss 0.547218\n",
      ">> Epoch 18 finished \tANN training loss 0.535447\n",
      ">> Epoch 19 finished \tANN training loss 0.533449\n",
      ">> Epoch 20 finished \tANN training loss 0.511990\n",
      ">> Epoch 21 finished \tANN training loss 0.494723\n",
      ">> Epoch 22 finished \tANN training loss 0.491025\n",
      ">> Epoch 23 finished \tANN training loss 0.485848\n",
      ">> Epoch 24 finished \tANN training loss 0.481236\n",
      ">> Epoch 25 finished \tANN training loss 0.467396\n",
      ">> Epoch 26 finished \tANN training loss 0.457248\n",
      ">> Epoch 27 finished \tANN training loss 0.464534\n",
      ">> Epoch 28 finished \tANN training loss 0.450463\n",
      ">> Epoch 29 finished \tANN training loss 0.439892\n",
      ">> Epoch 30 finished \tANN training loss 0.433559\n",
      ">> Epoch 31 finished \tANN training loss 0.438373\n",
      ">> Epoch 32 finished \tANN training loss 0.442093\n",
      ">> Epoch 33 finished \tANN training loss 0.418087\n",
      ">> Epoch 34 finished \tANN training loss 0.420564\n",
      ">> Epoch 35 finished \tANN training loss 0.410563\n",
      ">> Epoch 36 finished \tANN training loss 0.414934\n",
      ">> Epoch 37 finished \tANN training loss 0.407478\n",
      ">> Epoch 38 finished \tANN training loss 0.392053\n",
      ">> Epoch 39 finished \tANN training loss 0.380892\n",
      ">> Epoch 40 finished \tANN training loss 0.385128\n",
      ">> Epoch 41 finished \tANN training loss 0.378251\n",
      ">> Epoch 42 finished \tANN training loss 0.379751\n",
      ">> Epoch 43 finished \tANN training loss 0.364883\n",
      ">> Epoch 44 finished \tANN training loss 0.363258\n",
      ">> Epoch 45 finished \tANN training loss 0.431169\n",
      ">> Epoch 46 finished \tANN training loss 0.344239\n",
      ">> Epoch 47 finished \tANN training loss 0.335536\n",
      ">> Epoch 48 finished \tANN training loss 0.333128\n",
      ">> Epoch 49 finished \tANN training loss 0.335324\n",
      ">> Epoch 50 finished \tANN training loss 0.328913\n",
      ">> Epoch 51 finished \tANN training loss 0.410608\n",
      ">> Epoch 52 finished \tANN training loss 0.334034\n",
      ">> Epoch 53 finished \tANN training loss 0.322197\n",
      ">> Epoch 54 finished \tANN training loss 0.320532\n",
      ">> Epoch 55 finished \tANN training loss 0.321961\n",
      ">> Epoch 56 finished \tANN training loss 0.296925\n",
      ">> Epoch 57 finished \tANN training loss 0.362511\n",
      ">> Epoch 58 finished \tANN training loss 0.303651\n",
      ">> Epoch 59 finished \tANN training loss 0.296374\n",
      ">> Epoch 60 finished \tANN training loss 0.318986\n",
      ">> Epoch 61 finished \tANN training loss 0.295154\n",
      ">> Epoch 62 finished \tANN training loss 0.283442\n",
      ">> Epoch 63 finished \tANN training loss 0.273461\n",
      ">> Epoch 64 finished \tANN training loss 0.290493\n",
      ">> Epoch 65 finished \tANN training loss 0.269003\n",
      ">> Epoch 66 finished \tANN training loss 0.278140\n",
      ">> Epoch 67 finished \tANN training loss 0.267009\n",
      ">> Epoch 68 finished \tANN training loss 0.263254\n",
      ">> Epoch 69 finished \tANN training loss 0.282642\n",
      ">> Epoch 70 finished \tANN training loss 0.249296\n",
      ">> Epoch 71 finished \tANN training loss 0.239135\n",
      ">> Epoch 72 finished \tANN training loss 0.257711\n",
      ">> Epoch 73 finished \tANN training loss 0.259434\n",
      ">> Epoch 74 finished \tANN training loss 0.254930\n",
      ">> Epoch 75 finished \tANN training loss 0.257571\n",
      ">> Epoch 76 finished \tANN training loss 0.245373\n",
      ">> Epoch 77 finished \tANN training loss 0.234263\n",
      ">> Epoch 78 finished \tANN training loss 0.237352\n",
      ">> Epoch 79 finished \tANN training loss 0.241090\n",
      ">> Epoch 80 finished \tANN training loss 0.264765\n",
      ">> Epoch 81 finished \tANN training loss 0.237123\n",
      ">> Epoch 82 finished \tANN training loss 0.215911\n",
      ">> Epoch 83 finished \tANN training loss 0.219127\n",
      ">> Epoch 84 finished \tANN training loss 0.247529\n",
      ">> Epoch 85 finished \tANN training loss 0.220558\n",
      ">> Epoch 86 finished \tANN training loss 0.251239\n",
      ">> Epoch 87 finished \tANN training loss 0.238137\n",
      ">> Epoch 88 finished \tANN training loss 0.222657\n",
      ">> Epoch 89 finished \tANN training loss 0.207120\n",
      ">> Epoch 90 finished \tANN training loss 0.200621\n",
      ">> Epoch 91 finished \tANN training loss 0.202557\n",
      ">> Epoch 92 finished \tANN training loss 0.206328\n",
      ">> Epoch 93 finished \tANN training loss 0.242061\n",
      ">> Epoch 94 finished \tANN training loss 0.213290\n",
      ">> Epoch 95 finished \tANN training loss 0.198878\n",
      ">> Epoch 96 finished \tANN training loss 0.197586\n",
      ">> Epoch 97 finished \tANN training loss 0.261329\n",
      ">> Epoch 98 finished \tANN training loss 0.251716\n",
      ">> Epoch 99 finished \tANN training loss 0.207744\n",
      ">> Epoch 100 finished \tANN training loss 0.208246\n",
      ">> Epoch 101 finished \tANN training loss 0.205470\n",
      ">> Epoch 102 finished \tANN training loss 0.183111\n",
      ">> Epoch 103 finished \tANN training loss 0.208693\n",
      ">> Epoch 104 finished \tANN training loss 0.187871\n",
      ">> Epoch 105 finished \tANN training loss 0.194653\n",
      ">> Epoch 106 finished \tANN training loss 0.188508\n",
      ">> Epoch 107 finished \tANN training loss 0.182520\n",
      ">> Epoch 108 finished \tANN training loss 0.183923\n",
      ">> Epoch 109 finished \tANN training loss 0.180851\n",
      ">> Epoch 110 finished \tANN training loss 0.250683\n",
      ">> Epoch 111 finished \tANN training loss 0.183762\n",
      ">> Epoch 112 finished \tANN training loss 0.206773\n",
      ">> Epoch 113 finished \tANN training loss 0.233868\n",
      ">> Epoch 114 finished \tANN training loss 0.196699\n",
      ">> Epoch 115 finished \tANN training loss 0.183977\n",
      ">> Epoch 116 finished \tANN training loss 0.175478\n",
      ">> Epoch 117 finished \tANN training loss 0.190156\n",
      ">> Epoch 118 finished \tANN training loss 0.176959\n",
      ">> Epoch 119 finished \tANN training loss 0.165756\n",
      ">> Epoch 120 finished \tANN training loss 0.165985\n",
      ">> Epoch 121 finished \tANN training loss 0.169166\n",
      ">> Epoch 122 finished \tANN training loss 0.170402\n",
      ">> Epoch 123 finished \tANN training loss 0.210585\n",
      ">> Epoch 124 finished \tANN training loss 0.170028\n",
      ">> Epoch 125 finished \tANN training loss 0.165673\n",
      ">> Epoch 126 finished \tANN training loss 0.161131\n",
      ">> Epoch 127 finished \tANN training loss 0.163469\n",
      ">> Epoch 128 finished \tANN training loss 0.189315\n",
      ">> Epoch 129 finished \tANN training loss 0.191662\n",
      ">> Epoch 130 finished \tANN training loss 0.156807\n",
      ">> Epoch 131 finished \tANN training loss 0.162739\n",
      ">> Epoch 132 finished \tANN training loss 0.174417\n",
      ">> Epoch 133 finished \tANN training loss 0.152034\n",
      ">> Epoch 134 finished \tANN training loss 0.162078\n",
      ">> Epoch 135 finished \tANN training loss 0.188436\n",
      ">> Epoch 136 finished \tANN training loss 0.153927\n",
      ">> Epoch 137 finished \tANN training loss 0.195880\n",
      ">> Epoch 138 finished \tANN training loss 0.158419\n",
      ">> Epoch 139 finished \tANN training loss 0.159347\n",
      ">> Epoch 140 finished \tANN training loss 0.175600\n",
      ">> Epoch 141 finished \tANN training loss 0.166410\n",
      ">> Epoch 142 finished \tANN training loss 0.180520\n",
      ">> Epoch 143 finished \tANN training loss 0.156344\n",
      ">> Epoch 144 finished \tANN training loss 0.159908\n",
      ">> Epoch 145 finished \tANN training loss 0.143092\n",
      ">> Epoch 146 finished \tANN training loss 0.154460\n",
      ">> Epoch 147 finished \tANN training loss 0.149710\n",
      ">> Epoch 148 finished \tANN training loss 0.160600\n",
      ">> Epoch 149 finished \tANN training loss 0.140929\n",
      ">> Epoch 150 finished \tANN training loss 0.131346\n",
      ">> Epoch 151 finished \tANN training loss 0.165026\n",
      ">> Epoch 152 finished \tANN training loss 0.164087\n",
      ">> Epoch 153 finished \tANN training loss 0.157544\n",
      ">> Epoch 154 finished \tANN training loss 0.128712\n",
      ">> Epoch 155 finished \tANN training loss 0.172259\n",
      ">> Epoch 156 finished \tANN training loss 0.142308\n",
      ">> Epoch 157 finished \tANN training loss 0.138848\n",
      ">> Epoch 158 finished \tANN training loss 0.139708\n",
      ">> Epoch 159 finished \tANN training loss 0.172914\n",
      ">> Epoch 160 finished \tANN training loss 0.152315\n",
      ">> Epoch 161 finished \tANN training loss 0.126172\n",
      ">> Epoch 162 finished \tANN training loss 0.143923\n",
      ">> Epoch 163 finished \tANN training loss 0.164493\n",
      ">> Epoch 164 finished \tANN training loss 0.143562\n",
      ">> Epoch 165 finished \tANN training loss 0.152948\n",
      ">> Epoch 166 finished \tANN training loss 0.142821\n",
      ">> Epoch 167 finished \tANN training loss 0.134539\n",
      ">> Epoch 168 finished \tANN training loss 0.133937\n",
      ">> Epoch 169 finished \tANN training loss 0.132620\n",
      ">> Epoch 170 finished \tANN training loss 0.121743\n",
      ">> Epoch 171 finished \tANN training loss 0.153882\n",
      ">> Epoch 172 finished \tANN training loss 0.140300\n",
      ">> Epoch 173 finished \tANN training loss 0.159101\n",
      ">> Epoch 174 finished \tANN training loss 0.123219\n",
      ">> Epoch 175 finished \tANN training loss 0.124510\n",
      ">> Epoch 176 finished \tANN training loss 0.142108\n",
      ">> Epoch 177 finished \tANN training loss 0.123160\n",
      ">> Epoch 178 finished \tANN training loss 0.139308\n",
      ">> Epoch 179 finished \tANN training loss 0.147372\n",
      ">> Epoch 180 finished \tANN training loss 0.138827\n",
      ">> Epoch 181 finished \tANN training loss 0.142027\n",
      ">> Epoch 182 finished \tANN training loss 0.120830\n",
      ">> Epoch 183 finished \tANN training loss 0.226211\n",
      ">> Epoch 184 finished \tANN training loss 0.176554\n",
      ">> Epoch 185 finished \tANN training loss 0.122745\n",
      ">> Epoch 186 finished \tANN training loss 0.136694\n",
      ">> Epoch 187 finished \tANN training loss 0.125168\n",
      ">> Epoch 188 finished \tANN training loss 0.125137\n",
      ">> Epoch 189 finished \tANN training loss 0.171414\n",
      ">> Epoch 190 finished \tANN training loss 0.133800\n",
      ">> Epoch 191 finished \tANN training loss 0.117970\n",
      ">> Epoch 192 finished \tANN training loss 0.116769\n",
      ">> Epoch 193 finished \tANN training loss 0.114424\n",
      ">> Epoch 194 finished \tANN training loss 0.118404\n",
      ">> Epoch 195 finished \tANN training loss 0.116990\n",
      ">> Epoch 196 finished \tANN training loss 0.113062\n",
      ">> Epoch 197 finished \tANN training loss 0.116886\n",
      ">> Epoch 198 finished \tANN training loss 0.119780\n",
      ">> Epoch 199 finished \tANN training loss 0.122660\n",
      ">> Epoch 200 finished \tANN training loss 0.129490\n",
      ">> Epoch 201 finished \tANN training loss 0.132637\n",
      ">> Epoch 202 finished \tANN training loss 0.137530\n",
      ">> Epoch 203 finished \tANN training loss 0.126695\n",
      ">> Epoch 204 finished \tANN training loss 0.130499\n",
      ">> Epoch 205 finished \tANN training loss 0.127668\n",
      ">> Epoch 206 finished \tANN training loss 0.128191\n",
      ">> Epoch 207 finished \tANN training loss 0.142780\n",
      ">> Epoch 208 finished \tANN training loss 0.116308\n",
      ">> Epoch 209 finished \tANN training loss 0.132306\n",
      ">> Epoch 210 finished \tANN training loss 0.120043\n",
      ">> Epoch 211 finished \tANN training loss 0.125588\n",
      ">> Epoch 212 finished \tANN training loss 0.115567\n",
      ">> Epoch 213 finished \tANN training loss 0.114964\n",
      ">> Epoch 214 finished \tANN training loss 0.121812\n",
      ">> Epoch 215 finished \tANN training loss 0.132003\n",
      ">> Epoch 216 finished \tANN training loss 0.133261\n",
      ">> Epoch 217 finished \tANN training loss 0.111250\n",
      ">> Epoch 218 finished \tANN training loss 0.113177\n",
      ">> Epoch 219 finished \tANN training loss 0.141485\n",
      ">> Epoch 220 finished \tANN training loss 0.111887\n",
      ">> Epoch 221 finished \tANN training loss 0.116119\n",
      ">> Epoch 222 finished \tANN training loss 0.113690\n",
      ">> Epoch 223 finished \tANN training loss 0.119555\n",
      ">> Epoch 224 finished \tANN training loss 0.121663\n",
      ">> Epoch 225 finished \tANN training loss 0.119097\n",
      ">> Epoch 226 finished \tANN training loss 0.138024\n",
      ">> Epoch 227 finished \tANN training loss 0.117873\n",
      ">> Epoch 228 finished \tANN training loss 0.117359\n",
      ">> Epoch 229 finished \tANN training loss 0.112872\n",
      ">> Epoch 230 finished \tANN training loss 0.113356\n",
      ">> Epoch 231 finished \tANN training loss 0.117750\n",
      ">> Epoch 232 finished \tANN training loss 0.134353\n",
      ">> Epoch 233 finished \tANN training loss 0.108573\n",
      ">> Epoch 234 finished \tANN training loss 0.113972\n",
      ">> Epoch 235 finished \tANN training loss 0.179064\n",
      ">> Epoch 236 finished \tANN training loss 0.122016\n",
      ">> Epoch 237 finished \tANN training loss 0.114018\n",
      ">> Epoch 238 finished \tANN training loss 0.152669\n",
      ">> Epoch 239 finished \tANN training loss 0.133156\n",
      ">> Epoch 240 finished \tANN training loss 0.110452\n",
      ">> Epoch 241 finished \tANN training loss 0.117931\n",
      ">> Epoch 242 finished \tANN training loss 0.106051\n",
      ">> Epoch 243 finished \tANN training loss 0.108051\n",
      ">> Epoch 244 finished \tANN training loss 0.112855\n",
      ">> Epoch 245 finished \tANN training loss 0.114055\n",
      ">> Epoch 246 finished \tANN training loss 0.111775\n",
      ">> Epoch 247 finished \tANN training loss 0.115286\n",
      ">> Epoch 248 finished \tANN training loss 0.121888\n",
      ">> Epoch 249 finished \tANN training loss 0.109471\n",
      ">> Epoch 250 finished \tANN training loss 0.113635\n",
      ">> Epoch 251 finished \tANN training loss 0.133911\n",
      ">> Epoch 252 finished \tANN training loss 0.115696\n",
      ">> Epoch 253 finished \tANN training loss 0.124700\n",
      ">> Epoch 254 finished \tANN training loss 0.104230\n",
      ">> Epoch 255 finished \tANN training loss 0.106744\n",
      ">> Epoch 256 finished \tANN training loss 0.104457\n",
      ">> Epoch 257 finished \tANN training loss 0.114935\n",
      ">> Epoch 258 finished \tANN training loss 0.118504\n",
      ">> Epoch 259 finished \tANN training loss 0.101515\n",
      ">> Epoch 260 finished \tANN training loss 0.107869\n",
      ">> Epoch 261 finished \tANN training loss 0.111500\n",
      ">> Epoch 262 finished \tANN training loss 0.113847\n",
      ">> Epoch 263 finished \tANN training loss 0.113177\n",
      ">> Epoch 264 finished \tANN training loss 0.112425\n",
      ">> Epoch 265 finished \tANN training loss 0.126921\n",
      ">> Epoch 266 finished \tANN training loss 0.117201\n",
      ">> Epoch 267 finished \tANN training loss 0.111730\n",
      ">> Epoch 268 finished \tANN training loss 0.106880\n",
      ">> Epoch 269 finished \tANN training loss 0.116043\n",
      ">> Epoch 270 finished \tANN training loss 0.105330\n",
      ">> Epoch 271 finished \tANN training loss 0.099606\n",
      ">> Epoch 272 finished \tANN training loss 0.116219\n",
      ">> Epoch 273 finished \tANN training loss 0.132846\n",
      ">> Epoch 274 finished \tANN training loss 0.093674\n",
      ">> Epoch 275 finished \tANN training loss 0.114097\n",
      ">> Epoch 276 finished \tANN training loss 0.095601\n",
      ">> Epoch 277 finished \tANN training loss 0.102461\n",
      ">> Epoch 278 finished \tANN training loss 0.110664\n",
      ">> Epoch 279 finished \tANN training loss 0.109749\n",
      ">> Epoch 280 finished \tANN training loss 0.102137\n",
      ">> Epoch 281 finished \tANN training loss 0.128512\n",
      ">> Epoch 282 finished \tANN training loss 0.106998\n",
      ">> Epoch 283 finished \tANN training loss 0.119404\n",
      ">> Epoch 284 finished \tANN training loss 0.105634\n",
      ">> Epoch 285 finished \tANN training loss 0.113326\n",
      ">> Epoch 286 finished \tANN training loss 0.114512\n",
      ">> Epoch 287 finished \tANN training loss 0.097146\n",
      ">> Epoch 288 finished \tANN training loss 0.108202\n",
      ">> Epoch 289 finished \tANN training loss 0.130706\n",
      ">> Epoch 290 finished \tANN training loss 0.120591\n",
      ">> Epoch 291 finished \tANN training loss 0.106704\n",
      ">> Epoch 292 finished \tANN training loss 0.097352\n",
      ">> Epoch 293 finished \tANN training loss 0.096310\n",
      ">> Epoch 294 finished \tANN training loss 0.124418\n",
      ">> Epoch 295 finished \tANN training loss 0.105283\n",
      ">> Epoch 296 finished \tANN training loss 0.110735\n",
      ">> Epoch 297 finished \tANN training loss 0.101238\n",
      ">> Epoch 298 finished \tANN training loss 0.099232\n",
      ">> Epoch 299 finished \tANN training loss 0.111095\n",
      ">> Epoch 300 finished \tANN training loss 0.131870\n",
      ">> Epoch 301 finished \tANN training loss 0.096635\n",
      ">> Epoch 302 finished \tANN training loss 0.102577\n",
      ">> Epoch 303 finished \tANN training loss 0.093416\n",
      ">> Epoch 304 finished \tANN training loss 0.102324\n",
      ">> Epoch 305 finished \tANN training loss 0.102839\n",
      ">> Epoch 306 finished \tANN training loss 0.113328\n",
      ">> Epoch 307 finished \tANN training loss 0.118538\n",
      ">> Epoch 308 finished \tANN training loss 0.108043\n",
      ">> Epoch 309 finished \tANN training loss 0.099967\n",
      ">> Epoch 310 finished \tANN training loss 0.107648\n",
      ">> Epoch 311 finished \tANN training loss 0.090231\n",
      ">> Epoch 312 finished \tANN training loss 0.115561\n",
      ">> Epoch 313 finished \tANN training loss 0.091280\n",
      ">> Epoch 314 finished \tANN training loss 0.101002\n",
      ">> Epoch 315 finished \tANN training loss 0.100372\n",
      ">> Epoch 316 finished \tANN training loss 0.109638\n",
      ">> Epoch 317 finished \tANN training loss 0.095131\n",
      ">> Epoch 318 finished \tANN training loss 0.098557\n",
      ">> Epoch 319 finished \tANN training loss 0.114839\n",
      ">> Epoch 320 finished \tANN training loss 0.115455\n",
      ">> Epoch 321 finished \tANN training loss 0.114535\n",
      ">> Epoch 322 finished \tANN training loss 0.098641\n",
      ">> Epoch 323 finished \tANN training loss 0.125058\n",
      ">> Epoch 324 finished \tANN training loss 0.106747\n",
      ">> Epoch 325 finished \tANN training loss 0.096062\n",
      ">> Epoch 326 finished \tANN training loss 0.098792\n",
      ">> Epoch 327 finished \tANN training loss 0.119667\n",
      ">> Epoch 328 finished \tANN training loss 0.108334\n",
      ">> Epoch 329 finished \tANN training loss 0.125371\n",
      ">> Epoch 330 finished \tANN training loss 0.085972\n",
      ">> Epoch 331 finished \tANN training loss 0.091343\n",
      ">> Epoch 332 finished \tANN training loss 0.106345\n",
      ">> Epoch 333 finished \tANN training loss 0.113878\n",
      ">> Epoch 334 finished \tANN training loss 0.118213\n",
      ">> Epoch 335 finished \tANN training loss 0.096136\n",
      ">> Epoch 336 finished \tANN training loss 0.098096\n",
      ">> Epoch 337 finished \tANN training loss 0.092380\n",
      ">> Epoch 338 finished \tANN training loss 0.100696\n",
      ">> Epoch 339 finished \tANN training loss 0.112148\n",
      ">> Epoch 340 finished \tANN training loss 0.098451\n",
      ">> Epoch 341 finished \tANN training loss 0.087920\n",
      ">> Epoch 342 finished \tANN training loss 0.104765\n",
      ">> Epoch 343 finished \tANN training loss 0.093375\n",
      ">> Epoch 344 finished \tANN training loss 0.091542\n",
      ">> Epoch 345 finished \tANN training loss 0.094899\n",
      ">> Epoch 346 finished \tANN training loss 0.113452\n",
      ">> Epoch 347 finished \tANN training loss 0.102119\n",
      ">> Epoch 348 finished \tANN training loss 0.099315\n",
      ">> Epoch 349 finished \tANN training loss 0.113738\n",
      ">> Epoch 350 finished \tANN training loss 0.099585\n",
      ">> Epoch 351 finished \tANN training loss 0.092072\n",
      ">> Epoch 352 finished \tANN training loss 0.094667\n",
      ">> Epoch 353 finished \tANN training loss 0.092625\n",
      ">> Epoch 354 finished \tANN training loss 0.099967\n",
      ">> Epoch 355 finished \tANN training loss 0.096642\n",
      ">> Epoch 356 finished \tANN training loss 0.096912\n",
      ">> Epoch 357 finished \tANN training loss 0.100911\n",
      ">> Epoch 358 finished \tANN training loss 0.089466\n",
      ">> Epoch 359 finished \tANN training loss 0.090721\n",
      ">> Epoch 360 finished \tANN training loss 0.150633\n",
      ">> Epoch 361 finished \tANN training loss 0.103146\n",
      ">> Epoch 362 finished \tANN training loss 0.120958\n",
      ">> Epoch 363 finished \tANN training loss 0.099771\n",
      ">> Epoch 364 finished \tANN training loss 0.100552\n",
      ">> Epoch 365 finished \tANN training loss 0.097443\n",
      ">> Epoch 366 finished \tANN training loss 0.124065\n",
      ">> Epoch 367 finished \tANN training loss 0.096850\n",
      ">> Epoch 368 finished \tANN training loss 0.095324\n",
      ">> Epoch 369 finished \tANN training loss 0.090697\n",
      ">> Epoch 370 finished \tANN training loss 0.087998\n",
      ">> Epoch 371 finished \tANN training loss 0.092554\n",
      ">> Epoch 372 finished \tANN training loss 0.086125\n",
      ">> Epoch 373 finished \tANN training loss 0.089461\n",
      ">> Epoch 374 finished \tANN training loss 0.113562\n",
      ">> Epoch 375 finished \tANN training loss 0.095809\n",
      ">> Epoch 376 finished \tANN training loss 0.096325\n",
      ">> Epoch 377 finished \tANN training loss 0.107553\n",
      ">> Epoch 378 finished \tANN training loss 0.100058\n",
      ">> Epoch 379 finished \tANN training loss 0.113242\n",
      ">> Epoch 380 finished \tANN training loss 0.089034\n",
      ">> Epoch 381 finished \tANN training loss 0.083036\n",
      ">> Epoch 382 finished \tANN training loss 0.119473\n",
      ">> Epoch 383 finished \tANN training loss 0.126137\n",
      ">> Epoch 384 finished \tANN training loss 0.089509\n",
      ">> Epoch 385 finished \tANN training loss 0.102016\n",
      ">> Epoch 386 finished \tANN training loss 0.110439\n",
      ">> Epoch 387 finished \tANN training loss 0.134663\n",
      ">> Epoch 388 finished \tANN training loss 0.125964\n",
      ">> Epoch 389 finished \tANN training loss 0.108260\n",
      ">> Epoch 390 finished \tANN training loss 0.108108\n",
      ">> Epoch 391 finished \tANN training loss 0.088336\n",
      ">> Epoch 392 finished \tANN training loss 0.106053\n",
      ">> Epoch 393 finished \tANN training loss 0.097701\n",
      ">> Epoch 394 finished \tANN training loss 0.087694\n",
      ">> Epoch 395 finished \tANN training loss 0.131684\n",
      ">> Epoch 396 finished \tANN training loss 0.104156\n",
      ">> Epoch 397 finished \tANN training loss 0.103221\n",
      ">> Epoch 398 finished \tANN training loss 0.097737\n",
      ">> Epoch 399 finished \tANN training loss 0.094219\n",
      ">> Epoch 400 finished \tANN training loss 0.097644\n",
      ">> Epoch 401 finished \tANN training loss 0.089206\n",
      ">> Epoch 402 finished \tANN training loss 0.080746\n",
      ">> Epoch 403 finished \tANN training loss 0.086478\n",
      ">> Epoch 404 finished \tANN training loss 0.086158\n",
      ">> Epoch 405 finished \tANN training loss 0.097917\n",
      ">> Epoch 406 finished \tANN training loss 0.121441\n",
      ">> Epoch 407 finished \tANN training loss 0.086450\n",
      ">> Epoch 408 finished \tANN training loss 0.112608\n",
      ">> Epoch 409 finished \tANN training loss 0.101453\n",
      ">> Epoch 410 finished \tANN training loss 0.087545\n",
      ">> Epoch 411 finished \tANN training loss 0.094298\n",
      ">> Epoch 412 finished \tANN training loss 0.115657\n",
      ">> Epoch 413 finished \tANN training loss 0.088789\n",
      ">> Epoch 414 finished \tANN training loss 0.091275\n",
      ">> Epoch 415 finished \tANN training loss 0.092237\n",
      ">> Epoch 416 finished \tANN training loss 0.096077\n",
      ">> Epoch 417 finished \tANN training loss 0.110035\n",
      ">> Epoch 418 finished \tANN training loss 0.083563\n",
      ">> Epoch 419 finished \tANN training loss 0.104749\n",
      ">> Epoch 420 finished \tANN training loss 0.089649\n",
      ">> Epoch 421 finished \tANN training loss 0.084922\n",
      ">> Epoch 422 finished \tANN training loss 0.087122\n",
      ">> Epoch 423 finished \tANN training loss 0.085463\n",
      ">> Epoch 424 finished \tANN training loss 0.088573\n",
      ">> Epoch 425 finished \tANN training loss 0.105168\n",
      ">> Epoch 426 finished \tANN training loss 0.090037\n",
      ">> Epoch 427 finished \tANN training loss 0.092620\n",
      ">> Epoch 428 finished \tANN training loss 0.092319\n",
      ">> Epoch 429 finished \tANN training loss 0.079298\n",
      ">> Epoch 430 finished \tANN training loss 0.088053\n",
      ">> Epoch 431 finished \tANN training loss 0.093610\n",
      ">> Epoch 432 finished \tANN training loss 0.095139\n",
      ">> Epoch 433 finished \tANN training loss 0.093311\n",
      ">> Epoch 434 finished \tANN training loss 0.088117\n",
      ">> Epoch 435 finished \tANN training loss 0.095764\n",
      ">> Epoch 436 finished \tANN training loss 0.118592\n",
      ">> Epoch 437 finished \tANN training loss 0.096722\n",
      ">> Epoch 438 finished \tANN training loss 0.092325\n",
      ">> Epoch 439 finished \tANN training loss 0.119677\n",
      ">> Epoch 440 finished \tANN training loss 0.098951\n",
      ">> Epoch 441 finished \tANN training loss 0.095272\n",
      ">> Epoch 442 finished \tANN training loss 0.102743\n",
      ">> Epoch 443 finished \tANN training loss 0.092641\n",
      ">> Epoch 444 finished \tANN training loss 0.091649\n",
      ">> Epoch 445 finished \tANN training loss 0.090273\n",
      ">> Epoch 446 finished \tANN training loss 0.095764\n",
      ">> Epoch 447 finished \tANN training loss 0.103970\n",
      ">> Epoch 448 finished \tANN training loss 0.085917\n",
      ">> Epoch 449 finished \tANN training loss 0.090509\n",
      ">> Epoch 450 finished \tANN training loss 0.088308\n",
      ">> Epoch 451 finished \tANN training loss 0.098547\n",
      ">> Epoch 452 finished \tANN training loss 0.093130\n",
      ">> Epoch 453 finished \tANN training loss 0.091411\n",
      ">> Epoch 454 finished \tANN training loss 0.079836\n",
      ">> Epoch 455 finished \tANN training loss 0.090257\n",
      ">> Epoch 456 finished \tANN training loss 0.079917\n",
      ">> Epoch 457 finished \tANN training loss 0.099591\n",
      ">> Epoch 458 finished \tANN training loss 0.080485\n",
      ">> Epoch 459 finished \tANN training loss 0.095986\n",
      ">> Epoch 460 finished \tANN training loss 0.088921\n",
      ">> Epoch 461 finished \tANN training loss 0.089778\n",
      ">> Epoch 462 finished \tANN training loss 0.088949\n",
      ">> Epoch 463 finished \tANN training loss 0.100947\n",
      ">> Epoch 464 finished \tANN training loss 0.108624\n",
      ">> Epoch 465 finished \tANN training loss 0.083655\n",
      ">> Epoch 466 finished \tANN training loss 0.093597\n",
      ">> Epoch 467 finished \tANN training loss 0.084246\n",
      ">> Epoch 468 finished \tANN training loss 0.085744\n",
      ">> Epoch 469 finished \tANN training loss 0.091489\n",
      ">> Epoch 470 finished \tANN training loss 0.087261\n",
      ">> Epoch 471 finished \tANN training loss 0.097633\n",
      ">> Epoch 472 finished \tANN training loss 0.091424\n",
      ">> Epoch 473 finished \tANN training loss 0.081821\n",
      ">> Epoch 474 finished \tANN training loss 0.079822\n",
      ">> Epoch 475 finished \tANN training loss 0.079721\n",
      ">> Epoch 476 finished \tANN training loss 0.078853\n",
      ">> Epoch 477 finished \tANN training loss 0.081372\n",
      ">> Epoch 478 finished \tANN training loss 0.079978\n",
      ">> Epoch 479 finished \tANN training loss 0.075541\n",
      ">> Epoch 480 finished \tANN training loss 0.078239\n",
      ">> Epoch 481 finished \tANN training loss 0.102958\n",
      ">> Epoch 482 finished \tANN training loss 0.089001\n",
      ">> Epoch 483 finished \tANN training loss 0.076186\n",
      ">> Epoch 484 finished \tANN training loss 0.090150\n",
      ">> Epoch 485 finished \tANN training loss 0.080382\n",
      ">> Epoch 486 finished \tANN training loss 0.088232\n",
      ">> Epoch 487 finished \tANN training loss 0.084949\n",
      ">> Epoch 488 finished \tANN training loss 0.091263\n",
      ">> Epoch 489 finished \tANN training loss 0.119576\n",
      ">> Epoch 490 finished \tANN training loss 0.086525\n",
      ">> Epoch 491 finished \tANN training loss 0.077831\n",
      ">> Epoch 492 finished \tANN training loss 0.078026\n",
      ">> Epoch 493 finished \tANN training loss 0.081041\n",
      ">> Epoch 494 finished \tANN training loss 0.093156\n",
      ">> Epoch 495 finished \tANN training loss 0.086101\n",
      ">> Epoch 496 finished \tANN training loss 0.081465\n",
      ">> Epoch 497 finished \tANN training loss 0.078192\n",
      ">> Epoch 498 finished \tANN training loss 0.074548\n",
      ">> Epoch 499 finished \tANN training loss 0.080502\n",
      ">> Epoch 500 finished \tANN training loss 0.084111\n",
      ">> Epoch 501 finished \tANN training loss 0.088862\n",
      ">> Epoch 502 finished \tANN training loss 0.081007\n",
      ">> Epoch 503 finished \tANN training loss 0.093594\n",
      ">> Epoch 504 finished \tANN training loss 0.077136\n",
      ">> Epoch 505 finished \tANN training loss 0.077986\n",
      ">> Epoch 506 finished \tANN training loss 0.076615\n",
      ">> Epoch 507 finished \tANN training loss 0.077476\n",
      ">> Epoch 508 finished \tANN training loss 0.078358\n",
      ">> Epoch 509 finished \tANN training loss 0.069075\n",
      ">> Epoch 510 finished \tANN training loss 0.099085\n",
      ">> Epoch 511 finished \tANN training loss 0.081743\n",
      ">> Epoch 512 finished \tANN training loss 0.076252\n",
      ">> Epoch 513 finished \tANN training loss 0.091208\n",
      ">> Epoch 514 finished \tANN training loss 0.089858\n",
      ">> Epoch 515 finished \tANN training loss 0.087494\n",
      ">> Epoch 516 finished \tANN training loss 0.097184\n",
      ">> Epoch 517 finished \tANN training loss 0.082700\n",
      ">> Epoch 518 finished \tANN training loss 0.074561\n",
      ">> Epoch 519 finished \tANN training loss 0.085384\n",
      ">> Epoch 520 finished \tANN training loss 0.099562\n",
      ">> Epoch 521 finished \tANN training loss 0.086946\n",
      ">> Epoch 522 finished \tANN training loss 0.080449\n",
      ">> Epoch 523 finished \tANN training loss 0.080267\n",
      ">> Epoch 524 finished \tANN training loss 0.080953\n",
      ">> Epoch 525 finished \tANN training loss 0.089123\n",
      ">> Epoch 526 finished \tANN training loss 0.082364\n",
      ">> Epoch 527 finished \tANN training loss 0.072574\n",
      ">> Epoch 528 finished \tANN training loss 0.074251\n",
      ">> Epoch 529 finished \tANN training loss 0.100631\n",
      ">> Epoch 530 finished \tANN training loss 0.087127\n",
      ">> Epoch 531 finished \tANN training loss 0.095258\n",
      ">> Epoch 532 finished \tANN training loss 0.094826\n",
      ">> Epoch 533 finished \tANN training loss 0.075440\n",
      ">> Epoch 534 finished \tANN training loss 0.076979\n",
      ">> Epoch 535 finished \tANN training loss 0.077947\n",
      ">> Epoch 536 finished \tANN training loss 0.077248\n",
      ">> Epoch 537 finished \tANN training loss 0.067066\n",
      ">> Epoch 538 finished \tANN training loss 0.078535\n",
      ">> Epoch 539 finished \tANN training loss 0.070606\n",
      ">> Epoch 540 finished \tANN training loss 0.083025\n",
      ">> Epoch 541 finished \tANN training loss 0.077037\n",
      ">> Epoch 542 finished \tANN training loss 0.089136\n",
      ">> Epoch 543 finished \tANN training loss 0.077795\n",
      ">> Epoch 544 finished \tANN training loss 0.073497\n",
      ">> Epoch 545 finished \tANN training loss 0.086089\n",
      ">> Epoch 546 finished \tANN training loss 0.079956\n",
      ">> Epoch 547 finished \tANN training loss 0.075270\n",
      ">> Epoch 548 finished \tANN training loss 0.074019\n",
      ">> Epoch 549 finished \tANN training loss 0.074592\n",
      ">> Epoch 550 finished \tANN training loss 0.096671\n",
      ">> Epoch 551 finished \tANN training loss 0.080355\n",
      ">> Epoch 552 finished \tANN training loss 0.087430\n",
      ">> Epoch 553 finished \tANN training loss 0.070767\n",
      ">> Epoch 554 finished \tANN training loss 0.085037\n",
      ">> Epoch 555 finished \tANN training loss 0.072122\n",
      ">> Epoch 556 finished \tANN training loss 0.072116\n",
      ">> Epoch 557 finished \tANN training loss 0.082606\n",
      ">> Epoch 558 finished \tANN training loss 0.092275\n",
      ">> Epoch 559 finished \tANN training loss 0.077619\n",
      ">> Epoch 560 finished \tANN training loss 0.074300\n",
      ">> Epoch 561 finished \tANN training loss 0.064676\n",
      ">> Epoch 562 finished \tANN training loss 0.068052\n",
      ">> Epoch 563 finished \tANN training loss 0.076783\n",
      ">> Epoch 564 finished \tANN training loss 0.071613\n",
      ">> Epoch 565 finished \tANN training loss 0.078173\n",
      ">> Epoch 566 finished \tANN training loss 0.068449\n",
      ">> Epoch 567 finished \tANN training loss 0.073650\n",
      ">> Epoch 568 finished \tANN training loss 0.065665\n",
      ">> Epoch 569 finished \tANN training loss 0.081750\n",
      ">> Epoch 570 finished \tANN training loss 0.070612\n",
      ">> Epoch 571 finished \tANN training loss 0.077496\n",
      ">> Epoch 572 finished \tANN training loss 0.083293\n",
      ">> Epoch 573 finished \tANN training loss 0.076608\n",
      ">> Epoch 574 finished \tANN training loss 0.072152\n",
      ">> Epoch 575 finished \tANN training loss 0.096200\n",
      ">> Epoch 576 finished \tANN training loss 0.092698\n",
      ">> Epoch 577 finished \tANN training loss 0.076880\n",
      ">> Epoch 578 finished \tANN training loss 0.080723\n",
      ">> Epoch 579 finished \tANN training loss 0.076274\n",
      ">> Epoch 580 finished \tANN training loss 0.075265\n",
      ">> Epoch 581 finished \tANN training loss 0.068280\n",
      ">> Epoch 582 finished \tANN training loss 0.077366\n",
      ">> Epoch 583 finished \tANN training loss 0.113046\n",
      ">> Epoch 584 finished \tANN training loss 0.069910\n",
      ">> Epoch 585 finished \tANN training loss 0.074361\n",
      ">> Epoch 586 finished \tANN training loss 0.076699\n",
      ">> Epoch 587 finished \tANN training loss 0.079449\n",
      ">> Epoch 588 finished \tANN training loss 0.066407\n",
      ">> Epoch 589 finished \tANN training loss 0.066765\n",
      ">> Epoch 590 finished \tANN training loss 0.067802\n",
      ">> Epoch 591 finished \tANN training loss 0.074079\n",
      ">> Epoch 592 finished \tANN training loss 0.067003\n",
      ">> Epoch 593 finished \tANN training loss 0.073431\n",
      ">> Epoch 594 finished \tANN training loss 0.099590\n",
      ">> Epoch 595 finished \tANN training loss 0.073238\n",
      ">> Epoch 596 finished \tANN training loss 0.078414\n",
      ">> Epoch 597 finished \tANN training loss 0.065116\n",
      ">> Epoch 598 finished \tANN training loss 0.070759\n",
      ">> Epoch 599 finished \tANN training loss 0.078925\n",
      ">> Epoch 600 finished \tANN training loss 0.071387\n",
      ">> Epoch 601 finished \tANN training loss 0.078111\n",
      ">> Epoch 602 finished \tANN training loss 0.078300\n",
      ">> Epoch 603 finished \tANN training loss 0.092918\n",
      ">> Epoch 604 finished \tANN training loss 0.091596\n",
      ">> Epoch 605 finished \tANN training loss 0.070851\n",
      ">> Epoch 606 finished \tANN training loss 0.078071\n",
      ">> Epoch 607 finished \tANN training loss 0.080520\n",
      ">> Epoch 608 finished \tANN training loss 0.074368\n",
      ">> Epoch 609 finished \tANN training loss 0.086901\n",
      ">> Epoch 610 finished \tANN training loss 0.085456\n",
      ">> Epoch 611 finished \tANN training loss 0.084171\n",
      ">> Epoch 612 finished \tANN training loss 0.078098\n",
      ">> Epoch 613 finished \tANN training loss 0.071669\n",
      ">> Epoch 614 finished \tANN training loss 0.079184\n",
      ">> Epoch 615 finished \tANN training loss 0.074769\n",
      ">> Epoch 616 finished \tANN training loss 0.084125\n",
      ">> Epoch 617 finished \tANN training loss 0.076842\n",
      ">> Epoch 618 finished \tANN training loss 0.109072\n",
      ">> Epoch 619 finished \tANN training loss 0.074737\n",
      ">> Epoch 620 finished \tANN training loss 0.076853\n",
      ">> Epoch 621 finished \tANN training loss 0.082953\n",
      ">> Epoch 622 finished \tANN training loss 0.084749\n",
      ">> Epoch 623 finished \tANN training loss 0.074461\n",
      ">> Epoch 624 finished \tANN training loss 0.071664\n",
      ">> Epoch 625 finished \tANN training loss 0.091033\n",
      ">> Epoch 626 finished \tANN training loss 0.079417\n",
      ">> Epoch 627 finished \tANN training loss 0.074143\n",
      ">> Epoch 628 finished \tANN training loss 0.071221\n",
      ">> Epoch 629 finished \tANN training loss 0.073145\n",
      ">> Epoch 630 finished \tANN training loss 0.068580\n",
      ">> Epoch 631 finished \tANN training loss 0.069692\n",
      ">> Epoch 632 finished \tANN training loss 0.076722\n",
      ">> Epoch 633 finished \tANN training loss 0.073996\n",
      ">> Epoch 634 finished \tANN training loss 0.082305\n",
      ">> Epoch 635 finished \tANN training loss 0.076969\n",
      ">> Epoch 636 finished \tANN training loss 0.080313\n",
      ">> Epoch 637 finished \tANN training loss 0.076274\n",
      ">> Epoch 638 finished \tANN training loss 0.071058\n",
      ">> Epoch 639 finished \tANN training loss 0.077099\n",
      ">> Epoch 640 finished \tANN training loss 0.078330\n",
      ">> Epoch 641 finished \tANN training loss 0.076448\n",
      ">> Epoch 642 finished \tANN training loss 0.084587\n",
      ">> Epoch 643 finished \tANN training loss 0.087123\n",
      ">> Epoch 644 finished \tANN training loss 0.070304\n",
      ">> Epoch 645 finished \tANN training loss 0.074791\n",
      ">> Epoch 646 finished \tANN training loss 0.073238\n",
      ">> Epoch 647 finished \tANN training loss 0.077174\n",
      ">> Epoch 648 finished \tANN training loss 0.079738\n",
      ">> Epoch 649 finished \tANN training loss 0.071808\n",
      ">> Epoch 650 finished \tANN training loss 0.082331\n",
      ">> Epoch 651 finished \tANN training loss 0.088614\n",
      ">> Epoch 652 finished \tANN training loss 0.103311\n",
      ">> Epoch 653 finished \tANN training loss 0.072617\n",
      ">> Epoch 654 finished \tANN training loss 0.074918\n",
      ">> Epoch 655 finished \tANN training loss 0.075242\n",
      ">> Epoch 656 finished \tANN training loss 0.076063\n",
      ">> Epoch 657 finished \tANN training loss 0.083996\n",
      ">> Epoch 658 finished \tANN training loss 0.075360\n",
      ">> Epoch 659 finished \tANN training loss 0.071679\n",
      ">> Epoch 660 finished \tANN training loss 0.081052\n",
      ">> Epoch 661 finished \tANN training loss 0.067392\n",
      ">> Epoch 662 finished \tANN training loss 0.078163\n",
      ">> Epoch 663 finished \tANN training loss 0.083243\n",
      ">> Epoch 664 finished \tANN training loss 0.074999\n",
      ">> Epoch 665 finished \tANN training loss 0.085645\n",
      ">> Epoch 666 finished \tANN training loss 0.072686\n",
      ">> Epoch 667 finished \tANN training loss 0.073635\n",
      ">> Epoch 668 finished \tANN training loss 0.079184\n",
      ">> Epoch 669 finished \tANN training loss 0.072551\n",
      ">> Epoch 670 finished \tANN training loss 0.071108\n",
      ">> Epoch 671 finished \tANN training loss 0.073463\n",
      ">> Epoch 672 finished \tANN training loss 0.105925\n",
      ">> Epoch 673 finished \tANN training loss 0.071575\n",
      ">> Epoch 674 finished \tANN training loss 0.071392\n",
      ">> Epoch 675 finished \tANN training loss 0.067722\n",
      ">> Epoch 676 finished \tANN training loss 0.065764\n",
      ">> Epoch 677 finished \tANN training loss 0.065690\n",
      ">> Epoch 678 finished \tANN training loss 0.065232\n",
      ">> Epoch 679 finished \tANN training loss 0.065459\n",
      ">> Epoch 680 finished \tANN training loss 0.073307\n",
      ">> Epoch 681 finished \tANN training loss 0.074199\n",
      ">> Epoch 682 finished \tANN training loss 0.063408\n",
      ">> Epoch 683 finished \tANN training loss 0.072764\n",
      ">> Epoch 684 finished \tANN training loss 0.100179\n",
      ">> Epoch 685 finished \tANN training loss 0.080369\n",
      ">> Epoch 686 finished \tANN training loss 0.080768\n",
      ">> Epoch 687 finished \tANN training loss 0.082717\n",
      ">> Epoch 688 finished \tANN training loss 0.081460\n",
      ">> Epoch 689 finished \tANN training loss 0.077003\n",
      ">> Epoch 690 finished \tANN training loss 0.069370\n",
      ">> Epoch 691 finished \tANN training loss 0.071496\n",
      ">> Epoch 692 finished \tANN training loss 0.068417\n",
      ">> Epoch 693 finished \tANN training loss 0.073791\n",
      ">> Epoch 694 finished \tANN training loss 0.070932\n",
      ">> Epoch 695 finished \tANN training loss 0.078456\n",
      ">> Epoch 696 finished \tANN training loss 0.073236\n",
      ">> Epoch 697 finished \tANN training loss 0.073400\n",
      ">> Epoch 698 finished \tANN training loss 0.068421\n",
      ">> Epoch 699 finished \tANN training loss 0.062903\n",
      ">> Epoch 700 finished \tANN training loss 0.087348\n",
      ">> Epoch 701 finished \tANN training loss 0.058885\n",
      ">> Epoch 702 finished \tANN training loss 0.067288\n",
      ">> Epoch 703 finished \tANN training loss 0.072895\n",
      ">> Epoch 704 finished \tANN training loss 0.066741\n",
      ">> Epoch 705 finished \tANN training loss 0.079320\n",
      ">> Epoch 706 finished \tANN training loss 0.071425\n",
      ">> Epoch 707 finished \tANN training loss 0.070530\n",
      ">> Epoch 708 finished \tANN training loss 0.085046\n",
      ">> Epoch 709 finished \tANN training loss 0.083605\n",
      ">> Epoch 710 finished \tANN training loss 0.071503\n",
      ">> Epoch 711 finished \tANN training loss 0.080115\n",
      ">> Epoch 712 finished \tANN training loss 0.085870\n",
      ">> Epoch 713 finished \tANN training loss 0.070380\n",
      ">> Epoch 714 finished \tANN training loss 0.074136\n",
      ">> Epoch 715 finished \tANN training loss 0.078402\n",
      ">> Epoch 716 finished \tANN training loss 0.076612\n",
      ">> Epoch 717 finished \tANN training loss 0.073751\n",
      ">> Epoch 718 finished \tANN training loss 0.076657\n",
      ">> Epoch 719 finished \tANN training loss 0.066706\n",
      ">> Epoch 720 finished \tANN training loss 0.076238\n",
      ">> Epoch 721 finished \tANN training loss 0.075703\n",
      ">> Epoch 722 finished \tANN training loss 0.066026\n",
      ">> Epoch 723 finished \tANN training loss 0.071874\n",
      ">> Epoch 724 finished \tANN training loss 0.074921\n",
      ">> Epoch 725 finished \tANN training loss 0.074458\n",
      ">> Epoch 726 finished \tANN training loss 0.062976\n",
      ">> Epoch 727 finished \tANN training loss 0.066064\n",
      ">> Epoch 728 finished \tANN training loss 0.070135\n",
      ">> Epoch 729 finished \tANN training loss 0.076433\n",
      ">> Epoch 730 finished \tANN training loss 0.069876\n",
      ">> Epoch 731 finished \tANN training loss 0.069072\n",
      ">> Epoch 732 finished \tANN training loss 0.075322\n",
      ">> Epoch 733 finished \tANN training loss 0.080106\n",
      ">> Epoch 734 finished \tANN training loss 0.071696\n",
      ">> Epoch 735 finished \tANN training loss 0.076463\n",
      ">> Epoch 736 finished \tANN training loss 0.067675\n",
      ">> Epoch 737 finished \tANN training loss 0.060029\n",
      ">> Epoch 738 finished \tANN training loss 0.065749\n",
      ">> Epoch 739 finished \tANN training loss 0.070490\n",
      ">> Epoch 740 finished \tANN training loss 0.067849\n",
      ">> Epoch 741 finished \tANN training loss 0.068135\n",
      ">> Epoch 742 finished \tANN training loss 0.059944\n",
      ">> Epoch 743 finished \tANN training loss 0.059473\n",
      ">> Epoch 744 finished \tANN training loss 0.082312\n",
      ">> Epoch 745 finished \tANN training loss 0.078735\n",
      ">> Epoch 746 finished \tANN training loss 0.067015\n",
      ">> Epoch 747 finished \tANN training loss 0.089179\n",
      ">> Epoch 748 finished \tANN training loss 0.078692\n",
      ">> Epoch 749 finished \tANN training loss 0.067734\n",
      ">> Epoch 750 finished \tANN training loss 0.062973\n",
      ">> Epoch 751 finished \tANN training loss 0.075687\n",
      ">> Epoch 752 finished \tANN training loss 0.075853\n",
      ">> Epoch 753 finished \tANN training loss 0.071258\n",
      ">> Epoch 754 finished \tANN training loss 0.066462\n",
      ">> Epoch 755 finished \tANN training loss 0.083995\n",
      ">> Epoch 756 finished \tANN training loss 0.073503\n",
      ">> Epoch 757 finished \tANN training loss 0.061358\n",
      ">> Epoch 758 finished \tANN training loss 0.063932\n",
      ">> Epoch 759 finished \tANN training loss 0.069578\n",
      ">> Epoch 760 finished \tANN training loss 0.064387\n",
      ">> Epoch 761 finished \tANN training loss 0.071283\n",
      ">> Epoch 762 finished \tANN training loss 0.071078\n",
      ">> Epoch 763 finished \tANN training loss 0.077259\n",
      ">> Epoch 764 finished \tANN training loss 0.077271\n",
      ">> Epoch 765 finished \tANN training loss 0.072865\n",
      ">> Epoch 766 finished \tANN training loss 0.071730\n",
      ">> Epoch 767 finished \tANN training loss 0.075760\n",
      ">> Epoch 768 finished \tANN training loss 0.072999\n",
      ">> Epoch 769 finished \tANN training loss 0.067690\n",
      ">> Epoch 770 finished \tANN training loss 0.076610\n",
      ">> Epoch 771 finished \tANN training loss 0.093132\n",
      ">> Epoch 772 finished \tANN training loss 0.072707\n",
      ">> Epoch 773 finished \tANN training loss 0.081515\n",
      ">> Epoch 774 finished \tANN training loss 0.069881\n",
      ">> Epoch 775 finished \tANN training loss 0.079239\n",
      ">> Epoch 776 finished \tANN training loss 0.065599\n",
      ">> Epoch 777 finished \tANN training loss 0.067357\n",
      ">> Epoch 778 finished \tANN training loss 0.071848\n",
      ">> Epoch 779 finished \tANN training loss 0.063345\n",
      ">> Epoch 780 finished \tANN training loss 0.065121\n",
      ">> Epoch 781 finished \tANN training loss 0.068554\n",
      ">> Epoch 782 finished \tANN training loss 0.058533\n",
      ">> Epoch 783 finished \tANN training loss 0.063344\n",
      ">> Epoch 784 finished \tANN training loss 0.062066\n",
      ">> Epoch 785 finished \tANN training loss 0.076923\n",
      ">> Epoch 786 finished \tANN training loss 0.069375\n",
      ">> Epoch 787 finished \tANN training loss 0.071171\n",
      ">> Epoch 788 finished \tANN training loss 0.064857\n",
      ">> Epoch 789 finished \tANN training loss 0.066999\n",
      ">> Epoch 790 finished \tANN training loss 0.062576\n",
      ">> Epoch 791 finished \tANN training loss 0.067577\n",
      ">> Epoch 792 finished \tANN training loss 0.074463\n",
      ">> Epoch 793 finished \tANN training loss 0.069412\n",
      ">> Epoch 794 finished \tANN training loss 0.071338\n",
      ">> Epoch 795 finished \tANN training loss 0.065745\n",
      ">> Epoch 796 finished \tANN training loss 0.074177\n",
      ">> Epoch 797 finished \tANN training loss 0.069356\n",
      ">> Epoch 798 finished \tANN training loss 0.067319\n",
      ">> Epoch 799 finished \tANN training loss 0.065733\n",
      ">> Epoch 800 finished \tANN training loss 0.070527\n",
      ">> Epoch 801 finished \tANN training loss 0.063652\n",
      ">> Epoch 802 finished \tANN training loss 0.075649\n",
      ">> Epoch 803 finished \tANN training loss 0.072981\n",
      ">> Epoch 804 finished \tANN training loss 0.060575\n",
      ">> Epoch 805 finished \tANN training loss 0.067263\n",
      ">> Epoch 806 finished \tANN training loss 0.066369\n",
      ">> Epoch 807 finished \tANN training loss 0.063824\n",
      ">> Epoch 808 finished \tANN training loss 0.071135\n",
      ">> Epoch 809 finished \tANN training loss 0.059506\n",
      ">> Epoch 810 finished \tANN training loss 0.060265\n",
      ">> Epoch 811 finished \tANN training loss 0.070541\n",
      ">> Epoch 812 finished \tANN training loss 0.062616\n",
      ">> Epoch 813 finished \tANN training loss 0.059975\n",
      ">> Epoch 814 finished \tANN training loss 0.070695\n",
      ">> Epoch 815 finished \tANN training loss 0.066824\n",
      ">> Epoch 816 finished \tANN training loss 0.068439\n",
      ">> Epoch 817 finished \tANN training loss 0.072412\n",
      ">> Epoch 818 finished \tANN training loss 0.059815\n",
      ">> Epoch 819 finished \tANN training loss 0.064771\n",
      ">> Epoch 820 finished \tANN training loss 0.064574\n",
      ">> Epoch 821 finished \tANN training loss 0.061550\n",
      ">> Epoch 822 finished \tANN training loss 0.063312\n",
      ">> Epoch 823 finished \tANN training loss 0.061242\n",
      ">> Epoch 824 finished \tANN training loss 0.058815\n",
      ">> Epoch 825 finished \tANN training loss 0.058990\n",
      ">> Epoch 826 finished \tANN training loss 0.076311\n",
      ">> Epoch 827 finished \tANN training loss 0.073707\n",
      ">> Epoch 828 finished \tANN training loss 0.068877\n",
      ">> Epoch 829 finished \tANN training loss 0.071305\n",
      ">> Epoch 830 finished \tANN training loss 0.079917\n",
      ">> Epoch 831 finished \tANN training loss 0.073224\n",
      ">> Epoch 832 finished \tANN training loss 0.066844\n",
      ">> Epoch 833 finished \tANN training loss 0.065431\n",
      ">> Epoch 834 finished \tANN training loss 0.059769\n",
      ">> Epoch 835 finished \tANN training loss 0.059948\n",
      ">> Epoch 836 finished \tANN training loss 0.063957\n",
      ">> Epoch 837 finished \tANN training loss 0.064773\n",
      ">> Epoch 838 finished \tANN training loss 0.079273\n",
      ">> Epoch 839 finished \tANN training loss 0.088174\n",
      ">> Epoch 840 finished \tANN training loss 0.062974\n",
      ">> Epoch 841 finished \tANN training loss 0.052802\n",
      ">> Epoch 842 finished \tANN training loss 0.064810\n",
      ">> Epoch 843 finished \tANN training loss 0.062157\n",
      ">> Epoch 844 finished \tANN training loss 0.068134\n",
      ">> Epoch 845 finished \tANN training loss 0.063412\n",
      ">> Epoch 846 finished \tANN training loss 0.064056\n",
      ">> Epoch 847 finished \tANN training loss 0.056110\n",
      ">> Epoch 848 finished \tANN training loss 0.068351\n",
      ">> Epoch 849 finished \tANN training loss 0.064774\n",
      ">> Epoch 850 finished \tANN training loss 0.070654\n",
      ">> Epoch 851 finished \tANN training loss 0.072243\n",
      ">> Epoch 852 finished \tANN training loss 0.055411\n",
      ">> Epoch 853 finished \tANN training loss 0.057095\n",
      ">> Epoch 854 finished \tANN training loss 0.066820\n",
      ">> Epoch 855 finished \tANN training loss 0.071667\n",
      ">> Epoch 856 finished \tANN training loss 0.071761\n",
      ">> Epoch 857 finished \tANN training loss 0.061167\n",
      ">> Epoch 858 finished \tANN training loss 0.060115\n",
      ">> Epoch 859 finished \tANN training loss 0.061520\n",
      ">> Epoch 860 finished \tANN training loss 0.069317\n",
      ">> Epoch 861 finished \tANN training loss 0.061144\n",
      ">> Epoch 862 finished \tANN training loss 0.076779\n",
      ">> Epoch 863 finished \tANN training loss 0.070386\n",
      ">> Epoch 864 finished \tANN training loss 0.063270\n",
      ">> Epoch 865 finished \tANN training loss 0.063430\n",
      ">> Epoch 866 finished \tANN training loss 0.076448\n",
      ">> Epoch 867 finished \tANN training loss 0.063911\n",
      ">> Epoch 868 finished \tANN training loss 0.062210\n",
      ">> Epoch 869 finished \tANN training loss 0.082139\n",
      ">> Epoch 870 finished \tANN training loss 0.057260\n",
      ">> Epoch 871 finished \tANN training loss 0.079698\n",
      ">> Epoch 872 finished \tANN training loss 0.072805\n",
      ">> Epoch 873 finished \tANN training loss 0.067812\n",
      ">> Epoch 874 finished \tANN training loss 0.060267\n",
      ">> Epoch 875 finished \tANN training loss 0.069111\n",
      ">> Epoch 876 finished \tANN training loss 0.065669\n",
      ">> Epoch 877 finished \tANN training loss 0.065298\n",
      ">> Epoch 878 finished \tANN training loss 0.074808\n",
      ">> Epoch 879 finished \tANN training loss 0.053582\n",
      ">> Epoch 880 finished \tANN training loss 0.059282\n",
      ">> Epoch 881 finished \tANN training loss 0.079541\n",
      ">> Epoch 882 finished \tANN training loss 0.055877\n",
      ">> Epoch 883 finished \tANN training loss 0.055815\n",
      ">> Epoch 884 finished \tANN training loss 0.057735\n",
      ">> Epoch 885 finished \tANN training loss 0.072148\n",
      ">> Epoch 886 finished \tANN training loss 0.061394\n",
      ">> Epoch 887 finished \tANN training loss 0.056468\n",
      ">> Epoch 888 finished \tANN training loss 0.057781\n",
      ">> Epoch 889 finished \tANN training loss 0.061330\n",
      ">> Epoch 890 finished \tANN training loss 0.066064\n",
      ">> Epoch 891 finished \tANN training loss 0.060997\n",
      ">> Epoch 892 finished \tANN training loss 0.054994\n",
      ">> Epoch 893 finished \tANN training loss 0.065959\n",
      ">> Epoch 894 finished \tANN training loss 0.058533\n",
      ">> Epoch 895 finished \tANN training loss 0.067427\n",
      ">> Epoch 896 finished \tANN training loss 0.071893\n",
      ">> Epoch 897 finished \tANN training loss 0.063597\n",
      ">> Epoch 898 finished \tANN training loss 0.065063\n",
      ">> Epoch 899 finished \tANN training loss 0.069811\n",
      ">> Epoch 900 finished \tANN training loss 0.057988\n",
      ">> Epoch 901 finished \tANN training loss 0.056850\n",
      ">> Epoch 902 finished \tANN training loss 0.063871\n",
      ">> Epoch 903 finished \tANN training loss 0.070051\n",
      ">> Epoch 904 finished \tANN training loss 0.071381\n",
      ">> Epoch 905 finished \tANN training loss 0.060512\n",
      ">> Epoch 906 finished \tANN training loss 0.056168\n",
      ">> Epoch 907 finished \tANN training loss 0.053583\n",
      ">> Epoch 908 finished \tANN training loss 0.060598\n",
      ">> Epoch 909 finished \tANN training loss 0.054790\n",
      ">> Epoch 910 finished \tANN training loss 0.073603\n",
      ">> Epoch 911 finished \tANN training loss 0.057680\n",
      ">> Epoch 912 finished \tANN training loss 0.066867\n",
      ">> Epoch 913 finished \tANN training loss 0.064904\n",
      ">> Epoch 914 finished \tANN training loss 0.067029\n",
      ">> Epoch 915 finished \tANN training loss 0.067311\n",
      ">> Epoch 916 finished \tANN training loss 0.057590\n",
      ">> Epoch 917 finished \tANN training loss 0.073854\n",
      ">> Epoch 918 finished \tANN training loss 0.062955\n",
      ">> Epoch 919 finished \tANN training loss 0.061771\n",
      ">> Epoch 920 finished \tANN training loss 0.070639\n",
      ">> Epoch 921 finished \tANN training loss 0.068580\n",
      ">> Epoch 922 finished \tANN training loss 0.065899\n",
      ">> Epoch 923 finished \tANN training loss 0.066897\n",
      ">> Epoch 924 finished \tANN training loss 0.067099\n",
      ">> Epoch 925 finished \tANN training loss 0.076334\n",
      ">> Epoch 926 finished \tANN training loss 0.067995\n",
      ">> Epoch 927 finished \tANN training loss 0.073535\n",
      ">> Epoch 928 finished \tANN training loss 0.053794\n",
      ">> Epoch 929 finished \tANN training loss 0.063932\n",
      ">> Epoch 930 finished \tANN training loss 0.059008\n",
      ">> Epoch 931 finished \tANN training loss 0.056147\n",
      ">> Epoch 932 finished \tANN training loss 0.061600\n",
      ">> Epoch 933 finished \tANN training loss 0.075120\n",
      ">> Epoch 934 finished \tANN training loss 0.078978\n",
      ">> Epoch 935 finished \tANN training loss 0.068780\n",
      ">> Epoch 936 finished \tANN training loss 0.062814\n",
      ">> Epoch 937 finished \tANN training loss 0.056892\n",
      ">> Epoch 938 finished \tANN training loss 0.067705\n",
      ">> Epoch 939 finished \tANN training loss 0.061985\n",
      ">> Epoch 940 finished \tANN training loss 0.064945\n",
      ">> Epoch 941 finished \tANN training loss 0.057783\n",
      ">> Epoch 942 finished \tANN training loss 0.081298\n",
      ">> Epoch 943 finished \tANN training loss 0.062994\n",
      ">> Epoch 944 finished \tANN training loss 0.062482\n",
      ">> Epoch 945 finished \tANN training loss 0.077127\n",
      ">> Epoch 946 finished \tANN training loss 0.076471\n",
      ">> Epoch 947 finished \tANN training loss 0.072948\n",
      ">> Epoch 948 finished \tANN training loss 0.070155\n",
      ">> Epoch 949 finished \tANN training loss 0.074044\n",
      ">> Epoch 950 finished \tANN training loss 0.073442\n",
      ">> Epoch 951 finished \tANN training loss 0.070744\n",
      ">> Epoch 952 finished \tANN training loss 0.067541\n",
      ">> Epoch 953 finished \tANN training loss 0.071951\n",
      ">> Epoch 954 finished \tANN training loss 0.067416\n",
      ">> Epoch 955 finished \tANN training loss 0.065841\n",
      ">> Epoch 956 finished \tANN training loss 0.069234\n",
      ">> Epoch 957 finished \tANN training loss 0.066782\n",
      ">> Epoch 958 finished \tANN training loss 0.068840\n",
      ">> Epoch 959 finished \tANN training loss 0.067642\n",
      ">> Epoch 960 finished \tANN training loss 0.063542\n",
      ">> Epoch 961 finished \tANN training loss 0.054652\n",
      ">> Epoch 962 finished \tANN training loss 0.059954\n",
      ">> Epoch 963 finished \tANN training loss 0.059344\n",
      ">> Epoch 964 finished \tANN training loss 0.056972\n",
      ">> Epoch 965 finished \tANN training loss 0.059051\n",
      ">> Epoch 966 finished \tANN training loss 0.065865\n",
      ">> Epoch 967 finished \tANN training loss 0.075115\n",
      ">> Epoch 968 finished \tANN training loss 0.062119\n",
      ">> Epoch 969 finished \tANN training loss 0.061550\n",
      ">> Epoch 970 finished \tANN training loss 0.069444\n",
      ">> Epoch 971 finished \tANN training loss 0.069467\n",
      ">> Epoch 972 finished \tANN training loss 0.071587\n",
      ">> Epoch 973 finished \tANN training loss 0.065150\n",
      ">> Epoch 974 finished \tANN training loss 0.057777\n",
      ">> Epoch 975 finished \tANN training loss 0.057298\n",
      ">> Epoch 976 finished \tANN training loss 0.061843\n",
      ">> Epoch 977 finished \tANN training loss 0.057317\n",
      ">> Epoch 978 finished \tANN training loss 0.067354\n",
      ">> Epoch 979 finished \tANN training loss 0.062135\n",
      ">> Epoch 980 finished \tANN training loss 0.055283\n",
      ">> Epoch 981 finished \tANN training loss 0.063659\n",
      ">> Epoch 982 finished \tANN training loss 0.054585\n",
      ">> Epoch 983 finished \tANN training loss 0.060272\n",
      ">> Epoch 984 finished \tANN training loss 0.059232\n",
      ">> Epoch 985 finished \tANN training loss 0.070706\n",
      ">> Epoch 986 finished \tANN training loss 0.058416\n",
      ">> Epoch 987 finished \tANN training loss 0.063269\n",
      ">> Epoch 988 finished \tANN training loss 0.061088\n",
      ">> Epoch 989 finished \tANN training loss 0.072936\n",
      ">> Epoch 990 finished \tANN training loss 0.065086\n",
      ">> Epoch 991 finished \tANN training loss 0.071016\n",
      ">> Epoch 992 finished \tANN training loss 0.073050\n",
      ">> Epoch 993 finished \tANN training loss 0.079812\n",
      ">> Epoch 994 finished \tANN training loss 0.066812\n",
      ">> Epoch 995 finished \tANN training loss 0.073826\n",
      ">> Epoch 996 finished \tANN training loss 0.062164\n",
      ">> Epoch 997 finished \tANN training loss 0.061220\n",
      ">> Epoch 998 finished \tANN training loss 0.064836\n",
      ">> Epoch 999 finished \tANN training loss 0.063835\n",
      "[END] Fine tuning step\n",
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 11.087155\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 10.744361\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 11.220297\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 11.554363\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 11.237232\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 11.239017\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 9.470509\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 9.164248\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 8.271697\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 8.390155\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 8.513646\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 8.509847\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 8.743388\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 8.510552\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 8.385110\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 8.243419\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 8.182857\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 8.661903\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 8.652291\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 8.610722\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 18.301247\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 20.395018\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 21.244776\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 18.386089\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 17.977198\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 16.946981\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 18.896780\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 18.594805\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 15.621778\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 17.675138\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 17.799025\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 18.179073\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 16.688343\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 16.725677\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 19.136290\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 17.432993\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 17.687416\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 17.499153\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 17.981314\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 16.643484\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.898110\n",
      ">> Epoch 1 finished \tANN training loss 0.823163\n",
      ">> Epoch 2 finished \tANN training loss 0.818313\n",
      ">> Epoch 3 finished \tANN training loss 0.769301\n",
      ">> Epoch 4 finished \tANN training loss 0.745620\n",
      ">> Epoch 5 finished \tANN training loss 0.699886\n",
      ">> Epoch 6 finished \tANN training loss 0.662377\n",
      ">> Epoch 7 finished \tANN training loss 0.631454\n",
      ">> Epoch 8 finished \tANN training loss 0.629187\n",
      ">> Epoch 9 finished \tANN training loss 0.596308\n",
      ">> Epoch 10 finished \tANN training loss 0.571632\n",
      ">> Epoch 11 finished \tANN training loss 0.556323\n",
      ">> Epoch 12 finished \tANN training loss 0.577882\n",
      ">> Epoch 13 finished \tANN training loss 0.537857\n",
      ">> Epoch 14 finished \tANN training loss 0.508617\n",
      ">> Epoch 15 finished \tANN training loss 0.512268\n",
      ">> Epoch 16 finished \tANN training loss 0.479673\n",
      ">> Epoch 17 finished \tANN training loss 0.480425\n",
      ">> Epoch 18 finished \tANN training loss 0.460095\n",
      ">> Epoch 19 finished \tANN training loss 0.453363\n",
      ">> Epoch 20 finished \tANN training loss 0.435496\n",
      ">> Epoch 21 finished \tANN training loss 0.435540\n",
      ">> Epoch 22 finished \tANN training loss 0.424726\n",
      ">> Epoch 23 finished \tANN training loss 0.419506\n",
      ">> Epoch 24 finished \tANN training loss 0.411835\n",
      ">> Epoch 25 finished \tANN training loss 0.410735\n",
      ">> Epoch 26 finished \tANN training loss 0.394744\n",
      ">> Epoch 27 finished \tANN training loss 0.397012\n",
      ">> Epoch 28 finished \tANN training loss 0.387373\n",
      ">> Epoch 29 finished \tANN training loss 0.376314\n",
      ">> Epoch 30 finished \tANN training loss 0.393795\n",
      ">> Epoch 31 finished \tANN training loss 0.357672\n",
      ">> Epoch 32 finished \tANN training loss 0.358852\n",
      ">> Epoch 33 finished \tANN training loss 0.366754\n",
      ">> Epoch 34 finished \tANN training loss 0.340504\n",
      ">> Epoch 35 finished \tANN training loss 0.338130\n",
      ">> Epoch 36 finished \tANN training loss 0.346288\n",
      ">> Epoch 37 finished \tANN training loss 0.323762\n",
      ">> Epoch 38 finished \tANN training loss 0.315324\n",
      ">> Epoch 39 finished \tANN training loss 0.329609\n",
      ">> Epoch 40 finished \tANN training loss 0.333147\n",
      ">> Epoch 41 finished \tANN training loss 0.314947\n",
      ">> Epoch 42 finished \tANN training loss 0.311578\n",
      ">> Epoch 43 finished \tANN training loss 0.334381\n",
      ">> Epoch 44 finished \tANN training loss 0.307681\n",
      ">> Epoch 45 finished \tANN training loss 0.322567\n",
      ">> Epoch 46 finished \tANN training loss 0.301413\n",
      ">> Epoch 47 finished \tANN training loss 0.293005\n",
      ">> Epoch 48 finished \tANN training loss 0.283867\n",
      ">> Epoch 49 finished \tANN training loss 0.289598\n",
      ">> Epoch 50 finished \tANN training loss 0.316402\n",
      ">> Epoch 51 finished \tANN training loss 0.281351\n",
      ">> Epoch 52 finished \tANN training loss 0.304807\n",
      ">> Epoch 53 finished \tANN training loss 0.271935\n",
      ">> Epoch 54 finished \tANN training loss 0.293738\n",
      ">> Epoch 55 finished \tANN training loss 0.267161\n",
      ">> Epoch 56 finished \tANN training loss 0.293402\n",
      ">> Epoch 57 finished \tANN training loss 0.265355\n",
      ">> Epoch 58 finished \tANN training loss 0.297193\n",
      ">> Epoch 59 finished \tANN training loss 0.267003\n",
      ">> Epoch 60 finished \tANN training loss 0.257803\n",
      ">> Epoch 61 finished \tANN training loss 0.280209\n",
      ">> Epoch 62 finished \tANN training loss 0.283718\n",
      ">> Epoch 63 finished \tANN training loss 0.253364\n",
      ">> Epoch 64 finished \tANN training loss 0.291186\n",
      ">> Epoch 65 finished \tANN training loss 0.250248\n",
      ">> Epoch 66 finished \tANN training loss 0.252920\n",
      ">> Epoch 67 finished \tANN training loss 0.266190\n",
      ">> Epoch 68 finished \tANN training loss 0.262940\n",
      ">> Epoch 69 finished \tANN training loss 0.300075\n",
      ">> Epoch 70 finished \tANN training loss 0.293948\n",
      ">> Epoch 71 finished \tANN training loss 0.267662\n",
      ">> Epoch 72 finished \tANN training loss 0.242324\n",
      ">> Epoch 73 finished \tANN training loss 0.245882\n",
      ">> Epoch 74 finished \tANN training loss 0.269083\n",
      ">> Epoch 75 finished \tANN training loss 0.245064\n",
      ">> Epoch 76 finished \tANN training loss 0.236499\n",
      ">> Epoch 77 finished \tANN training loss 0.237749\n",
      ">> Epoch 78 finished \tANN training loss 0.241054\n",
      ">> Epoch 79 finished \tANN training loss 0.230458\n",
      ">> Epoch 80 finished \tANN training loss 0.284133\n",
      ">> Epoch 81 finished \tANN training loss 0.247893\n",
      ">> Epoch 82 finished \tANN training loss 0.258289\n",
      ">> Epoch 83 finished \tANN training loss 0.270166\n",
      ">> Epoch 84 finished \tANN training loss 0.252707\n",
      ">> Epoch 85 finished \tANN training loss 0.282521\n",
      ">> Epoch 86 finished \tANN training loss 0.233033\n",
      ">> Epoch 87 finished \tANN training loss 0.221496\n",
      ">> Epoch 88 finished \tANN training loss 0.230958\n",
      ">> Epoch 89 finished \tANN training loss 0.208138\n",
      ">> Epoch 90 finished \tANN training loss 0.208930\n",
      ">> Epoch 91 finished \tANN training loss 0.223282\n",
      ">> Epoch 92 finished \tANN training loss 0.219067\n",
      ">> Epoch 93 finished \tANN training loss 0.235435\n",
      ">> Epoch 94 finished \tANN training loss 0.200733\n",
      ">> Epoch 95 finished \tANN training loss 0.210923\n",
      ">> Epoch 96 finished \tANN training loss 0.231230\n",
      ">> Epoch 97 finished \tANN training loss 0.203701\n",
      ">> Epoch 98 finished \tANN training loss 0.194186\n",
      ">> Epoch 99 finished \tANN training loss 0.207189\n",
      ">> Epoch 100 finished \tANN training loss 0.209666\n",
      ">> Epoch 101 finished \tANN training loss 0.193585\n",
      ">> Epoch 102 finished \tANN training loss 0.260889\n",
      ">> Epoch 103 finished \tANN training loss 0.218816\n",
      ">> Epoch 104 finished \tANN training loss 0.196939\n",
      ">> Epoch 105 finished \tANN training loss 0.211970\n",
      ">> Epoch 106 finished \tANN training loss 0.195828\n",
      ">> Epoch 107 finished \tANN training loss 0.224033\n",
      ">> Epoch 108 finished \tANN training loss 0.201367\n",
      ">> Epoch 109 finished \tANN training loss 0.194907\n",
      ">> Epoch 110 finished \tANN training loss 0.204165\n",
      ">> Epoch 111 finished \tANN training loss 0.202067\n",
      ">> Epoch 112 finished \tANN training loss 0.209414\n",
      ">> Epoch 113 finished \tANN training loss 0.201010\n",
      ">> Epoch 114 finished \tANN training loss 0.208869\n",
      ">> Epoch 115 finished \tANN training loss 0.240560\n",
      ">> Epoch 116 finished \tANN training loss 0.207867\n",
      ">> Epoch 117 finished \tANN training loss 0.186666\n",
      ">> Epoch 118 finished \tANN training loss 0.203429\n",
      ">> Epoch 119 finished \tANN training loss 0.188117\n",
      ">> Epoch 120 finished \tANN training loss 0.207802\n",
      ">> Epoch 121 finished \tANN training loss 0.190913\n",
      ">> Epoch 122 finished \tANN training loss 0.186590\n",
      ">> Epoch 123 finished \tANN training loss 0.204973\n",
      ">> Epoch 124 finished \tANN training loss 0.203940\n",
      ">> Epoch 125 finished \tANN training loss 0.211095\n",
      ">> Epoch 126 finished \tANN training loss 0.218822\n",
      ">> Epoch 127 finished \tANN training loss 0.201118\n",
      ">> Epoch 128 finished \tANN training loss 0.188368\n",
      ">> Epoch 129 finished \tANN training loss 0.173857\n",
      ">> Epoch 130 finished \tANN training loss 0.176804\n",
      ">> Epoch 131 finished \tANN training loss 0.173449\n",
      ">> Epoch 132 finished \tANN training loss 0.196676\n",
      ">> Epoch 133 finished \tANN training loss 0.179511\n",
      ">> Epoch 134 finished \tANN training loss 0.163263\n",
      ">> Epoch 135 finished \tANN training loss 0.181164\n",
      ">> Epoch 136 finished \tANN training loss 0.175755\n",
      ">> Epoch 137 finished \tANN training loss 0.185149\n",
      ">> Epoch 138 finished \tANN training loss 0.169346\n",
      ">> Epoch 139 finished \tANN training loss 0.228615\n",
      ">> Epoch 140 finished \tANN training loss 0.173983\n",
      ">> Epoch 141 finished \tANN training loss 0.175425\n",
      ">> Epoch 142 finished \tANN training loss 0.166335\n",
      ">> Epoch 143 finished \tANN training loss 0.159415\n",
      ">> Epoch 144 finished \tANN training loss 0.149544\n",
      ">> Epoch 145 finished \tANN training loss 0.162813\n",
      ">> Epoch 146 finished \tANN training loss 0.158180\n",
      ">> Epoch 147 finished \tANN training loss 0.162688\n",
      ">> Epoch 148 finished \tANN training loss 0.189636\n",
      ">> Epoch 149 finished \tANN training loss 0.150223\n",
      ">> Epoch 150 finished \tANN training loss 0.163502\n",
      ">> Epoch 151 finished \tANN training loss 0.162447\n",
      ">> Epoch 152 finished \tANN training loss 0.188602\n",
      ">> Epoch 153 finished \tANN training loss 0.164945\n",
      ">> Epoch 154 finished \tANN training loss 0.145581\n",
      ">> Epoch 155 finished \tANN training loss 0.166465\n",
      ">> Epoch 156 finished \tANN training loss 0.152728\n",
      ">> Epoch 157 finished \tANN training loss 0.164893\n",
      ">> Epoch 158 finished \tANN training loss 0.191732\n",
      ">> Epoch 159 finished \tANN training loss 0.161654\n",
      ">> Epoch 160 finished \tANN training loss 0.166771\n",
      ">> Epoch 161 finished \tANN training loss 0.155860\n",
      ">> Epoch 162 finished \tANN training loss 0.153782\n",
      ">> Epoch 163 finished \tANN training loss 0.221124\n",
      ">> Epoch 164 finished \tANN training loss 0.155653\n",
      ">> Epoch 165 finished \tANN training loss 0.153850\n",
      ">> Epoch 166 finished \tANN training loss 0.144004\n",
      ">> Epoch 167 finished \tANN training loss 0.139756\n",
      ">> Epoch 168 finished \tANN training loss 0.146985\n",
      ">> Epoch 169 finished \tANN training loss 0.137281\n",
      ">> Epoch 170 finished \tANN training loss 0.191729\n",
      ">> Epoch 171 finished \tANN training loss 0.159966\n",
      ">> Epoch 172 finished \tANN training loss 0.166839\n",
      ">> Epoch 173 finished \tANN training loss 0.168783\n",
      ">> Epoch 174 finished \tANN training loss 0.152474\n",
      ">> Epoch 175 finished \tANN training loss 0.134481\n",
      ">> Epoch 176 finished \tANN training loss 0.167885\n",
      ">> Epoch 177 finished \tANN training loss 0.139875\n",
      ">> Epoch 178 finished \tANN training loss 0.128498\n",
      ">> Epoch 179 finished \tANN training loss 0.148232\n",
      ">> Epoch 180 finished \tANN training loss 0.143552\n",
      ">> Epoch 181 finished \tANN training loss 0.131773\n",
      ">> Epoch 182 finished \tANN training loss 0.155455\n",
      ">> Epoch 183 finished \tANN training loss 0.141194\n",
      ">> Epoch 184 finished \tANN training loss 0.149849\n",
      ">> Epoch 185 finished \tANN training loss 0.139880\n",
      ">> Epoch 186 finished \tANN training loss 0.123575\n",
      ">> Epoch 187 finished \tANN training loss 0.126944\n",
      ">> Epoch 188 finished \tANN training loss 0.130457\n",
      ">> Epoch 189 finished \tANN training loss 0.159810\n",
      ">> Epoch 190 finished \tANN training loss 0.124042\n",
      ">> Epoch 191 finished \tANN training loss 0.133355\n",
      ">> Epoch 192 finished \tANN training loss 0.135682\n",
      ">> Epoch 193 finished \tANN training loss 0.127739\n",
      ">> Epoch 194 finished \tANN training loss 0.150495\n",
      ">> Epoch 195 finished \tANN training loss 0.145670\n",
      ">> Epoch 196 finished \tANN training loss 0.136398\n",
      ">> Epoch 197 finished \tANN training loss 0.161980\n",
      ">> Epoch 198 finished \tANN training loss 0.148773\n",
      ">> Epoch 199 finished \tANN training loss 0.143182\n",
      ">> Epoch 200 finished \tANN training loss 0.136062\n",
      ">> Epoch 201 finished \tANN training loss 0.144047\n",
      ">> Epoch 202 finished \tANN training loss 0.129201\n",
      ">> Epoch 203 finished \tANN training loss 0.128455\n",
      ">> Epoch 204 finished \tANN training loss 0.155692\n",
      ">> Epoch 205 finished \tANN training loss 0.128124\n",
      ">> Epoch 206 finished \tANN training loss 0.138514\n",
      ">> Epoch 207 finished \tANN training loss 0.125390\n",
      ">> Epoch 208 finished \tANN training loss 0.125172\n",
      ">> Epoch 209 finished \tANN training loss 0.121313\n",
      ">> Epoch 210 finished \tANN training loss 0.117718\n",
      ">> Epoch 211 finished \tANN training loss 0.132021\n",
      ">> Epoch 212 finished \tANN training loss 0.126666\n",
      ">> Epoch 213 finished \tANN training loss 0.136635\n",
      ">> Epoch 214 finished \tANN training loss 0.131235\n",
      ">> Epoch 215 finished \tANN training loss 0.139158\n",
      ">> Epoch 216 finished \tANN training loss 0.142465\n",
      ">> Epoch 217 finished \tANN training loss 0.127163\n",
      ">> Epoch 218 finished \tANN training loss 0.173944\n",
      ">> Epoch 219 finished \tANN training loss 0.121188\n",
      ">> Epoch 220 finished \tANN training loss 0.170539\n",
      ">> Epoch 221 finished \tANN training loss 0.123929\n",
      ">> Epoch 222 finished \tANN training loss 0.133542\n",
      ">> Epoch 223 finished \tANN training loss 0.123093\n",
      ">> Epoch 224 finished \tANN training loss 0.118837\n",
      ">> Epoch 225 finished \tANN training loss 0.185324\n",
      ">> Epoch 226 finished \tANN training loss 0.136336\n",
      ">> Epoch 227 finished \tANN training loss 0.138521\n",
      ">> Epoch 228 finished \tANN training loss 0.159099\n",
      ">> Epoch 229 finished \tANN training loss 0.130171\n",
      ">> Epoch 230 finished \tANN training loss 0.137765\n",
      ">> Epoch 231 finished \tANN training loss 0.137595\n",
      ">> Epoch 232 finished \tANN training loss 0.138678\n",
      ">> Epoch 233 finished \tANN training loss 0.135803\n",
      ">> Epoch 234 finished \tANN training loss 0.126934\n",
      ">> Epoch 235 finished \tANN training loss 0.123671\n",
      ">> Epoch 236 finished \tANN training loss 0.130735\n",
      ">> Epoch 237 finished \tANN training loss 0.129994\n",
      ">> Epoch 238 finished \tANN training loss 0.126045\n",
      ">> Epoch 239 finished \tANN training loss 0.131743\n",
      ">> Epoch 240 finished \tANN training loss 0.133045\n",
      ">> Epoch 241 finished \tANN training loss 0.124128\n",
      ">> Epoch 242 finished \tANN training loss 0.131343\n",
      ">> Epoch 243 finished \tANN training loss 0.118993\n",
      ">> Epoch 244 finished \tANN training loss 0.126531\n",
      ">> Epoch 245 finished \tANN training loss 0.113022\n",
      ">> Epoch 246 finished \tANN training loss 0.119351\n",
      ">> Epoch 247 finished \tANN training loss 0.119139\n",
      ">> Epoch 248 finished \tANN training loss 0.134720\n",
      ">> Epoch 249 finished \tANN training loss 0.163858\n",
      ">> Epoch 250 finished \tANN training loss 0.130480\n",
      ">> Epoch 251 finished \tANN training loss 0.122394\n",
      ">> Epoch 252 finished \tANN training loss 0.128052\n",
      ">> Epoch 253 finished \tANN training loss 0.123485\n",
      ">> Epoch 254 finished \tANN training loss 0.105539\n",
      ">> Epoch 255 finished \tANN training loss 0.139199\n",
      ">> Epoch 256 finished \tANN training loss 0.139398\n",
      ">> Epoch 257 finished \tANN training loss 0.120675\n",
      ">> Epoch 258 finished \tANN training loss 0.115183\n",
      ">> Epoch 259 finished \tANN training loss 0.112267\n",
      ">> Epoch 260 finished \tANN training loss 0.109648\n",
      ">> Epoch 261 finished \tANN training loss 0.112265\n",
      ">> Epoch 262 finished \tANN training loss 0.109789\n",
      ">> Epoch 263 finished \tANN training loss 0.114144\n",
      ">> Epoch 264 finished \tANN training loss 0.131344\n",
      ">> Epoch 265 finished \tANN training loss 0.107891\n",
      ">> Epoch 266 finished \tANN training loss 0.109115\n",
      ">> Epoch 267 finished \tANN training loss 0.106332\n",
      ">> Epoch 268 finished \tANN training loss 0.115072\n",
      ">> Epoch 269 finished \tANN training loss 0.097229\n",
      ">> Epoch 270 finished \tANN training loss 0.097578\n",
      ">> Epoch 271 finished \tANN training loss 0.102476\n",
      ">> Epoch 272 finished \tANN training loss 0.117815\n",
      ">> Epoch 273 finished \tANN training loss 0.107043\n",
      ">> Epoch 274 finished \tANN training loss 0.101401\n",
      ">> Epoch 275 finished \tANN training loss 0.102475\n",
      ">> Epoch 276 finished \tANN training loss 0.112188\n",
      ">> Epoch 277 finished \tANN training loss 0.103349\n",
      ">> Epoch 278 finished \tANN training loss 0.106083\n",
      ">> Epoch 279 finished \tANN training loss 0.147714\n",
      ">> Epoch 280 finished \tANN training loss 0.106399\n",
      ">> Epoch 281 finished \tANN training loss 0.102536\n",
      ">> Epoch 282 finished \tANN training loss 0.124189\n",
      ">> Epoch 283 finished \tANN training loss 0.108434\n",
      ">> Epoch 284 finished \tANN training loss 0.118573\n",
      ">> Epoch 285 finished \tANN training loss 0.100203\n",
      ">> Epoch 286 finished \tANN training loss 0.096348\n",
      ">> Epoch 287 finished \tANN training loss 0.088620\n",
      ">> Epoch 288 finished \tANN training loss 0.116522\n",
      ">> Epoch 289 finished \tANN training loss 0.112388\n",
      ">> Epoch 290 finished \tANN training loss 0.099935\n",
      ">> Epoch 291 finished \tANN training loss 0.111347\n",
      ">> Epoch 292 finished \tANN training loss 0.094700\n",
      ">> Epoch 293 finished \tANN training loss 0.098473\n",
      ">> Epoch 294 finished \tANN training loss 0.103297\n",
      ">> Epoch 295 finished \tANN training loss 0.094265\n",
      ">> Epoch 296 finished \tANN training loss 0.103226\n",
      ">> Epoch 297 finished \tANN training loss 0.103526\n",
      ">> Epoch 298 finished \tANN training loss 0.115650\n",
      ">> Epoch 299 finished \tANN training loss 0.112482\n",
      ">> Epoch 300 finished \tANN training loss 0.099221\n",
      ">> Epoch 301 finished \tANN training loss 0.123717\n",
      ">> Epoch 302 finished \tANN training loss 0.101450\n",
      ">> Epoch 303 finished \tANN training loss 0.105127\n",
      ">> Epoch 304 finished \tANN training loss 0.097624\n",
      ">> Epoch 305 finished \tANN training loss 0.106905\n",
      ">> Epoch 306 finished \tANN training loss 0.094900\n",
      ">> Epoch 307 finished \tANN training loss 0.090486\n",
      ">> Epoch 308 finished \tANN training loss 0.104523\n",
      ">> Epoch 309 finished \tANN training loss 0.104525\n",
      ">> Epoch 310 finished \tANN training loss 0.100203\n",
      ">> Epoch 311 finished \tANN training loss 0.090507\n",
      ">> Epoch 312 finished \tANN training loss 0.094755\n",
      ">> Epoch 313 finished \tANN training loss 0.107652\n",
      ">> Epoch 314 finished \tANN training loss 0.091597\n",
      ">> Epoch 315 finished \tANN training loss 0.119537\n",
      ">> Epoch 316 finished \tANN training loss 0.084143\n",
      ">> Epoch 317 finished \tANN training loss 0.083397\n",
      ">> Epoch 318 finished \tANN training loss 0.089965\n",
      ">> Epoch 319 finished \tANN training loss 0.127322\n",
      ">> Epoch 320 finished \tANN training loss 0.093241\n",
      ">> Epoch 321 finished \tANN training loss 0.110421\n",
      ">> Epoch 322 finished \tANN training loss 0.080488\n",
      ">> Epoch 323 finished \tANN training loss 0.086802\n",
      ">> Epoch 324 finished \tANN training loss 0.087276\n",
      ">> Epoch 325 finished \tANN training loss 0.111389\n",
      ">> Epoch 326 finished \tANN training loss 0.094810\n",
      ">> Epoch 327 finished \tANN training loss 0.107500\n",
      ">> Epoch 328 finished \tANN training loss 0.094804\n",
      ">> Epoch 329 finished \tANN training loss 0.103072\n",
      ">> Epoch 330 finished \tANN training loss 0.099293\n",
      ">> Epoch 331 finished \tANN training loss 0.095701\n",
      ">> Epoch 332 finished \tANN training loss 0.095558\n",
      ">> Epoch 333 finished \tANN training loss 0.090690\n",
      ">> Epoch 334 finished \tANN training loss 0.088530\n",
      ">> Epoch 335 finished \tANN training loss 0.087312\n",
      ">> Epoch 336 finished \tANN training loss 0.077614\n",
      ">> Epoch 337 finished \tANN training loss 0.096508\n",
      ">> Epoch 338 finished \tANN training loss 0.097885\n",
      ">> Epoch 339 finished \tANN training loss 0.105539\n",
      ">> Epoch 340 finished \tANN training loss 0.109107\n",
      ">> Epoch 341 finished \tANN training loss 0.099128\n",
      ">> Epoch 342 finished \tANN training loss 0.086876\n",
      ">> Epoch 343 finished \tANN training loss 0.095934\n",
      ">> Epoch 344 finished \tANN training loss 0.090690\n",
      ">> Epoch 345 finished \tANN training loss 0.118956\n",
      ">> Epoch 346 finished \tANN training loss 0.107539\n",
      ">> Epoch 347 finished \tANN training loss 0.089630\n",
      ">> Epoch 348 finished \tANN training loss 0.082923\n",
      ">> Epoch 349 finished \tANN training loss 0.079573\n",
      ">> Epoch 350 finished \tANN training loss 0.088267\n",
      ">> Epoch 351 finished \tANN training loss 0.086150\n",
      ">> Epoch 352 finished \tANN training loss 0.079061\n",
      ">> Epoch 353 finished \tANN training loss 0.091379\n",
      ">> Epoch 354 finished \tANN training loss 0.088036\n",
      ">> Epoch 355 finished \tANN training loss 0.088893\n",
      ">> Epoch 356 finished \tANN training loss 0.081301\n",
      ">> Epoch 357 finished \tANN training loss 0.084618\n",
      ">> Epoch 358 finished \tANN training loss 0.080713\n",
      ">> Epoch 359 finished \tANN training loss 0.091334\n",
      ">> Epoch 360 finished \tANN training loss 0.092947\n",
      ">> Epoch 361 finished \tANN training loss 0.085171\n",
      ">> Epoch 362 finished \tANN training loss 0.083446\n",
      ">> Epoch 363 finished \tANN training loss 0.098371\n",
      ">> Epoch 364 finished \tANN training loss 0.079282\n",
      ">> Epoch 365 finished \tANN training loss 0.092301\n",
      ">> Epoch 366 finished \tANN training loss 0.097535\n",
      ">> Epoch 367 finished \tANN training loss 0.085966\n",
      ">> Epoch 368 finished \tANN training loss 0.095781\n",
      ">> Epoch 369 finished \tANN training loss 0.110215\n",
      ">> Epoch 370 finished \tANN training loss 0.077557\n",
      ">> Epoch 371 finished \tANN training loss 0.076600\n",
      ">> Epoch 372 finished \tANN training loss 0.091079\n",
      ">> Epoch 373 finished \tANN training loss 0.080959\n",
      ">> Epoch 374 finished \tANN training loss 0.081503\n",
      ">> Epoch 375 finished \tANN training loss 0.099607\n",
      ">> Epoch 376 finished \tANN training loss 0.085930\n",
      ">> Epoch 377 finished \tANN training loss 0.095857\n",
      ">> Epoch 378 finished \tANN training loss 0.093634\n",
      ">> Epoch 379 finished \tANN training loss 0.077706\n",
      ">> Epoch 380 finished \tANN training loss 0.109502\n",
      ">> Epoch 381 finished \tANN training loss 0.096388\n",
      ">> Epoch 382 finished \tANN training loss 0.093823\n",
      ">> Epoch 383 finished \tANN training loss 0.096940\n",
      ">> Epoch 384 finished \tANN training loss 0.090438\n",
      ">> Epoch 385 finished \tANN training loss 0.081414\n",
      ">> Epoch 386 finished \tANN training loss 0.095100\n",
      ">> Epoch 387 finished \tANN training loss 0.079769\n",
      ">> Epoch 388 finished \tANN training loss 0.081570\n",
      ">> Epoch 389 finished \tANN training loss 0.088210\n",
      ">> Epoch 390 finished \tANN training loss 0.082461\n",
      ">> Epoch 391 finished \tANN training loss 0.071907\n",
      ">> Epoch 392 finished \tANN training loss 0.074800\n",
      ">> Epoch 393 finished \tANN training loss 0.092742\n",
      ">> Epoch 394 finished \tANN training loss 0.088813\n",
      ">> Epoch 395 finished \tANN training loss 0.101843\n",
      ">> Epoch 396 finished \tANN training loss 0.081802\n",
      ">> Epoch 397 finished \tANN training loss 0.071489\n",
      ">> Epoch 398 finished \tANN training loss 0.089650\n",
      ">> Epoch 399 finished \tANN training loss 0.083418\n",
      ">> Epoch 400 finished \tANN training loss 0.081649\n",
      ">> Epoch 401 finished \tANN training loss 0.084847\n",
      ">> Epoch 402 finished \tANN training loss 0.081401\n",
      ">> Epoch 403 finished \tANN training loss 0.071449\n",
      ">> Epoch 404 finished \tANN training loss 0.075242\n",
      ">> Epoch 405 finished \tANN training loss 0.072579\n",
      ">> Epoch 406 finished \tANN training loss 0.091401\n",
      ">> Epoch 407 finished \tANN training loss 0.070187\n",
      ">> Epoch 408 finished \tANN training loss 0.071631\n",
      ">> Epoch 409 finished \tANN training loss 0.071306\n",
      ">> Epoch 410 finished \tANN training loss 0.074114\n",
      ">> Epoch 411 finished \tANN training loss 0.070081\n",
      ">> Epoch 412 finished \tANN training loss 0.075807\n",
      ">> Epoch 413 finished \tANN training loss 0.077389\n",
      ">> Epoch 414 finished \tANN training loss 0.075562\n",
      ">> Epoch 415 finished \tANN training loss 0.074955\n",
      ">> Epoch 416 finished \tANN training loss 0.087257\n",
      ">> Epoch 417 finished \tANN training loss 0.077883\n",
      ">> Epoch 418 finished \tANN training loss 0.079140\n",
      ">> Epoch 419 finished \tANN training loss 0.074177\n",
      ">> Epoch 420 finished \tANN training loss 0.074805\n",
      ">> Epoch 421 finished \tANN training loss 0.076876\n",
      ">> Epoch 422 finished \tANN training loss 0.083338\n",
      ">> Epoch 423 finished \tANN training loss 0.077685\n",
      ">> Epoch 424 finished \tANN training loss 0.073829\n",
      ">> Epoch 425 finished \tANN training loss 0.079813\n",
      ">> Epoch 426 finished \tANN training loss 0.094242\n",
      ">> Epoch 427 finished \tANN training loss 0.076740\n",
      ">> Epoch 428 finished \tANN training loss 0.072972\n",
      ">> Epoch 429 finished \tANN training loss 0.105236\n",
      ">> Epoch 430 finished \tANN training loss 0.086404\n",
      ">> Epoch 431 finished \tANN training loss 0.082415\n",
      ">> Epoch 432 finished \tANN training loss 0.089308\n",
      ">> Epoch 433 finished \tANN training loss 0.089078\n",
      ">> Epoch 434 finished \tANN training loss 0.091143\n",
      ">> Epoch 435 finished \tANN training loss 0.076076\n",
      ">> Epoch 436 finished \tANN training loss 0.085759\n",
      ">> Epoch 437 finished \tANN training loss 0.084161\n",
      ">> Epoch 438 finished \tANN training loss 0.080903\n",
      ">> Epoch 439 finished \tANN training loss 0.085754\n",
      ">> Epoch 440 finished \tANN training loss 0.068442\n",
      ">> Epoch 441 finished \tANN training loss 0.081655\n",
      ">> Epoch 442 finished \tANN training loss 0.108976\n",
      ">> Epoch 443 finished \tANN training loss 0.076390\n",
      ">> Epoch 444 finished \tANN training loss 0.080027\n",
      ">> Epoch 445 finished \tANN training loss 0.076804\n",
      ">> Epoch 446 finished \tANN training loss 0.084010\n",
      ">> Epoch 447 finished \tANN training loss 0.072314\n",
      ">> Epoch 448 finished \tANN training loss 0.078143\n",
      ">> Epoch 449 finished \tANN training loss 0.072517\n",
      ">> Epoch 450 finished \tANN training loss 0.073095\n",
      ">> Epoch 451 finished \tANN training loss 0.090223\n",
      ">> Epoch 452 finished \tANN training loss 0.077760\n",
      ">> Epoch 453 finished \tANN training loss 0.069865\n",
      ">> Epoch 454 finished \tANN training loss 0.071890\n",
      ">> Epoch 455 finished \tANN training loss 0.067473\n",
      ">> Epoch 456 finished \tANN training loss 0.069427\n",
      ">> Epoch 457 finished \tANN training loss 0.103522\n",
      ">> Epoch 458 finished \tANN training loss 0.148360\n",
      ">> Epoch 459 finished \tANN training loss 0.074980\n",
      ">> Epoch 460 finished \tANN training loss 0.080786\n",
      ">> Epoch 461 finished \tANN training loss 0.066273\n",
      ">> Epoch 462 finished \tANN training loss 0.099705\n",
      ">> Epoch 463 finished \tANN training loss 0.081493\n",
      ">> Epoch 464 finished \tANN training loss 0.077918\n",
      ">> Epoch 465 finished \tANN training loss 0.077120\n",
      ">> Epoch 466 finished \tANN training loss 0.071928\n",
      ">> Epoch 467 finished \tANN training loss 0.070245\n",
      ">> Epoch 468 finished \tANN training loss 0.067448\n",
      ">> Epoch 469 finished \tANN training loss 0.076574\n",
      ">> Epoch 470 finished \tANN training loss 0.098009\n",
      ">> Epoch 471 finished \tANN training loss 0.072671\n",
      ">> Epoch 472 finished \tANN training loss 0.071580\n",
      ">> Epoch 473 finished \tANN training loss 0.075485\n",
      ">> Epoch 474 finished \tANN training loss 0.071889\n",
      ">> Epoch 475 finished \tANN training loss 0.106143\n",
      ">> Epoch 476 finished \tANN training loss 0.069664\n",
      ">> Epoch 477 finished \tANN training loss 0.066847\n",
      ">> Epoch 478 finished \tANN training loss 0.069677\n",
      ">> Epoch 479 finished \tANN training loss 0.082517\n",
      ">> Epoch 480 finished \tANN training loss 0.068821\n",
      ">> Epoch 481 finished \tANN training loss 0.068631\n",
      ">> Epoch 482 finished \tANN training loss 0.075778\n",
      ">> Epoch 483 finished \tANN training loss 0.075985\n",
      ">> Epoch 484 finished \tANN training loss 0.075800\n",
      ">> Epoch 485 finished \tANN training loss 0.072031\n",
      ">> Epoch 486 finished \tANN training loss 0.074960\n",
      ">> Epoch 487 finished \tANN training loss 0.088207\n",
      ">> Epoch 488 finished \tANN training loss 0.067509\n",
      ">> Epoch 489 finished \tANN training loss 0.067968\n",
      ">> Epoch 490 finished \tANN training loss 0.067547\n",
      ">> Epoch 491 finished \tANN training loss 0.068880\n",
      ">> Epoch 492 finished \tANN training loss 0.083312\n",
      ">> Epoch 493 finished \tANN training loss 0.100159\n",
      ">> Epoch 494 finished \tANN training loss 0.068384\n",
      ">> Epoch 495 finished \tANN training loss 0.069826\n",
      ">> Epoch 496 finished \tANN training loss 0.072741\n",
      ">> Epoch 497 finished \tANN training loss 0.071535\n",
      ">> Epoch 498 finished \tANN training loss 0.067570\n",
      ">> Epoch 499 finished \tANN training loss 0.084404\n",
      ">> Epoch 500 finished \tANN training loss 0.087117\n",
      ">> Epoch 501 finished \tANN training loss 0.081760\n",
      ">> Epoch 502 finished \tANN training loss 0.080763\n",
      ">> Epoch 503 finished \tANN training loss 0.088094\n",
      ">> Epoch 504 finished \tANN training loss 0.077165\n",
      ">> Epoch 505 finished \tANN training loss 0.081461\n",
      ">> Epoch 506 finished \tANN training loss 0.075897\n",
      ">> Epoch 507 finished \tANN training loss 0.077393\n",
      ">> Epoch 508 finished \tANN training loss 0.079972\n",
      ">> Epoch 509 finished \tANN training loss 0.067868\n",
      ">> Epoch 510 finished \tANN training loss 0.068799\n",
      ">> Epoch 511 finished \tANN training loss 0.075750\n",
      ">> Epoch 512 finished \tANN training loss 0.072867\n",
      ">> Epoch 513 finished \tANN training loss 0.082389\n",
      ">> Epoch 514 finished \tANN training loss 0.081152\n",
      ">> Epoch 515 finished \tANN training loss 0.077428\n",
      ">> Epoch 516 finished \tANN training loss 0.084845\n",
      ">> Epoch 517 finished \tANN training loss 0.075798\n",
      ">> Epoch 518 finished \tANN training loss 0.079227\n",
      ">> Epoch 519 finished \tANN training loss 0.081703\n",
      ">> Epoch 520 finished \tANN training loss 0.072410\n",
      ">> Epoch 521 finished \tANN training loss 0.071933\n",
      ">> Epoch 522 finished \tANN training loss 0.084598\n",
      ">> Epoch 523 finished \tANN training loss 0.079476\n",
      ">> Epoch 524 finished \tANN training loss 0.082180\n",
      ">> Epoch 525 finished \tANN training loss 0.072556\n",
      ">> Epoch 526 finished \tANN training loss 0.075071\n",
      ">> Epoch 527 finished \tANN training loss 0.072202\n",
      ">> Epoch 528 finished \tANN training loss 0.069870\n",
      ">> Epoch 529 finished \tANN training loss 0.079093\n",
      ">> Epoch 530 finished \tANN training loss 0.077883\n",
      ">> Epoch 531 finished \tANN training loss 0.072358\n",
      ">> Epoch 532 finished \tANN training loss 0.069473\n",
      ">> Epoch 533 finished \tANN training loss 0.075461\n",
      ">> Epoch 534 finished \tANN training loss 0.074900\n",
      ">> Epoch 535 finished \tANN training loss 0.069392\n",
      ">> Epoch 536 finished \tANN training loss 0.068286\n",
      ">> Epoch 537 finished \tANN training loss 0.068103\n",
      ">> Epoch 538 finished \tANN training loss 0.083170\n",
      ">> Epoch 539 finished \tANN training loss 0.061625\n",
      ">> Epoch 540 finished \tANN training loss 0.064053\n",
      ">> Epoch 541 finished \tANN training loss 0.086749\n",
      ">> Epoch 542 finished \tANN training loss 0.080064\n",
      ">> Epoch 543 finished \tANN training loss 0.096425\n",
      ">> Epoch 544 finished \tANN training loss 0.098313\n",
      ">> Epoch 545 finished \tANN training loss 0.073104\n",
      ">> Epoch 546 finished \tANN training loss 0.075321\n",
      ">> Epoch 547 finished \tANN training loss 0.073296\n",
      ">> Epoch 548 finished \tANN training loss 0.065039\n",
      ">> Epoch 549 finished \tANN training loss 0.068014\n",
      ">> Epoch 550 finished \tANN training loss 0.068733\n",
      ">> Epoch 551 finished \tANN training loss 0.063538\n",
      ">> Epoch 552 finished \tANN training loss 0.059885\n",
      ">> Epoch 553 finished \tANN training loss 0.060845\n",
      ">> Epoch 554 finished \tANN training loss 0.079257\n",
      ">> Epoch 555 finished \tANN training loss 0.084611\n",
      ">> Epoch 556 finished \tANN training loss 0.066434\n",
      ">> Epoch 557 finished \tANN training loss 0.080035\n",
      ">> Epoch 558 finished \tANN training loss 0.068942\n",
      ">> Epoch 559 finished \tANN training loss 0.072050\n",
      ">> Epoch 560 finished \tANN training loss 0.068155\n",
      ">> Epoch 561 finished \tANN training loss 0.069042\n",
      ">> Epoch 562 finished \tANN training loss 0.075596\n",
      ">> Epoch 563 finished \tANN training loss 0.066320\n",
      ">> Epoch 564 finished \tANN training loss 0.077264\n",
      ">> Epoch 565 finished \tANN training loss 0.072067\n",
      ">> Epoch 566 finished \tANN training loss 0.075999\n",
      ">> Epoch 567 finished \tANN training loss 0.067942\n",
      ">> Epoch 568 finished \tANN training loss 0.072838\n",
      ">> Epoch 569 finished \tANN training loss 0.073845\n",
      ">> Epoch 570 finished \tANN training loss 0.075830\n",
      ">> Epoch 571 finished \tANN training loss 0.066325\n",
      ">> Epoch 572 finished \tANN training loss 0.068869\n",
      ">> Epoch 573 finished \tANN training loss 0.063668\n",
      ">> Epoch 574 finished \tANN training loss 0.065432\n",
      ">> Epoch 575 finished \tANN training loss 0.065752\n",
      ">> Epoch 576 finished \tANN training loss 0.072285\n",
      ">> Epoch 577 finished \tANN training loss 0.071972\n",
      ">> Epoch 578 finished \tANN training loss 0.087642\n",
      ">> Epoch 579 finished \tANN training loss 0.078358\n",
      ">> Epoch 580 finished \tANN training loss 0.081954\n",
      ">> Epoch 581 finished \tANN training loss 0.071018\n",
      ">> Epoch 582 finished \tANN training loss 0.068670\n",
      ">> Epoch 583 finished \tANN training loss 0.061380\n",
      ">> Epoch 584 finished \tANN training loss 0.068526\n",
      ">> Epoch 585 finished \tANN training loss 0.077975\n",
      ">> Epoch 586 finished \tANN training loss 0.068228\n",
      ">> Epoch 587 finished \tANN training loss 0.068428\n",
      ">> Epoch 588 finished \tANN training loss 0.061893\n",
      ">> Epoch 589 finished \tANN training loss 0.061368\n",
      ">> Epoch 590 finished \tANN training loss 0.061847\n",
      ">> Epoch 591 finished \tANN training loss 0.063907\n",
      ">> Epoch 592 finished \tANN training loss 0.073074\n",
      ">> Epoch 593 finished \tANN training loss 0.074988\n",
      ">> Epoch 594 finished \tANN training loss 0.067636\n",
      ">> Epoch 595 finished \tANN training loss 0.075524\n",
      ">> Epoch 596 finished \tANN training loss 0.075741\n",
      ">> Epoch 597 finished \tANN training loss 0.063877\n",
      ">> Epoch 598 finished \tANN training loss 0.074478\n",
      ">> Epoch 599 finished \tANN training loss 0.071091\n",
      ">> Epoch 600 finished \tANN training loss 0.078533\n",
      ">> Epoch 601 finished \tANN training loss 0.070191\n",
      ">> Epoch 602 finished \tANN training loss 0.063670\n",
      ">> Epoch 603 finished \tANN training loss 0.067137\n",
      ">> Epoch 604 finished \tANN training loss 0.068948\n",
      ">> Epoch 605 finished \tANN training loss 0.070778\n",
      ">> Epoch 606 finished \tANN training loss 0.066765\n",
      ">> Epoch 607 finished \tANN training loss 0.071867\n",
      ">> Epoch 608 finished \tANN training loss 0.063721\n",
      ">> Epoch 609 finished \tANN training loss 0.059147\n",
      ">> Epoch 610 finished \tANN training loss 0.064415\n",
      ">> Epoch 611 finished \tANN training loss 0.067579\n",
      ">> Epoch 612 finished \tANN training loss 0.062903\n",
      ">> Epoch 613 finished \tANN training loss 0.073006\n",
      ">> Epoch 614 finished \tANN training loss 0.066108\n",
      ">> Epoch 615 finished \tANN training loss 0.079216\n",
      ">> Epoch 616 finished \tANN training loss 0.086925\n",
      ">> Epoch 617 finished \tANN training loss 0.067108\n",
      ">> Epoch 618 finished \tANN training loss 0.070769\n",
      ">> Epoch 619 finished \tANN training loss 0.062803\n",
      ">> Epoch 620 finished \tANN training loss 0.069866\n",
      ">> Epoch 621 finished \tANN training loss 0.059096\n",
      ">> Epoch 622 finished \tANN training loss 0.062905\n",
      ">> Epoch 623 finished \tANN training loss 0.072454\n",
      ">> Epoch 624 finished \tANN training loss 0.066018\n",
      ">> Epoch 625 finished \tANN training loss 0.062809\n",
      ">> Epoch 626 finished \tANN training loss 0.065940\n",
      ">> Epoch 627 finished \tANN training loss 0.067160\n",
      ">> Epoch 628 finished \tANN training loss 0.061265\n",
      ">> Epoch 629 finished \tANN training loss 0.069792\n",
      ">> Epoch 630 finished \tANN training loss 0.062788\n",
      ">> Epoch 631 finished \tANN training loss 0.069234\n",
      ">> Epoch 632 finished \tANN training loss 0.063941\n",
      ">> Epoch 633 finished \tANN training loss 0.072121\n",
      ">> Epoch 634 finished \tANN training loss 0.070476\n",
      ">> Epoch 635 finished \tANN training loss 0.073215\n",
      ">> Epoch 636 finished \tANN training loss 0.081561\n",
      ">> Epoch 637 finished \tANN training loss 0.066735\n",
      ">> Epoch 638 finished \tANN training loss 0.070942\n",
      ">> Epoch 639 finished \tANN training loss 0.077658\n",
      ">> Epoch 640 finished \tANN training loss 0.071943\n",
      ">> Epoch 641 finished \tANN training loss 0.068178\n",
      ">> Epoch 642 finished \tANN training loss 0.076605\n",
      ">> Epoch 643 finished \tANN training loss 0.080014\n",
      ">> Epoch 644 finished \tANN training loss 0.075362\n",
      ">> Epoch 645 finished \tANN training loss 0.076005\n",
      ">> Epoch 646 finished \tANN training loss 0.069865\n",
      ">> Epoch 647 finished \tANN training loss 0.071751\n",
      ">> Epoch 648 finished \tANN training loss 0.088232\n",
      ">> Epoch 649 finished \tANN training loss 0.064318\n",
      ">> Epoch 650 finished \tANN training loss 0.071636\n",
      ">> Epoch 651 finished \tANN training loss 0.065625\n",
      ">> Epoch 652 finished \tANN training loss 0.079939\n",
      ">> Epoch 653 finished \tANN training loss 0.079712\n",
      ">> Epoch 654 finished \tANN training loss 0.073310\n",
      ">> Epoch 655 finished \tANN training loss 0.078047\n",
      ">> Epoch 656 finished \tANN training loss 0.063834\n",
      ">> Epoch 657 finished \tANN training loss 0.073022\n",
      ">> Epoch 658 finished \tANN training loss 0.062965\n",
      ">> Epoch 659 finished \tANN training loss 0.077416\n",
      ">> Epoch 660 finished \tANN training loss 0.063944\n",
      ">> Epoch 661 finished \tANN training loss 0.056051\n",
      ">> Epoch 662 finished \tANN training loss 0.061412\n",
      ">> Epoch 663 finished \tANN training loss 0.056912\n",
      ">> Epoch 664 finished \tANN training loss 0.061171\n",
      ">> Epoch 665 finished \tANN training loss 0.060586\n",
      ">> Epoch 666 finished \tANN training loss 0.059248\n",
      ">> Epoch 667 finished \tANN training loss 0.069734\n",
      ">> Epoch 668 finished \tANN training loss 0.063754\n",
      ">> Epoch 669 finished \tANN training loss 0.063266\n",
      ">> Epoch 670 finished \tANN training loss 0.067517\n",
      ">> Epoch 671 finished \tANN training loss 0.070921\n",
      ">> Epoch 672 finished \tANN training loss 0.072808\n",
      ">> Epoch 673 finished \tANN training loss 0.072711\n",
      ">> Epoch 674 finished \tANN training loss 0.058922\n",
      ">> Epoch 675 finished \tANN training loss 0.061740\n",
      ">> Epoch 676 finished \tANN training loss 0.076328\n",
      ">> Epoch 677 finished \tANN training loss 0.056230\n",
      ">> Epoch 678 finished \tANN training loss 0.061767\n",
      ">> Epoch 679 finished \tANN training loss 0.056119\n",
      ">> Epoch 680 finished \tANN training loss 0.064761\n",
      ">> Epoch 681 finished \tANN training loss 0.064949\n",
      ">> Epoch 682 finished \tANN training loss 0.059173\n",
      ">> Epoch 683 finished \tANN training loss 0.054574\n",
      ">> Epoch 684 finished \tANN training loss 0.059781\n",
      ">> Epoch 685 finished \tANN training loss 0.067473\n",
      ">> Epoch 686 finished \tANN training loss 0.067001\n",
      ">> Epoch 687 finished \tANN training loss 0.062512\n",
      ">> Epoch 688 finished \tANN training loss 0.063440\n",
      ">> Epoch 689 finished \tANN training loss 0.060789\n",
      ">> Epoch 690 finished \tANN training loss 0.067545\n",
      ">> Epoch 691 finished \tANN training loss 0.064395\n",
      ">> Epoch 692 finished \tANN training loss 0.068255\n",
      ">> Epoch 693 finished \tANN training loss 0.060090\n",
      ">> Epoch 694 finished \tANN training loss 0.064493\n",
      ">> Epoch 695 finished \tANN training loss 0.060100\n",
      ">> Epoch 696 finished \tANN training loss 0.059581\n",
      ">> Epoch 697 finished \tANN training loss 0.062308\n",
      ">> Epoch 698 finished \tANN training loss 0.059214\n",
      ">> Epoch 699 finished \tANN training loss 0.071242\n",
      ">> Epoch 700 finished \tANN training loss 0.059390\n",
      ">> Epoch 701 finished \tANN training loss 0.059747\n",
      ">> Epoch 702 finished \tANN training loss 0.066950\n",
      ">> Epoch 703 finished \tANN training loss 0.066665\n",
      ">> Epoch 704 finished \tANN training loss 0.074328\n",
      ">> Epoch 705 finished \tANN training loss 0.073055\n",
      ">> Epoch 706 finished \tANN training loss 0.077441\n",
      ">> Epoch 707 finished \tANN training loss 0.063201\n",
      ">> Epoch 708 finished \tANN training loss 0.064281\n",
      ">> Epoch 709 finished \tANN training loss 0.057459\n",
      ">> Epoch 710 finished \tANN training loss 0.058280\n",
      ">> Epoch 711 finished \tANN training loss 0.063191\n",
      ">> Epoch 712 finished \tANN training loss 0.065043\n",
      ">> Epoch 713 finished \tANN training loss 0.068721\n",
      ">> Epoch 714 finished \tANN training loss 0.058315\n",
      ">> Epoch 715 finished \tANN training loss 0.059848\n",
      ">> Epoch 716 finished \tANN training loss 0.057508\n",
      ">> Epoch 717 finished \tANN training loss 0.060461\n",
      ">> Epoch 718 finished \tANN training loss 0.060212\n",
      ">> Epoch 719 finished \tANN training loss 0.056152\n",
      ">> Epoch 720 finished \tANN training loss 0.064048\n",
      ">> Epoch 721 finished \tANN training loss 0.060998\n",
      ">> Epoch 722 finished \tANN training loss 0.060654\n",
      ">> Epoch 723 finished \tANN training loss 0.057686\n",
      ">> Epoch 724 finished \tANN training loss 0.063907\n",
      ">> Epoch 725 finished \tANN training loss 0.065476\n",
      ">> Epoch 726 finished \tANN training loss 0.064233\n",
      ">> Epoch 727 finished \tANN training loss 0.057343\n",
      ">> Epoch 728 finished \tANN training loss 0.057076\n",
      ">> Epoch 729 finished \tANN training loss 0.064581\n",
      ">> Epoch 730 finished \tANN training loss 0.057065\n",
      ">> Epoch 731 finished \tANN training loss 0.067975\n",
      ">> Epoch 732 finished \tANN training loss 0.065911\n",
      ">> Epoch 733 finished \tANN training loss 0.057112\n",
      ">> Epoch 734 finished \tANN training loss 0.059922\n",
      ">> Epoch 735 finished \tANN training loss 0.059075\n",
      ">> Epoch 736 finished \tANN training loss 0.053103\n",
      ">> Epoch 737 finished \tANN training loss 0.054404\n",
      ">> Epoch 738 finished \tANN training loss 0.059794\n",
      ">> Epoch 739 finished \tANN training loss 0.056949\n",
      ">> Epoch 740 finished \tANN training loss 0.055833\n",
      ">> Epoch 741 finished \tANN training loss 0.058666\n",
      ">> Epoch 742 finished \tANN training loss 0.052638\n",
      ">> Epoch 743 finished \tANN training loss 0.060350\n",
      ">> Epoch 744 finished \tANN training loss 0.059128\n",
      ">> Epoch 745 finished \tANN training loss 0.057057\n",
      ">> Epoch 746 finished \tANN training loss 0.050704\n",
      ">> Epoch 747 finished \tANN training loss 0.056069\n",
      ">> Epoch 748 finished \tANN training loss 0.050953\n",
      ">> Epoch 749 finished \tANN training loss 0.060439\n",
      ">> Epoch 750 finished \tANN training loss 0.054963\n",
      ">> Epoch 751 finished \tANN training loss 0.053683\n",
      ">> Epoch 752 finished \tANN training loss 0.055632\n",
      ">> Epoch 753 finished \tANN training loss 0.057619\n",
      ">> Epoch 754 finished \tANN training loss 0.051938\n",
      ">> Epoch 755 finished \tANN training loss 0.064440\n",
      ">> Epoch 756 finished \tANN training loss 0.056433\n",
      ">> Epoch 757 finished \tANN training loss 0.062837\n",
      ">> Epoch 758 finished \tANN training loss 0.059521\n",
      ">> Epoch 759 finished \tANN training loss 0.062290\n",
      ">> Epoch 760 finished \tANN training loss 0.061145\n",
      ">> Epoch 761 finished \tANN training loss 0.054189\n",
      ">> Epoch 762 finished \tANN training loss 0.057242\n",
      ">> Epoch 763 finished \tANN training loss 0.064209\n",
      ">> Epoch 764 finished \tANN training loss 0.060990\n",
      ">> Epoch 765 finished \tANN training loss 0.066266\n",
      ">> Epoch 766 finished \tANN training loss 0.057009\n",
      ">> Epoch 767 finished \tANN training loss 0.056612\n",
      ">> Epoch 768 finished \tANN training loss 0.065304\n",
      ">> Epoch 769 finished \tANN training loss 0.060902\n",
      ">> Epoch 770 finished \tANN training loss 0.066110\n",
      ">> Epoch 771 finished \tANN training loss 0.062841\n",
      ">> Epoch 772 finished \tANN training loss 0.069534\n",
      ">> Epoch 773 finished \tANN training loss 0.073469\n",
      ">> Epoch 774 finished \tANN training loss 0.061480\n",
      ">> Epoch 775 finished \tANN training loss 0.064943\n",
      ">> Epoch 776 finished \tANN training loss 0.065751\n",
      ">> Epoch 777 finished \tANN training loss 0.076860\n",
      ">> Epoch 778 finished \tANN training loss 0.059846\n",
      ">> Epoch 779 finished \tANN training loss 0.061135\n",
      ">> Epoch 780 finished \tANN training loss 0.056873\n",
      ">> Epoch 781 finished \tANN training loss 0.061148\n",
      ">> Epoch 782 finished \tANN training loss 0.074797\n",
      ">> Epoch 783 finished \tANN training loss 0.061133\n",
      ">> Epoch 784 finished \tANN training loss 0.059138\n",
      ">> Epoch 785 finished \tANN training loss 0.062704\n",
      ">> Epoch 786 finished \tANN training loss 0.069221\n",
      ">> Epoch 787 finished \tANN training loss 0.086043\n",
      ">> Epoch 788 finished \tANN training loss 0.068776\n",
      ">> Epoch 789 finished \tANN training loss 0.059267\n",
      ">> Epoch 790 finished \tANN training loss 0.055685\n",
      ">> Epoch 791 finished \tANN training loss 0.056971\n",
      ">> Epoch 792 finished \tANN training loss 0.059273\n",
      ">> Epoch 793 finished \tANN training loss 0.058263\n",
      ">> Epoch 794 finished \tANN training loss 0.066358\n",
      ">> Epoch 795 finished \tANN training loss 0.053944\n",
      ">> Epoch 796 finished \tANN training loss 0.054778\n",
      ">> Epoch 797 finished \tANN training loss 0.055158\n",
      ">> Epoch 798 finished \tANN training loss 0.056904\n",
      ">> Epoch 799 finished \tANN training loss 0.059718\n",
      ">> Epoch 800 finished \tANN training loss 0.062352\n",
      ">> Epoch 801 finished \tANN training loss 0.062453\n",
      ">> Epoch 802 finished \tANN training loss 0.057879\n",
      ">> Epoch 803 finished \tANN training loss 0.064911\n",
      ">> Epoch 804 finished \tANN training loss 0.052889\n",
      ">> Epoch 805 finished \tANN training loss 0.061960\n",
      ">> Epoch 806 finished \tANN training loss 0.057090\n",
      ">> Epoch 807 finished \tANN training loss 0.055162\n",
      ">> Epoch 808 finished \tANN training loss 0.069205\n",
      ">> Epoch 809 finished \tANN training loss 0.054901\n",
      ">> Epoch 810 finished \tANN training loss 0.057130\n",
      ">> Epoch 811 finished \tANN training loss 0.051695\n",
      ">> Epoch 812 finished \tANN training loss 0.057646\n",
      ">> Epoch 813 finished \tANN training loss 0.050507\n",
      ">> Epoch 814 finished \tANN training loss 0.053415\n",
      ">> Epoch 815 finished \tANN training loss 0.049471\n",
      ">> Epoch 816 finished \tANN training loss 0.052319\n",
      ">> Epoch 817 finished \tANN training loss 0.047010\n",
      ">> Epoch 818 finished \tANN training loss 0.057798\n",
      ">> Epoch 819 finished \tANN training loss 0.054375\n",
      ">> Epoch 820 finished \tANN training loss 0.051598\n",
      ">> Epoch 821 finished \tANN training loss 0.054821\n",
      ">> Epoch 822 finished \tANN training loss 0.053987\n",
      ">> Epoch 823 finished \tANN training loss 0.051549\n",
      ">> Epoch 824 finished \tANN training loss 0.053808\n",
      ">> Epoch 825 finished \tANN training loss 0.048745\n",
      ">> Epoch 826 finished \tANN training loss 0.061788\n",
      ">> Epoch 827 finished \tANN training loss 0.070066\n",
      ">> Epoch 828 finished \tANN training loss 0.061519\n",
      ">> Epoch 829 finished \tANN training loss 0.057301\n",
      ">> Epoch 830 finished \tANN training loss 0.051288\n",
      ">> Epoch 831 finished \tANN training loss 0.053183\n",
      ">> Epoch 832 finished \tANN training loss 0.060299\n",
      ">> Epoch 833 finished \tANN training loss 0.054242\n",
      ">> Epoch 834 finished \tANN training loss 0.051311\n",
      ">> Epoch 835 finished \tANN training loss 0.052006\n",
      ">> Epoch 836 finished \tANN training loss 0.061126\n",
      ">> Epoch 837 finished \tANN training loss 0.063994\n",
      ">> Epoch 838 finished \tANN training loss 0.064271\n",
      ">> Epoch 839 finished \tANN training loss 0.055603\n",
      ">> Epoch 840 finished \tANN training loss 0.057946\n",
      ">> Epoch 841 finished \tANN training loss 0.053323\n",
      ">> Epoch 842 finished \tANN training loss 0.056581\n",
      ">> Epoch 843 finished \tANN training loss 0.050772\n",
      ">> Epoch 844 finished \tANN training loss 0.050978\n",
      ">> Epoch 845 finished \tANN training loss 0.053449\n",
      ">> Epoch 846 finished \tANN training loss 0.058337\n",
      ">> Epoch 847 finished \tANN training loss 0.054299\n",
      ">> Epoch 848 finished \tANN training loss 0.057368\n",
      ">> Epoch 849 finished \tANN training loss 0.053929\n",
      ">> Epoch 850 finished \tANN training loss 0.055416\n",
      ">> Epoch 851 finished \tANN training loss 0.061886\n",
      ">> Epoch 852 finished \tANN training loss 0.061975\n",
      ">> Epoch 853 finished \tANN training loss 0.057916\n",
      ">> Epoch 854 finished \tANN training loss 0.062671\n",
      ">> Epoch 855 finished \tANN training loss 0.062098\n",
      ">> Epoch 856 finished \tANN training loss 0.057840\n",
      ">> Epoch 857 finished \tANN training loss 0.060988\n",
      ">> Epoch 858 finished \tANN training loss 0.057736\n",
      ">> Epoch 859 finished \tANN training loss 0.055984\n",
      ">> Epoch 860 finished \tANN training loss 0.055456\n",
      ">> Epoch 861 finished \tANN training loss 0.053452\n",
      ">> Epoch 862 finished \tANN training loss 0.066708\n",
      ">> Epoch 863 finished \tANN training loss 0.053242\n",
      ">> Epoch 864 finished \tANN training loss 0.050711\n",
      ">> Epoch 865 finished \tANN training loss 0.054397\n",
      ">> Epoch 866 finished \tANN training loss 0.063853\n",
      ">> Epoch 867 finished \tANN training loss 0.059584\n",
      ">> Epoch 868 finished \tANN training loss 0.055742\n",
      ">> Epoch 869 finished \tANN training loss 0.057039\n",
      ">> Epoch 870 finished \tANN training loss 0.063115\n",
      ">> Epoch 871 finished \tANN training loss 0.057365\n",
      ">> Epoch 872 finished \tANN training loss 0.053839\n",
      ">> Epoch 873 finished \tANN training loss 0.062692\n",
      ">> Epoch 874 finished \tANN training loss 0.054986\n",
      ">> Epoch 875 finished \tANN training loss 0.066026\n",
      ">> Epoch 876 finished \tANN training loss 0.052063\n",
      ">> Epoch 877 finished \tANN training loss 0.059118\n",
      ">> Epoch 878 finished \tANN training loss 0.051406\n",
      ">> Epoch 879 finished \tANN training loss 0.046762\n",
      ">> Epoch 880 finished \tANN training loss 0.047272\n",
      ">> Epoch 881 finished \tANN training loss 0.052788\n",
      ">> Epoch 882 finished \tANN training loss 0.052870\n",
      ">> Epoch 883 finished \tANN training loss 0.046011\n",
      ">> Epoch 884 finished \tANN training loss 0.058541\n",
      ">> Epoch 885 finished \tANN training loss 0.071384\n",
      ">> Epoch 886 finished \tANN training loss 0.056552\n",
      ">> Epoch 887 finished \tANN training loss 0.069217\n",
      ">> Epoch 888 finished \tANN training loss 0.079728\n",
      ">> Epoch 889 finished \tANN training loss 0.053355\n",
      ">> Epoch 890 finished \tANN training loss 0.056708\n",
      ">> Epoch 891 finished \tANN training loss 0.052611\n",
      ">> Epoch 892 finished \tANN training loss 0.051574\n",
      ">> Epoch 893 finished \tANN training loss 0.055776\n",
      ">> Epoch 894 finished \tANN training loss 0.053180\n",
      ">> Epoch 895 finished \tANN training loss 0.053419\n",
      ">> Epoch 896 finished \tANN training loss 0.051422\n",
      ">> Epoch 897 finished \tANN training loss 0.045864\n",
      ">> Epoch 898 finished \tANN training loss 0.060220\n",
      ">> Epoch 899 finished \tANN training loss 0.061511\n",
      ">> Epoch 900 finished \tANN training loss 0.051040\n",
      ">> Epoch 901 finished \tANN training loss 0.054435\n",
      ">> Epoch 902 finished \tANN training loss 0.057277\n",
      ">> Epoch 903 finished \tANN training loss 0.054337\n",
      ">> Epoch 904 finished \tANN training loss 0.052023\n",
      ">> Epoch 905 finished \tANN training loss 0.053736\n",
      ">> Epoch 906 finished \tANN training loss 0.062178\n",
      ">> Epoch 907 finished \tANN training loss 0.055626\n",
      ">> Epoch 908 finished \tANN training loss 0.071333\n",
      ">> Epoch 909 finished \tANN training loss 0.062017\n",
      ">> Epoch 910 finished \tANN training loss 0.057699\n",
      ">> Epoch 911 finished \tANN training loss 0.054433\n",
      ">> Epoch 912 finished \tANN training loss 0.058148\n",
      ">> Epoch 913 finished \tANN training loss 0.061912\n",
      ">> Epoch 914 finished \tANN training loss 0.049863\n",
      ">> Epoch 915 finished \tANN training loss 0.053897\n",
      ">> Epoch 916 finished \tANN training loss 0.054754\n",
      ">> Epoch 917 finished \tANN training loss 0.059841\n",
      ">> Epoch 918 finished \tANN training loss 0.056925\n",
      ">> Epoch 919 finished \tANN training loss 0.066030\n",
      ">> Epoch 920 finished \tANN training loss 0.055883\n",
      ">> Epoch 921 finished \tANN training loss 0.057097\n",
      ">> Epoch 922 finished \tANN training loss 0.048786\n",
      ">> Epoch 923 finished \tANN training loss 0.057349\n",
      ">> Epoch 924 finished \tANN training loss 0.059017\n",
      ">> Epoch 925 finished \tANN training loss 0.071881\n",
      ">> Epoch 926 finished \tANN training loss 0.059686\n",
      ">> Epoch 927 finished \tANN training loss 0.054536\n",
      ">> Epoch 928 finished \tANN training loss 0.063688\n",
      ">> Epoch 929 finished \tANN training loss 0.076275\n",
      ">> Epoch 930 finished \tANN training loss 0.057612\n",
      ">> Epoch 931 finished \tANN training loss 0.059885\n",
      ">> Epoch 932 finished \tANN training loss 0.069967\n",
      ">> Epoch 933 finished \tANN training loss 0.063345\n",
      ">> Epoch 934 finished \tANN training loss 0.054550\n",
      ">> Epoch 935 finished \tANN training loss 0.050328\n",
      ">> Epoch 936 finished \tANN training loss 0.054466\n",
      ">> Epoch 937 finished \tANN training loss 0.063742\n",
      ">> Epoch 938 finished \tANN training loss 0.078210\n",
      ">> Epoch 939 finished \tANN training loss 0.055358\n",
      ">> Epoch 940 finished \tANN training loss 0.057234\n",
      ">> Epoch 941 finished \tANN training loss 0.065361\n",
      ">> Epoch 942 finished \tANN training loss 0.068145\n",
      ">> Epoch 943 finished \tANN training loss 0.056344\n",
      ">> Epoch 944 finished \tANN training loss 0.059194\n",
      ">> Epoch 945 finished \tANN training loss 0.071655\n",
      ">> Epoch 946 finished \tANN training loss 0.057646\n",
      ">> Epoch 947 finished \tANN training loss 0.057372\n",
      ">> Epoch 948 finished \tANN training loss 0.062356\n",
      ">> Epoch 949 finished \tANN training loss 0.051957\n",
      ">> Epoch 950 finished \tANN training loss 0.061587\n",
      ">> Epoch 951 finished \tANN training loss 0.053373\n",
      ">> Epoch 952 finished \tANN training loss 0.054572\n",
      ">> Epoch 953 finished \tANN training loss 0.052360\n",
      ">> Epoch 954 finished \tANN training loss 0.055987\n",
      ">> Epoch 955 finished \tANN training loss 0.055302\n",
      ">> Epoch 956 finished \tANN training loss 0.054907\n",
      ">> Epoch 957 finished \tANN training loss 0.069871\n",
      ">> Epoch 958 finished \tANN training loss 0.059881\n",
      ">> Epoch 959 finished \tANN training loss 0.052025\n",
      ">> Epoch 960 finished \tANN training loss 0.059034\n",
      ">> Epoch 961 finished \tANN training loss 0.050451\n",
      ">> Epoch 962 finished \tANN training loss 0.052295\n",
      ">> Epoch 963 finished \tANN training loss 0.052022\n",
      ">> Epoch 964 finished \tANN training loss 0.056006\n",
      ">> Epoch 965 finished \tANN training loss 0.051126\n",
      ">> Epoch 966 finished \tANN training loss 0.051753\n",
      ">> Epoch 967 finished \tANN training loss 0.061973\n",
      ">> Epoch 968 finished \tANN training loss 0.053739\n",
      ">> Epoch 969 finished \tANN training loss 0.052869\n",
      ">> Epoch 970 finished \tANN training loss 0.053304\n",
      ">> Epoch 971 finished \tANN training loss 0.055649\n",
      ">> Epoch 972 finished \tANN training loss 0.054358\n",
      ">> Epoch 973 finished \tANN training loss 0.058852\n",
      ">> Epoch 974 finished \tANN training loss 0.057108\n",
      ">> Epoch 975 finished \tANN training loss 0.057780\n",
      ">> Epoch 976 finished \tANN training loss 0.055774\n",
      ">> Epoch 977 finished \tANN training loss 0.049512\n",
      ">> Epoch 978 finished \tANN training loss 0.049458\n",
      ">> Epoch 979 finished \tANN training loss 0.058673\n",
      ">> Epoch 980 finished \tANN training loss 0.049369\n",
      ">> Epoch 981 finished \tANN training loss 0.047108\n",
      ">> Epoch 982 finished \tANN training loss 0.052285\n",
      ">> Epoch 983 finished \tANN training loss 0.060522\n",
      ">> Epoch 984 finished \tANN training loss 0.061477\n",
      ">> Epoch 985 finished \tANN training loss 0.049845\n",
      ">> Epoch 986 finished \tANN training loss 0.056151\n",
      ">> Epoch 987 finished \tANN training loss 0.056291\n",
      ">> Epoch 988 finished \tANN training loss 0.054927\n",
      ">> Epoch 989 finished \tANN training loss 0.069031\n",
      ">> Epoch 990 finished \tANN training loss 0.057872\n",
      ">> Epoch 991 finished \tANN training loss 0.058750\n",
      ">> Epoch 992 finished \tANN training loss 0.065065\n",
      ">> Epoch 993 finished \tANN training loss 0.060237\n",
      ">> Epoch 994 finished \tANN training loss 0.049412\n",
      ">> Epoch 995 finished \tANN training loss 0.047557\n",
      ">> Epoch 996 finished \tANN training loss 0.044557\n",
      ">> Epoch 997 finished \tANN training loss 0.044407\n",
      ">> Epoch 998 finished \tANN training loss 0.049001\n",
      ">> Epoch 999 finished \tANN training loss 0.048506\n",
      "[END] Fine tuning step\n",
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 10.083802\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 13.817960\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 14.234372\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 13.832772\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 13.126359\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 12.881816\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 12.976967\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 11.483830\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 10.563320\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 9.737078\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 9.510432\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 8.769263\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 9.282324\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 9.526083\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 9.149473\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 9.364622\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 9.182426\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 10.046270\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 9.678665\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 9.552288\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 15.610179\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 17.354540\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 17.629452\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 15.837178\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 17.112131\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 16.897982\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 16.009497\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 15.412057\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 15.886342\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 16.806396\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 17.028086\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 16.831844\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 16.446348\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 16.413454\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 17.780867\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 17.218061\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 18.226824\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 18.919752\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 17.469315\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 18.088331\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 1.056713\n",
      ">> Epoch 1 finished \tANN training loss 0.921178\n",
      ">> Epoch 2 finished \tANN training loss 0.970490\n",
      ">> Epoch 3 finished \tANN training loss 0.888601\n",
      ">> Epoch 4 finished \tANN training loss 0.844230\n",
      ">> Epoch 5 finished \tANN training loss 0.810589\n",
      ">> Epoch 6 finished \tANN training loss 0.790113\n",
      ">> Epoch 7 finished \tANN training loss 0.747947\n",
      ">> Epoch 8 finished \tANN training loss 0.755632\n",
      ">> Epoch 9 finished \tANN training loss 0.717640\n",
      ">> Epoch 10 finished \tANN training loss 0.688907\n",
      ">> Epoch 11 finished \tANN training loss 0.657855\n",
      ">> Epoch 12 finished \tANN training loss 0.650440\n",
      ">> Epoch 13 finished \tANN training loss 0.618023\n",
      ">> Epoch 14 finished \tANN training loss 0.616154\n",
      ">> Epoch 15 finished \tANN training loss 0.588263\n",
      ">> Epoch 16 finished \tANN training loss 0.572155\n",
      ">> Epoch 17 finished \tANN training loss 0.531029\n",
      ">> Epoch 18 finished \tANN training loss 0.541724\n",
      ">> Epoch 19 finished \tANN training loss 0.505947\n",
      ">> Epoch 20 finished \tANN training loss 0.494677\n",
      ">> Epoch 21 finished \tANN training loss 0.497317\n",
      ">> Epoch 22 finished \tANN training loss 0.478449\n",
      ">> Epoch 23 finished \tANN training loss 0.469142\n",
      ">> Epoch 24 finished \tANN training loss 0.458440\n",
      ">> Epoch 25 finished \tANN training loss 0.456423\n",
      ">> Epoch 26 finished \tANN training loss 0.462444\n",
      ">> Epoch 27 finished \tANN training loss 0.437864\n",
      ">> Epoch 28 finished \tANN training loss 0.434558\n",
      ">> Epoch 29 finished \tANN training loss 0.408616\n",
      ">> Epoch 30 finished \tANN training loss 0.409254\n",
      ">> Epoch 31 finished \tANN training loss 0.390365\n",
      ">> Epoch 32 finished \tANN training loss 0.385050\n",
      ">> Epoch 33 finished \tANN training loss 0.385970\n",
      ">> Epoch 34 finished \tANN training loss 0.400511\n",
      ">> Epoch 35 finished \tANN training loss 0.383473\n",
      ">> Epoch 36 finished \tANN training loss 0.369951\n",
      ">> Epoch 37 finished \tANN training loss 0.348731\n",
      ">> Epoch 38 finished \tANN training loss 0.337344\n",
      ">> Epoch 39 finished \tANN training loss 0.388860\n",
      ">> Epoch 40 finished \tANN training loss 0.345716\n",
      ">> Epoch 41 finished \tANN training loss 0.335250\n",
      ">> Epoch 42 finished \tANN training loss 0.330782\n",
      ">> Epoch 43 finished \tANN training loss 0.320766\n",
      ">> Epoch 44 finished \tANN training loss 0.313628\n",
      ">> Epoch 45 finished \tANN training loss 0.309240\n",
      ">> Epoch 46 finished \tANN training loss 0.311181\n",
      ">> Epoch 47 finished \tANN training loss 0.299029\n",
      ">> Epoch 48 finished \tANN training loss 0.290126\n",
      ">> Epoch 49 finished \tANN training loss 0.298945\n",
      ">> Epoch 50 finished \tANN training loss 0.344392\n",
      ">> Epoch 51 finished \tANN training loss 0.292103\n",
      ">> Epoch 52 finished \tANN training loss 0.280527\n",
      ">> Epoch 53 finished \tANN training loss 0.284769\n",
      ">> Epoch 54 finished \tANN training loss 0.265606\n",
      ">> Epoch 55 finished \tANN training loss 0.264058\n",
      ">> Epoch 56 finished \tANN training loss 0.268965\n",
      ">> Epoch 57 finished \tANN training loss 0.279476\n",
      ">> Epoch 58 finished \tANN training loss 0.266087\n",
      ">> Epoch 59 finished \tANN training loss 0.251545\n",
      ">> Epoch 60 finished \tANN training loss 0.252989\n",
      ">> Epoch 61 finished \tANN training loss 0.285072\n",
      ">> Epoch 62 finished \tANN training loss 0.255098\n",
      ">> Epoch 63 finished \tANN training loss 0.298305\n",
      ">> Epoch 64 finished \tANN training loss 0.257866\n",
      ">> Epoch 65 finished \tANN training loss 0.249921\n",
      ">> Epoch 66 finished \tANN training loss 0.223815\n",
      ">> Epoch 67 finished \tANN training loss 0.225257\n",
      ">> Epoch 68 finished \tANN training loss 0.264929\n",
      ">> Epoch 69 finished \tANN training loss 0.239874\n",
      ">> Epoch 70 finished \tANN training loss 0.230227\n",
      ">> Epoch 71 finished \tANN training loss 0.224385\n",
      ">> Epoch 72 finished \tANN training loss 0.228774\n",
      ">> Epoch 73 finished \tANN training loss 0.217986\n",
      ">> Epoch 74 finished \tANN training loss 0.219818\n",
      ">> Epoch 75 finished \tANN training loss 0.204139\n",
      ">> Epoch 76 finished \tANN training loss 0.214810\n",
      ">> Epoch 77 finished \tANN training loss 0.246552\n",
      ">> Epoch 78 finished \tANN training loss 0.209356\n",
      ">> Epoch 79 finished \tANN training loss 0.211705\n",
      ">> Epoch 80 finished \tANN training loss 0.209512\n",
      ">> Epoch 81 finished \tANN training loss 0.188681\n",
      ">> Epoch 82 finished \tANN training loss 0.200609\n",
      ">> Epoch 83 finished \tANN training loss 0.181573\n",
      ">> Epoch 84 finished \tANN training loss 0.201309\n",
      ">> Epoch 85 finished \tANN training loss 0.191831\n",
      ">> Epoch 86 finished \tANN training loss 0.192493\n",
      ">> Epoch 87 finished \tANN training loss 0.204450\n",
      ">> Epoch 88 finished \tANN training loss 0.203169\n",
      ">> Epoch 89 finished \tANN training loss 0.174971\n",
      ">> Epoch 90 finished \tANN training loss 0.219159\n",
      ">> Epoch 91 finished \tANN training loss 0.183268\n",
      ">> Epoch 92 finished \tANN training loss 0.193767\n",
      ">> Epoch 93 finished \tANN training loss 0.170913\n",
      ">> Epoch 94 finished \tANN training loss 0.211036\n",
      ">> Epoch 95 finished \tANN training loss 0.215470\n",
      ">> Epoch 96 finished \tANN training loss 0.179160\n",
      ">> Epoch 97 finished \tANN training loss 0.158101\n",
      ">> Epoch 98 finished \tANN training loss 0.169448\n",
      ">> Epoch 99 finished \tANN training loss 0.221464\n",
      ">> Epoch 100 finished \tANN training loss 0.172532\n",
      ">> Epoch 101 finished \tANN training loss 0.160134\n",
      ">> Epoch 102 finished \tANN training loss 0.169168\n",
      ">> Epoch 103 finished \tANN training loss 0.221465\n",
      ">> Epoch 104 finished \tANN training loss 0.162479\n",
      ">> Epoch 105 finished \tANN training loss 0.149404\n",
      ">> Epoch 106 finished \tANN training loss 0.152869\n",
      ">> Epoch 107 finished \tANN training loss 0.179307\n",
      ">> Epoch 108 finished \tANN training loss 0.167554\n",
      ">> Epoch 109 finished \tANN training loss 0.156252\n",
      ">> Epoch 110 finished \tANN training loss 0.176877\n",
      ">> Epoch 111 finished \tANN training loss 0.138842\n",
      ">> Epoch 112 finished \tANN training loss 0.151437\n",
      ">> Epoch 113 finished \tANN training loss 0.156768\n",
      ">> Epoch 114 finished \tANN training loss 0.168644\n",
      ">> Epoch 115 finished \tANN training loss 0.141060\n",
      ">> Epoch 116 finished \tANN training loss 0.152014\n",
      ">> Epoch 117 finished \tANN training loss 0.152738\n",
      ">> Epoch 118 finished \tANN training loss 0.166651\n",
      ">> Epoch 119 finished \tANN training loss 0.144048\n",
      ">> Epoch 120 finished \tANN training loss 0.148695\n",
      ">> Epoch 121 finished \tANN training loss 0.145882\n",
      ">> Epoch 122 finished \tANN training loss 0.134969\n",
      ">> Epoch 123 finished \tANN training loss 0.153505\n",
      ">> Epoch 124 finished \tANN training loss 0.141982\n",
      ">> Epoch 125 finished \tANN training loss 0.140695\n",
      ">> Epoch 126 finished \tANN training loss 0.147723\n",
      ">> Epoch 127 finished \tANN training loss 0.136870\n",
      ">> Epoch 128 finished \tANN training loss 0.172383\n",
      ">> Epoch 129 finished \tANN training loss 0.163871\n",
      ">> Epoch 130 finished \tANN training loss 0.153163\n",
      ">> Epoch 131 finished \tANN training loss 0.142344\n",
      ">> Epoch 132 finished \tANN training loss 0.134314\n",
      ">> Epoch 133 finished \tANN training loss 0.120289\n",
      ">> Epoch 134 finished \tANN training loss 0.134290\n",
      ">> Epoch 135 finished \tANN training loss 0.148700\n",
      ">> Epoch 136 finished \tANN training loss 0.122809\n",
      ">> Epoch 137 finished \tANN training loss 0.160670\n",
      ">> Epoch 138 finished \tANN training loss 0.165426\n",
      ">> Epoch 139 finished \tANN training loss 0.146424\n",
      ">> Epoch 140 finished \tANN training loss 0.153790\n",
      ">> Epoch 141 finished \tANN training loss 0.117739\n",
      ">> Epoch 142 finished \tANN training loss 0.116227\n",
      ">> Epoch 143 finished \tANN training loss 0.142862\n",
      ">> Epoch 144 finished \tANN training loss 0.153650\n",
      ">> Epoch 145 finished \tANN training loss 0.123143\n",
      ">> Epoch 146 finished \tANN training loss 0.126476\n",
      ">> Epoch 147 finished \tANN training loss 0.127986\n",
      ">> Epoch 148 finished \tANN training loss 0.132387\n",
      ">> Epoch 149 finished \tANN training loss 0.123750\n",
      ">> Epoch 150 finished \tANN training loss 0.125265\n",
      ">> Epoch 151 finished \tANN training loss 0.124850\n",
      ">> Epoch 152 finished \tANN training loss 0.121347\n",
      ">> Epoch 153 finished \tANN training loss 0.150238\n",
      ">> Epoch 154 finished \tANN training loss 0.175262\n",
      ">> Epoch 155 finished \tANN training loss 0.120172\n",
      ">> Epoch 156 finished \tANN training loss 0.143754\n",
      ">> Epoch 157 finished \tANN training loss 0.110780\n",
      ">> Epoch 158 finished \tANN training loss 0.138589\n",
      ">> Epoch 159 finished \tANN training loss 0.119497\n",
      ">> Epoch 160 finished \tANN training loss 0.111460\n",
      ">> Epoch 161 finished \tANN training loss 0.125836\n",
      ">> Epoch 162 finished \tANN training loss 0.135743\n",
      ">> Epoch 163 finished \tANN training loss 0.108826\n",
      ">> Epoch 164 finished \tANN training loss 0.112649\n",
      ">> Epoch 165 finished \tANN training loss 0.127732\n",
      ">> Epoch 166 finished \tANN training loss 0.110146\n",
      ">> Epoch 167 finished \tANN training loss 0.123655\n",
      ">> Epoch 168 finished \tANN training loss 0.105448\n",
      ">> Epoch 169 finished \tANN training loss 0.109952\n",
      ">> Epoch 170 finished \tANN training loss 0.121409\n",
      ">> Epoch 171 finished \tANN training loss 0.116344\n",
      ">> Epoch 172 finished \tANN training loss 0.112349\n",
      ">> Epoch 173 finished \tANN training loss 0.125798\n",
      ">> Epoch 174 finished \tANN training loss 0.115635\n",
      ">> Epoch 175 finished \tANN training loss 0.108389\n",
      ">> Epoch 176 finished \tANN training loss 0.105722\n",
      ">> Epoch 177 finished \tANN training loss 0.121117\n",
      ">> Epoch 178 finished \tANN training loss 0.114987\n",
      ">> Epoch 179 finished \tANN training loss 0.113137\n",
      ">> Epoch 180 finished \tANN training loss 0.102097\n",
      ">> Epoch 181 finished \tANN training loss 0.108884\n",
      ">> Epoch 182 finished \tANN training loss 0.104846\n",
      ">> Epoch 183 finished \tANN training loss 0.104193\n",
      ">> Epoch 184 finished \tANN training loss 0.105321\n",
      ">> Epoch 185 finished \tANN training loss 0.118676\n",
      ">> Epoch 186 finished \tANN training loss 0.103528\n",
      ">> Epoch 187 finished \tANN training loss 0.122952\n",
      ">> Epoch 188 finished \tANN training loss 0.110279\n",
      ">> Epoch 189 finished \tANN training loss 0.113091\n",
      ">> Epoch 190 finished \tANN training loss 0.097287\n",
      ">> Epoch 191 finished \tANN training loss 0.099618\n",
      ">> Epoch 192 finished \tANN training loss 0.092158\n",
      ">> Epoch 193 finished \tANN training loss 0.098267\n",
      ">> Epoch 194 finished \tANN training loss 0.093365\n",
      ">> Epoch 195 finished \tANN training loss 0.104644\n",
      ">> Epoch 196 finished \tANN training loss 0.094614\n",
      ">> Epoch 197 finished \tANN training loss 0.106225\n",
      ">> Epoch 198 finished \tANN training loss 0.093927\n",
      ">> Epoch 199 finished \tANN training loss 0.096694\n",
      ">> Epoch 200 finished \tANN training loss 0.116885\n",
      ">> Epoch 201 finished \tANN training loss 0.119074\n",
      ">> Epoch 202 finished \tANN training loss 0.224187\n",
      ">> Epoch 203 finished \tANN training loss 0.144553\n",
      ">> Epoch 204 finished \tANN training loss 0.099253\n",
      ">> Epoch 205 finished \tANN training loss 0.096848\n",
      ">> Epoch 206 finished \tANN training loss 0.103259\n",
      ">> Epoch 207 finished \tANN training loss 0.095160\n",
      ">> Epoch 208 finished \tANN training loss 0.106270\n",
      ">> Epoch 209 finished \tANN training loss 0.099935\n",
      ">> Epoch 210 finished \tANN training loss 0.104437\n",
      ">> Epoch 211 finished \tANN training loss 0.128278\n",
      ">> Epoch 212 finished \tANN training loss 0.133299\n",
      ">> Epoch 213 finished \tANN training loss 0.090350\n",
      ">> Epoch 214 finished \tANN training loss 0.104395\n",
      ">> Epoch 215 finished \tANN training loss 0.100416\n",
      ">> Epoch 216 finished \tANN training loss 0.099087\n",
      ">> Epoch 217 finished \tANN training loss 0.098334\n",
      ">> Epoch 218 finished \tANN training loss 0.109295\n",
      ">> Epoch 219 finished \tANN training loss 0.109827\n",
      ">> Epoch 220 finished \tANN training loss 0.091805\n",
      ">> Epoch 221 finished \tANN training loss 0.093477\n",
      ">> Epoch 222 finished \tANN training loss 0.091090\n",
      ">> Epoch 223 finished \tANN training loss 0.094277\n",
      ">> Epoch 224 finished \tANN training loss 0.106969\n",
      ">> Epoch 225 finished \tANN training loss 0.091712\n",
      ">> Epoch 226 finished \tANN training loss 0.100013\n",
      ">> Epoch 227 finished \tANN training loss 0.101172\n",
      ">> Epoch 228 finished \tANN training loss 0.126040\n",
      ">> Epoch 229 finished \tANN training loss 0.092038\n",
      ">> Epoch 230 finished \tANN training loss 0.098780\n",
      ">> Epoch 231 finished \tANN training loss 0.101084\n",
      ">> Epoch 232 finished \tANN training loss 0.093065\n",
      ">> Epoch 233 finished \tANN training loss 0.099133\n",
      ">> Epoch 234 finished \tANN training loss 0.115701\n",
      ">> Epoch 235 finished \tANN training loss 0.097286\n",
      ">> Epoch 236 finished \tANN training loss 0.112493\n",
      ">> Epoch 237 finished \tANN training loss 0.093381\n",
      ">> Epoch 238 finished \tANN training loss 0.091244\n",
      ">> Epoch 239 finished \tANN training loss 0.093831\n",
      ">> Epoch 240 finished \tANN training loss 0.090897\n",
      ">> Epoch 241 finished \tANN training loss 0.091920\n",
      ">> Epoch 242 finished \tANN training loss 0.089114\n",
      ">> Epoch 243 finished \tANN training loss 0.085744\n",
      ">> Epoch 244 finished \tANN training loss 0.085518\n",
      ">> Epoch 245 finished \tANN training loss 0.083423\n",
      ">> Epoch 246 finished \tANN training loss 0.082946\n",
      ">> Epoch 247 finished \tANN training loss 0.086724\n",
      ">> Epoch 248 finished \tANN training loss 0.091992\n",
      ">> Epoch 249 finished \tANN training loss 0.109107\n",
      ">> Epoch 250 finished \tANN training loss 0.099757\n",
      ">> Epoch 251 finished \tANN training loss 0.096570\n",
      ">> Epoch 252 finished \tANN training loss 0.090851\n",
      ">> Epoch 253 finished \tANN training loss 0.093819\n",
      ">> Epoch 254 finished \tANN training loss 0.092749\n",
      ">> Epoch 255 finished \tANN training loss 0.114588\n",
      ">> Epoch 256 finished \tANN training loss 0.106845\n",
      ">> Epoch 257 finished \tANN training loss 0.102033\n",
      ">> Epoch 258 finished \tANN training loss 0.119678\n",
      ">> Epoch 259 finished \tANN training loss 0.101337\n",
      ">> Epoch 260 finished \tANN training loss 0.092524\n",
      ">> Epoch 261 finished \tANN training loss 0.091731\n",
      ">> Epoch 262 finished \tANN training loss 0.090776\n",
      ">> Epoch 263 finished \tANN training loss 0.103817\n",
      ">> Epoch 264 finished \tANN training loss 0.088916\n",
      ">> Epoch 265 finished \tANN training loss 0.081227\n",
      ">> Epoch 266 finished \tANN training loss 0.087295\n",
      ">> Epoch 267 finished \tANN training loss 0.086218\n",
      ">> Epoch 268 finished \tANN training loss 0.088461\n",
      ">> Epoch 269 finished \tANN training loss 0.093720\n",
      ">> Epoch 270 finished \tANN training loss 0.084279\n",
      ">> Epoch 271 finished \tANN training loss 0.085638\n",
      ">> Epoch 272 finished \tANN training loss 0.098160\n",
      ">> Epoch 273 finished \tANN training loss 0.094226\n",
      ">> Epoch 274 finished \tANN training loss 0.098975\n",
      ">> Epoch 275 finished \tANN training loss 0.086384\n",
      ">> Epoch 276 finished \tANN training loss 0.087216\n",
      ">> Epoch 277 finished \tANN training loss 0.081539\n",
      ">> Epoch 278 finished \tANN training loss 0.091530\n",
      ">> Epoch 279 finished \tANN training loss 0.079839\n",
      ">> Epoch 280 finished \tANN training loss 0.095590\n",
      ">> Epoch 281 finished \tANN training loss 0.093725\n",
      ">> Epoch 282 finished \tANN training loss 0.090713\n",
      ">> Epoch 283 finished \tANN training loss 0.085256\n",
      ">> Epoch 284 finished \tANN training loss 0.078461\n",
      ">> Epoch 285 finished \tANN training loss 0.101094\n",
      ">> Epoch 286 finished \tANN training loss 0.080297\n",
      ">> Epoch 287 finished \tANN training loss 0.089766\n",
      ">> Epoch 288 finished \tANN training loss 0.082182\n",
      ">> Epoch 289 finished \tANN training loss 0.081181\n",
      ">> Epoch 290 finished \tANN training loss 0.083049\n",
      ">> Epoch 291 finished \tANN training loss 0.077727\n",
      ">> Epoch 292 finished \tANN training loss 0.080568\n",
      ">> Epoch 293 finished \tANN training loss 0.080394\n",
      ">> Epoch 294 finished \tANN training loss 0.088239\n",
      ">> Epoch 295 finished \tANN training loss 0.079588\n",
      ">> Epoch 296 finished \tANN training loss 0.086821\n",
      ">> Epoch 297 finished \tANN training loss 0.075885\n",
      ">> Epoch 298 finished \tANN training loss 0.093779\n",
      ">> Epoch 299 finished \tANN training loss 0.090123\n",
      ">> Epoch 300 finished \tANN training loss 0.088782\n",
      ">> Epoch 301 finished \tANN training loss 0.087158\n",
      ">> Epoch 302 finished \tANN training loss 0.085688\n",
      ">> Epoch 303 finished \tANN training loss 0.102080\n",
      ">> Epoch 304 finished \tANN training loss 0.086629\n",
      ">> Epoch 305 finished \tANN training loss 0.079288\n",
      ">> Epoch 306 finished \tANN training loss 0.083449\n",
      ">> Epoch 307 finished \tANN training loss 0.086793\n",
      ">> Epoch 308 finished \tANN training loss 0.087467\n",
      ">> Epoch 309 finished \tANN training loss 0.075539\n",
      ">> Epoch 310 finished \tANN training loss 0.091345\n",
      ">> Epoch 311 finished \tANN training loss 0.094045\n",
      ">> Epoch 312 finished \tANN training loss 0.078206\n",
      ">> Epoch 313 finished \tANN training loss 0.081548\n",
      ">> Epoch 314 finished \tANN training loss 0.076206\n",
      ">> Epoch 315 finished \tANN training loss 0.093603\n",
      ">> Epoch 316 finished \tANN training loss 0.092806\n",
      ">> Epoch 317 finished \tANN training loss 0.091058\n",
      ">> Epoch 318 finished \tANN training loss 0.080056\n",
      ">> Epoch 319 finished \tANN training loss 0.093392\n",
      ">> Epoch 320 finished \tANN training loss 0.091205\n",
      ">> Epoch 321 finished \tANN training loss 0.079484\n",
      ">> Epoch 322 finished \tANN training loss 0.086590\n",
      ">> Epoch 323 finished \tANN training loss 0.076641\n",
      ">> Epoch 324 finished \tANN training loss 0.075158\n",
      ">> Epoch 325 finished \tANN training loss 0.076856\n",
      ">> Epoch 326 finished \tANN training loss 0.068596\n",
      ">> Epoch 327 finished \tANN training loss 0.091495\n",
      ">> Epoch 328 finished \tANN training loss 0.082098\n",
      ">> Epoch 329 finished \tANN training loss 0.068789\n",
      ">> Epoch 330 finished \tANN training loss 0.074381\n",
      ">> Epoch 331 finished \tANN training loss 0.073532\n",
      ">> Epoch 332 finished \tANN training loss 0.071337\n",
      ">> Epoch 333 finished \tANN training loss 0.077002\n",
      ">> Epoch 334 finished \tANN training loss 0.080124\n",
      ">> Epoch 335 finished \tANN training loss 0.085902\n",
      ">> Epoch 336 finished \tANN training loss 0.076254\n",
      ">> Epoch 337 finished \tANN training loss 0.083921\n",
      ">> Epoch 338 finished \tANN training loss 0.078358\n",
      ">> Epoch 339 finished \tANN training loss 0.080155\n",
      ">> Epoch 340 finished \tANN training loss 0.089961\n",
      ">> Epoch 341 finished \tANN training loss 0.094102\n",
      ">> Epoch 342 finished \tANN training loss 0.073434\n",
      ">> Epoch 343 finished \tANN training loss 0.074319\n",
      ">> Epoch 344 finished \tANN training loss 0.088667\n",
      ">> Epoch 345 finished \tANN training loss 0.084324\n",
      ">> Epoch 346 finished \tANN training loss 0.083987\n",
      ">> Epoch 347 finished \tANN training loss 0.135944\n",
      ">> Epoch 348 finished \tANN training loss 0.091883\n",
      ">> Epoch 349 finished \tANN training loss 0.081322\n",
      ">> Epoch 350 finished \tANN training loss 0.071741\n",
      ">> Epoch 351 finished \tANN training loss 0.091646\n",
      ">> Epoch 352 finished \tANN training loss 0.075588\n",
      ">> Epoch 353 finished \tANN training loss 0.072341\n",
      ">> Epoch 354 finished \tANN training loss 0.068241\n",
      ">> Epoch 355 finished \tANN training loss 0.069944\n",
      ">> Epoch 356 finished \tANN training loss 0.074506\n",
      ">> Epoch 357 finished \tANN training loss 0.074055\n",
      ">> Epoch 358 finished \tANN training loss 0.078819\n",
      ">> Epoch 359 finished \tANN training loss 0.071837\n",
      ">> Epoch 360 finished \tANN training loss 0.077720\n",
      ">> Epoch 361 finished \tANN training loss 0.078354\n",
      ">> Epoch 362 finished \tANN training loss 0.080280\n",
      ">> Epoch 363 finished \tANN training loss 0.076560\n",
      ">> Epoch 364 finished \tANN training loss 0.078185\n",
      ">> Epoch 365 finished \tANN training loss 0.098678\n",
      ">> Epoch 366 finished \tANN training loss 0.072638\n",
      ">> Epoch 367 finished \tANN training loss 0.077586\n",
      ">> Epoch 368 finished \tANN training loss 0.078856\n",
      ">> Epoch 369 finished \tANN training loss 0.073222\n",
      ">> Epoch 370 finished \tANN training loss 0.076666\n",
      ">> Epoch 371 finished \tANN training loss 0.075370\n",
      ">> Epoch 372 finished \tANN training loss 0.082517\n",
      ">> Epoch 373 finished \tANN training loss 0.068971\n",
      ">> Epoch 374 finished \tANN training loss 0.069781\n",
      ">> Epoch 375 finished \tANN training loss 0.075319\n",
      ">> Epoch 376 finished \tANN training loss 0.077586\n",
      ">> Epoch 377 finished \tANN training loss 0.076437\n",
      ">> Epoch 378 finished \tANN training loss 0.083836\n",
      ">> Epoch 379 finished \tANN training loss 0.074580\n",
      ">> Epoch 380 finished \tANN training loss 0.068810\n",
      ">> Epoch 381 finished \tANN training loss 0.074545\n",
      ">> Epoch 382 finished \tANN training loss 0.066343\n",
      ">> Epoch 383 finished \tANN training loss 0.070097\n",
      ">> Epoch 384 finished \tANN training loss 0.068655\n",
      ">> Epoch 385 finished \tANN training loss 0.067563\n",
      ">> Epoch 386 finished \tANN training loss 0.078495\n",
      ">> Epoch 387 finished \tANN training loss 0.074321\n",
      ">> Epoch 388 finished \tANN training loss 0.084737\n",
      ">> Epoch 389 finished \tANN training loss 0.071505\n",
      ">> Epoch 390 finished \tANN training loss 0.080201\n",
      ">> Epoch 391 finished \tANN training loss 0.072611\n",
      ">> Epoch 392 finished \tANN training loss 0.083022\n",
      ">> Epoch 393 finished \tANN training loss 0.076850\n",
      ">> Epoch 394 finished \tANN training loss 0.086586\n",
      ">> Epoch 395 finished \tANN training loss 0.067649\n",
      ">> Epoch 396 finished \tANN training loss 0.073840\n",
      ">> Epoch 397 finished \tANN training loss 0.087362\n",
      ">> Epoch 398 finished \tANN training loss 0.071460\n",
      ">> Epoch 399 finished \tANN training loss 0.084056\n",
      ">> Epoch 400 finished \tANN training loss 0.077991\n",
      ">> Epoch 401 finished \tANN training loss 0.090089\n",
      ">> Epoch 402 finished \tANN training loss 0.066205\n",
      ">> Epoch 403 finished \tANN training loss 0.069907\n",
      ">> Epoch 404 finished \tANN training loss 0.071398\n",
      ">> Epoch 405 finished \tANN training loss 0.075848\n",
      ">> Epoch 406 finished \tANN training loss 0.074656\n",
      ">> Epoch 407 finished \tANN training loss 0.080247\n",
      ">> Epoch 408 finished \tANN training loss 0.080194\n",
      ">> Epoch 409 finished \tANN training loss 0.067519\n",
      ">> Epoch 410 finished \tANN training loss 0.063894\n",
      ">> Epoch 411 finished \tANN training loss 0.067897\n",
      ">> Epoch 412 finished \tANN training loss 0.100030\n",
      ">> Epoch 413 finished \tANN training loss 0.066902\n",
      ">> Epoch 414 finished \tANN training loss 0.068369\n",
      ">> Epoch 415 finished \tANN training loss 0.065384\n",
      ">> Epoch 416 finished \tANN training loss 0.067414\n",
      ">> Epoch 417 finished \tANN training loss 0.077956\n",
      ">> Epoch 418 finished \tANN training loss 0.067233\n",
      ">> Epoch 419 finished \tANN training loss 0.074692\n",
      ">> Epoch 420 finished \tANN training loss 0.081602\n",
      ">> Epoch 421 finished \tANN training loss 0.072267\n",
      ">> Epoch 422 finished \tANN training loss 0.067984\n",
      ">> Epoch 423 finished \tANN training loss 0.077584\n",
      ">> Epoch 424 finished \tANN training loss 0.079081\n",
      ">> Epoch 425 finished \tANN training loss 0.078249\n",
      ">> Epoch 426 finished \tANN training loss 0.068265\n",
      ">> Epoch 427 finished \tANN training loss 0.071341\n",
      ">> Epoch 428 finished \tANN training loss 0.072368\n",
      ">> Epoch 429 finished \tANN training loss 0.064482\n",
      ">> Epoch 430 finished \tANN training loss 0.065655\n",
      ">> Epoch 431 finished \tANN training loss 0.074966\n",
      ">> Epoch 432 finished \tANN training loss 0.068788\n",
      ">> Epoch 433 finished \tANN training loss 0.072391\n",
      ">> Epoch 434 finished \tANN training loss 0.075311\n",
      ">> Epoch 435 finished \tANN training loss 0.084326\n",
      ">> Epoch 436 finished \tANN training loss 0.070412\n",
      ">> Epoch 437 finished \tANN training loss 0.067047\n",
      ">> Epoch 438 finished \tANN training loss 0.071121\n",
      ">> Epoch 439 finished \tANN training loss 0.069518\n",
      ">> Epoch 440 finished \tANN training loss 0.085904\n",
      ">> Epoch 441 finished \tANN training loss 0.068466\n",
      ">> Epoch 442 finished \tANN training loss 0.075938\n",
      ">> Epoch 443 finished \tANN training loss 0.065661\n",
      ">> Epoch 444 finished \tANN training loss 0.074900\n",
      ">> Epoch 445 finished \tANN training loss 0.070016\n",
      ">> Epoch 446 finished \tANN training loss 0.064236\n",
      ">> Epoch 447 finished \tANN training loss 0.069866\n",
      ">> Epoch 448 finished \tANN training loss 0.068982\n",
      ">> Epoch 449 finished \tANN training loss 0.063649\n",
      ">> Epoch 450 finished \tANN training loss 0.065247\n",
      ">> Epoch 451 finished \tANN training loss 0.064429\n",
      ">> Epoch 452 finished \tANN training loss 0.071702\n",
      ">> Epoch 453 finished \tANN training loss 0.064142\n",
      ">> Epoch 454 finished \tANN training loss 0.068668\n",
      ">> Epoch 455 finished \tANN training loss 0.079907\n",
      ">> Epoch 456 finished \tANN training loss 0.087407\n",
      ">> Epoch 457 finished \tANN training loss 0.084301\n",
      ">> Epoch 458 finished \tANN training loss 0.081285\n",
      ">> Epoch 459 finished \tANN training loss 0.070164\n",
      ">> Epoch 460 finished \tANN training loss 0.072708\n",
      ">> Epoch 461 finished \tANN training loss 0.068575\n",
      ">> Epoch 462 finished \tANN training loss 0.063555\n",
      ">> Epoch 463 finished \tANN training loss 0.074234\n",
      ">> Epoch 464 finished \tANN training loss 0.064724\n",
      ">> Epoch 465 finished \tANN training loss 0.066574\n",
      ">> Epoch 466 finished \tANN training loss 0.081463\n",
      ">> Epoch 467 finished \tANN training loss 0.093308\n",
      ">> Epoch 468 finished \tANN training loss 0.070662\n",
      ">> Epoch 469 finished \tANN training loss 0.071076\n",
      ">> Epoch 470 finished \tANN training loss 0.111014\n",
      ">> Epoch 471 finished \tANN training loss 0.075486\n",
      ">> Epoch 472 finished \tANN training loss 0.069083\n",
      ">> Epoch 473 finished \tANN training loss 0.073392\n",
      ">> Epoch 474 finished \tANN training loss 0.072307\n",
      ">> Epoch 475 finished \tANN training loss 0.071845\n",
      ">> Epoch 476 finished \tANN training loss 0.085636\n",
      ">> Epoch 477 finished \tANN training loss 0.066265\n",
      ">> Epoch 478 finished \tANN training loss 0.073580\n",
      ">> Epoch 479 finished \tANN training loss 0.070160\n",
      ">> Epoch 480 finished \tANN training loss 0.067148\n",
      ">> Epoch 481 finished \tANN training loss 0.072744\n",
      ">> Epoch 482 finished \tANN training loss 0.076730\n",
      ">> Epoch 483 finished \tANN training loss 0.078745\n",
      ">> Epoch 484 finished \tANN training loss 0.072529\n",
      ">> Epoch 485 finished \tANN training loss 0.068070\n",
      ">> Epoch 486 finished \tANN training loss 0.065589\n",
      ">> Epoch 487 finished \tANN training loss 0.066891\n",
      ">> Epoch 488 finished \tANN training loss 0.061254\n",
      ">> Epoch 489 finished \tANN training loss 0.080799\n",
      ">> Epoch 490 finished \tANN training loss 0.073330\n",
      ">> Epoch 491 finished \tANN training loss 0.065752\n",
      ">> Epoch 492 finished \tANN training loss 0.069179\n",
      ">> Epoch 493 finished \tANN training loss 0.074717\n",
      ">> Epoch 494 finished \tANN training loss 0.070707\n",
      ">> Epoch 495 finished \tANN training loss 0.069481\n",
      ">> Epoch 496 finished \tANN training loss 0.070274\n",
      ">> Epoch 497 finished \tANN training loss 0.071476\n",
      ">> Epoch 498 finished \tANN training loss 0.067934\n",
      ">> Epoch 499 finished \tANN training loss 0.064408\n",
      ">> Epoch 500 finished \tANN training loss 0.063918\n",
      ">> Epoch 501 finished \tANN training loss 0.063011\n",
      ">> Epoch 502 finished \tANN training loss 0.062860\n",
      ">> Epoch 503 finished \tANN training loss 0.063199\n",
      ">> Epoch 504 finished \tANN training loss 0.065861\n",
      ">> Epoch 505 finished \tANN training loss 0.062608\n",
      ">> Epoch 506 finished \tANN training loss 0.073334\n",
      ">> Epoch 507 finished \tANN training loss 0.066362\n",
      ">> Epoch 508 finished \tANN training loss 0.067396\n",
      ">> Epoch 509 finished \tANN training loss 0.064527\n",
      ">> Epoch 510 finished \tANN training loss 0.076490\n",
      ">> Epoch 511 finished \tANN training loss 0.064805\n",
      ">> Epoch 512 finished \tANN training loss 0.076498\n",
      ">> Epoch 513 finished \tANN training loss 0.069454\n",
      ">> Epoch 514 finished \tANN training loss 0.065964\n",
      ">> Epoch 515 finished \tANN training loss 0.066233\n",
      ">> Epoch 516 finished \tANN training loss 0.065763\n",
      ">> Epoch 517 finished \tANN training loss 0.060158\n",
      ">> Epoch 518 finished \tANN training loss 0.064322\n",
      ">> Epoch 519 finished \tANN training loss 0.090779\n",
      ">> Epoch 520 finished \tANN training loss 0.070840\n",
      ">> Epoch 521 finished \tANN training loss 0.067699\n",
      ">> Epoch 522 finished \tANN training loss 0.062928\n",
      ">> Epoch 523 finished \tANN training loss 0.076954\n",
      ">> Epoch 524 finished \tANN training loss 0.070462\n",
      ">> Epoch 525 finished \tANN training loss 0.059947\n",
      ">> Epoch 526 finished \tANN training loss 0.061420\n",
      ">> Epoch 527 finished \tANN training loss 0.059651\n",
      ">> Epoch 528 finished \tANN training loss 0.064289\n",
      ">> Epoch 529 finished \tANN training loss 0.087057\n",
      ">> Epoch 530 finished \tANN training loss 0.072410\n",
      ">> Epoch 531 finished \tANN training loss 0.077948\n",
      ">> Epoch 532 finished \tANN training loss 0.072468\n",
      ">> Epoch 533 finished \tANN training loss 0.071542\n",
      ">> Epoch 534 finished \tANN training loss 0.066288\n",
      ">> Epoch 535 finished \tANN training loss 0.069965\n",
      ">> Epoch 536 finished \tANN training loss 0.090163\n",
      ">> Epoch 537 finished \tANN training loss 0.065563\n",
      ">> Epoch 538 finished \tANN training loss 0.073267\n",
      ">> Epoch 539 finished \tANN training loss 0.076610\n",
      ">> Epoch 540 finished \tANN training loss 0.075938\n",
      ">> Epoch 541 finished \tANN training loss 0.079083\n",
      ">> Epoch 542 finished \tANN training loss 0.070863\n",
      ">> Epoch 543 finished \tANN training loss 0.077301\n",
      ">> Epoch 544 finished \tANN training loss 0.086144\n",
      ">> Epoch 545 finished \tANN training loss 0.073355\n",
      ">> Epoch 546 finished \tANN training loss 0.073807\n",
      ">> Epoch 547 finished \tANN training loss 0.075440\n",
      ">> Epoch 548 finished \tANN training loss 0.065154\n",
      ">> Epoch 549 finished \tANN training loss 0.071751\n",
      ">> Epoch 550 finished \tANN training loss 0.071093\n",
      ">> Epoch 551 finished \tANN training loss 0.085265\n",
      ">> Epoch 552 finished \tANN training loss 0.075242\n",
      ">> Epoch 553 finished \tANN training loss 0.078583\n",
      ">> Epoch 554 finished \tANN training loss 0.081928\n",
      ">> Epoch 555 finished \tANN training loss 0.080839\n",
      ">> Epoch 556 finished \tANN training loss 0.066742\n",
      ">> Epoch 557 finished \tANN training loss 0.069791\n",
      ">> Epoch 558 finished \tANN training loss 0.070282\n",
      ">> Epoch 559 finished \tANN training loss 0.070004\n",
      ">> Epoch 560 finished \tANN training loss 0.069951\n",
      ">> Epoch 561 finished \tANN training loss 0.063066\n",
      ">> Epoch 562 finished \tANN training loss 0.074453\n",
      ">> Epoch 563 finished \tANN training loss 0.073349\n",
      ">> Epoch 564 finished \tANN training loss 0.064544\n",
      ">> Epoch 565 finished \tANN training loss 0.069175\n",
      ">> Epoch 566 finished \tANN training loss 0.067041\n",
      ">> Epoch 567 finished \tANN training loss 0.082643\n",
      ">> Epoch 568 finished \tANN training loss 0.069838\n",
      ">> Epoch 569 finished \tANN training loss 0.081931\n",
      ">> Epoch 570 finished \tANN training loss 0.064522\n",
      ">> Epoch 571 finished \tANN training loss 0.064427\n",
      ">> Epoch 572 finished \tANN training loss 0.090728\n",
      ">> Epoch 573 finished \tANN training loss 0.069402\n",
      ">> Epoch 574 finished \tANN training loss 0.061739\n",
      ">> Epoch 575 finished \tANN training loss 0.065132\n",
      ">> Epoch 576 finished \tANN training loss 0.059866\n",
      ">> Epoch 577 finished \tANN training loss 0.060787\n",
      ">> Epoch 578 finished \tANN training loss 0.060328\n",
      ">> Epoch 579 finished \tANN training loss 0.055762\n",
      ">> Epoch 580 finished \tANN training loss 0.063898\n",
      ">> Epoch 581 finished \tANN training loss 0.055744\n",
      ">> Epoch 582 finished \tANN training loss 0.073834\n",
      ">> Epoch 583 finished \tANN training loss 0.058450\n",
      ">> Epoch 584 finished \tANN training loss 0.066916\n",
      ">> Epoch 585 finished \tANN training loss 0.059438\n",
      ">> Epoch 586 finished \tANN training loss 0.060743\n",
      ">> Epoch 587 finished \tANN training loss 0.061561\n",
      ">> Epoch 588 finished \tANN training loss 0.059743\n",
      ">> Epoch 589 finished \tANN training loss 0.057201\n",
      ">> Epoch 590 finished \tANN training loss 0.059481\n",
      ">> Epoch 591 finished \tANN training loss 0.059223\n",
      ">> Epoch 592 finished \tANN training loss 0.059785\n",
      ">> Epoch 593 finished \tANN training loss 0.061503\n",
      ">> Epoch 594 finished \tANN training loss 0.060545\n",
      ">> Epoch 595 finished \tANN training loss 0.061307\n",
      ">> Epoch 596 finished \tANN training loss 0.058159\n",
      ">> Epoch 597 finished \tANN training loss 0.074098\n",
      ">> Epoch 598 finished \tANN training loss 0.058478\n",
      ">> Epoch 599 finished \tANN training loss 0.055565\n",
      ">> Epoch 600 finished \tANN training loss 0.060550\n",
      ">> Epoch 601 finished \tANN training loss 0.062966\n",
      ">> Epoch 602 finished \tANN training loss 0.082632\n",
      ">> Epoch 603 finished \tANN training loss 0.062536\n",
      ">> Epoch 604 finished \tANN training loss 0.059210\n",
      ">> Epoch 605 finished \tANN training loss 0.062340\n",
      ">> Epoch 606 finished \tANN training loss 0.069095\n",
      ">> Epoch 607 finished \tANN training loss 0.068486\n",
      ">> Epoch 608 finished \tANN training loss 0.066093\n",
      ">> Epoch 609 finished \tANN training loss 0.060798\n",
      ">> Epoch 610 finished \tANN training loss 0.064678\n",
      ">> Epoch 611 finished \tANN training loss 0.068654\n",
      ">> Epoch 612 finished \tANN training loss 0.062385\n",
      ">> Epoch 613 finished \tANN training loss 0.062198\n",
      ">> Epoch 614 finished \tANN training loss 0.062730\n",
      ">> Epoch 615 finished \tANN training loss 0.060562\n",
      ">> Epoch 616 finished \tANN training loss 0.060966\n",
      ">> Epoch 617 finished \tANN training loss 0.058684\n",
      ">> Epoch 618 finished \tANN training loss 0.084298\n",
      ">> Epoch 619 finished \tANN training loss 0.064634\n",
      ">> Epoch 620 finished \tANN training loss 0.065235\n",
      ">> Epoch 621 finished \tANN training loss 0.061179\n",
      ">> Epoch 622 finished \tANN training loss 0.057690\n",
      ">> Epoch 623 finished \tANN training loss 0.064392\n",
      ">> Epoch 624 finished \tANN training loss 0.063993\n",
      ">> Epoch 625 finished \tANN training loss 0.068111\n",
      ">> Epoch 626 finished \tANN training loss 0.082344\n",
      ">> Epoch 627 finished \tANN training loss 0.065217\n",
      ">> Epoch 628 finished \tANN training loss 0.066839\n",
      ">> Epoch 629 finished \tANN training loss 0.064947\n",
      ">> Epoch 630 finished \tANN training loss 0.064393\n",
      ">> Epoch 631 finished \tANN training loss 0.068330\n",
      ">> Epoch 632 finished \tANN training loss 0.082390\n",
      ">> Epoch 633 finished \tANN training loss 0.070836\n",
      ">> Epoch 634 finished \tANN training loss 0.057022\n",
      ">> Epoch 635 finished \tANN training loss 0.072157\n",
      ">> Epoch 636 finished \tANN training loss 0.068161\n",
      ">> Epoch 637 finished \tANN training loss 0.074854\n",
      ">> Epoch 638 finished \tANN training loss 0.073107\n",
      ">> Epoch 639 finished \tANN training loss 0.064692\n",
      ">> Epoch 640 finished \tANN training loss 0.078533\n",
      ">> Epoch 641 finished \tANN training loss 0.061322\n",
      ">> Epoch 642 finished \tANN training loss 0.065186\n",
      ">> Epoch 643 finished \tANN training loss 0.064780\n",
      ">> Epoch 644 finished \tANN training loss 0.065090\n",
      ">> Epoch 645 finished \tANN training loss 0.057676\n",
      ">> Epoch 646 finished \tANN training loss 0.056063\n",
      ">> Epoch 647 finished \tANN training loss 0.065525\n",
      ">> Epoch 648 finished \tANN training loss 0.058635\n",
      ">> Epoch 649 finished \tANN training loss 0.063815\n",
      ">> Epoch 650 finished \tANN training loss 0.069553\n",
      ">> Epoch 651 finished \tANN training loss 0.073010\n",
      ">> Epoch 652 finished \tANN training loss 0.063829\n",
      ">> Epoch 653 finished \tANN training loss 0.067682\n",
      ">> Epoch 654 finished \tANN training loss 0.064336\n",
      ">> Epoch 655 finished \tANN training loss 0.061198\n",
      ">> Epoch 656 finished \tANN training loss 0.057530\n",
      ">> Epoch 657 finished \tANN training loss 0.061516\n",
      ">> Epoch 658 finished \tANN training loss 0.067215\n",
      ">> Epoch 659 finished \tANN training loss 0.056831\n",
      ">> Epoch 660 finished \tANN training loss 0.057431\n",
      ">> Epoch 661 finished \tANN training loss 0.058135\n",
      ">> Epoch 662 finished \tANN training loss 0.067025\n",
      ">> Epoch 663 finished \tANN training loss 0.057553\n",
      ">> Epoch 664 finished \tANN training loss 0.078870\n",
      ">> Epoch 665 finished \tANN training loss 0.073121\n",
      ">> Epoch 666 finished \tANN training loss 0.063276\n",
      ">> Epoch 667 finished \tANN training loss 0.064543\n",
      ">> Epoch 668 finished \tANN training loss 0.068777\n",
      ">> Epoch 669 finished \tANN training loss 0.072358\n",
      ">> Epoch 670 finished \tANN training loss 0.094084\n",
      ">> Epoch 671 finished \tANN training loss 0.067800\n",
      ">> Epoch 672 finished \tANN training loss 0.079046\n",
      ">> Epoch 673 finished \tANN training loss 0.067104\n",
      ">> Epoch 674 finished \tANN training loss 0.071517\n",
      ">> Epoch 675 finished \tANN training loss 0.080729\n",
      ">> Epoch 676 finished \tANN training loss 0.061440\n",
      ">> Epoch 677 finished \tANN training loss 0.069282\n",
      ">> Epoch 678 finished \tANN training loss 0.078092\n",
      ">> Epoch 679 finished \tANN training loss 0.070435\n",
      ">> Epoch 680 finished \tANN training loss 0.061090\n",
      ">> Epoch 681 finished \tANN training loss 0.064921\n",
      ">> Epoch 682 finished \tANN training loss 0.067042\n",
      ">> Epoch 683 finished \tANN training loss 0.066809\n",
      ">> Epoch 684 finished \tANN training loss 0.062060\n",
      ">> Epoch 685 finished \tANN training loss 0.063485\n",
      ">> Epoch 686 finished \tANN training loss 0.061872\n",
      ">> Epoch 687 finished \tANN training loss 0.062812\n",
      ">> Epoch 688 finished \tANN training loss 0.062605\n",
      ">> Epoch 689 finished \tANN training loss 0.071886\n",
      ">> Epoch 690 finished \tANN training loss 0.065156\n",
      ">> Epoch 691 finished \tANN training loss 0.064728\n",
      ">> Epoch 692 finished \tANN training loss 0.062759\n",
      ">> Epoch 693 finished \tANN training loss 0.063638\n",
      ">> Epoch 694 finished \tANN training loss 0.064121\n",
      ">> Epoch 695 finished \tANN training loss 0.068713\n",
      ">> Epoch 696 finished \tANN training loss 0.063603\n",
      ">> Epoch 697 finished \tANN training loss 0.062861\n",
      ">> Epoch 698 finished \tANN training loss 0.074064\n",
      ">> Epoch 699 finished \tANN training loss 0.063566\n",
      ">> Epoch 700 finished \tANN training loss 0.064573\n",
      ">> Epoch 701 finished \tANN training loss 0.067076\n",
      ">> Epoch 702 finished \tANN training loss 0.058486\n",
      ">> Epoch 703 finished \tANN training loss 0.063057\n",
      ">> Epoch 704 finished \tANN training loss 0.064547\n",
      ">> Epoch 705 finished \tANN training loss 0.063982\n",
      ">> Epoch 706 finished \tANN training loss 0.065285\n",
      ">> Epoch 707 finished \tANN training loss 0.063842\n",
      ">> Epoch 708 finished \tANN training loss 0.075350\n",
      ">> Epoch 709 finished \tANN training loss 0.066890\n",
      ">> Epoch 710 finished \tANN training loss 0.058859\n",
      ">> Epoch 711 finished \tANN training loss 0.057607\n",
      ">> Epoch 712 finished \tANN training loss 0.062973\n",
      ">> Epoch 713 finished \tANN training loss 0.059923\n",
      ">> Epoch 714 finished \tANN training loss 0.063581\n",
      ">> Epoch 715 finished \tANN training loss 0.072370\n",
      ">> Epoch 716 finished \tANN training loss 0.058987\n",
      ">> Epoch 717 finished \tANN training loss 0.060935\n",
      ">> Epoch 718 finished \tANN training loss 0.074671\n",
      ">> Epoch 719 finished \tANN training loss 0.071558\n",
      ">> Epoch 720 finished \tANN training loss 0.068318\n",
      ">> Epoch 721 finished \tANN training loss 0.066205\n",
      ">> Epoch 722 finished \tANN training loss 0.061754\n",
      ">> Epoch 723 finished \tANN training loss 0.058194\n",
      ">> Epoch 724 finished \tANN training loss 0.064261\n",
      ">> Epoch 725 finished \tANN training loss 0.063963\n",
      ">> Epoch 726 finished \tANN training loss 0.067491\n",
      ">> Epoch 727 finished \tANN training loss 0.063176\n",
      ">> Epoch 728 finished \tANN training loss 0.066307\n",
      ">> Epoch 729 finished \tANN training loss 0.064701\n",
      ">> Epoch 730 finished \tANN training loss 0.073523\n",
      ">> Epoch 731 finished \tANN training loss 0.064683\n",
      ">> Epoch 732 finished \tANN training loss 0.058336\n",
      ">> Epoch 733 finished \tANN training loss 0.065574\n",
      ">> Epoch 734 finished \tANN training loss 0.061735\n",
      ">> Epoch 735 finished \tANN training loss 0.057901\n",
      ">> Epoch 736 finished \tANN training loss 0.060986\n",
      ">> Epoch 737 finished \tANN training loss 0.062259\n",
      ">> Epoch 738 finished \tANN training loss 0.064440\n",
      ">> Epoch 739 finished \tANN training loss 0.077149\n",
      ">> Epoch 740 finished \tANN training loss 0.060798\n",
      ">> Epoch 741 finished \tANN training loss 0.068205\n",
      ">> Epoch 742 finished \tANN training loss 0.062953\n",
      ">> Epoch 743 finished \tANN training loss 0.068458\n",
      ">> Epoch 744 finished \tANN training loss 0.073650\n",
      ">> Epoch 745 finished \tANN training loss 0.064233\n",
      ">> Epoch 746 finished \tANN training loss 0.065113\n",
      ">> Epoch 747 finished \tANN training loss 0.068540\n",
      ">> Epoch 748 finished \tANN training loss 0.067524\n",
      ">> Epoch 749 finished \tANN training loss 0.062317\n",
      ">> Epoch 750 finished \tANN training loss 0.064535\n",
      ">> Epoch 751 finished \tANN training loss 0.061460\n",
      ">> Epoch 752 finished \tANN training loss 0.061701\n",
      ">> Epoch 753 finished \tANN training loss 0.058481\n",
      ">> Epoch 754 finished \tANN training loss 0.062770\n",
      ">> Epoch 755 finished \tANN training loss 0.066567\n",
      ">> Epoch 756 finished \tANN training loss 0.078993\n",
      ">> Epoch 757 finished \tANN training loss 0.069847\n",
      ">> Epoch 758 finished \tANN training loss 0.060532\n",
      ">> Epoch 759 finished \tANN training loss 0.061933\n",
      ">> Epoch 760 finished \tANN training loss 0.067010\n",
      ">> Epoch 761 finished \tANN training loss 0.068836\n",
      ">> Epoch 762 finished \tANN training loss 0.079450\n",
      ">> Epoch 763 finished \tANN training loss 0.071357\n",
      ">> Epoch 764 finished \tANN training loss 0.063588\n",
      ">> Epoch 765 finished \tANN training loss 0.063127\n",
      ">> Epoch 766 finished \tANN training loss 0.061775\n",
      ">> Epoch 767 finished \tANN training loss 0.063785\n",
      ">> Epoch 768 finished \tANN training loss 0.062603\n",
      ">> Epoch 769 finished \tANN training loss 0.057770\n",
      ">> Epoch 770 finished \tANN training loss 0.060031\n",
      ">> Epoch 771 finished \tANN training loss 0.060015\n",
      ">> Epoch 772 finished \tANN training loss 0.060051\n",
      ">> Epoch 773 finished \tANN training loss 0.063469\n",
      ">> Epoch 774 finished \tANN training loss 0.064437\n",
      ">> Epoch 775 finished \tANN training loss 0.058389\n",
      ">> Epoch 776 finished \tANN training loss 0.061588\n",
      ">> Epoch 777 finished \tANN training loss 0.065829\n",
      ">> Epoch 778 finished \tANN training loss 0.062052\n",
      ">> Epoch 779 finished \tANN training loss 0.075041\n",
      ">> Epoch 780 finished \tANN training loss 0.064118\n",
      ">> Epoch 781 finished \tANN training loss 0.059772\n",
      ">> Epoch 782 finished \tANN training loss 0.068951\n",
      ">> Epoch 783 finished \tANN training loss 0.056265\n",
      ">> Epoch 784 finished \tANN training loss 0.055711\n",
      ">> Epoch 785 finished \tANN training loss 0.053303\n",
      ">> Epoch 786 finished \tANN training loss 0.054562\n",
      ">> Epoch 787 finished \tANN training loss 0.051163\n",
      ">> Epoch 788 finished \tANN training loss 0.057330\n",
      ">> Epoch 789 finished \tANN training loss 0.060133\n",
      ">> Epoch 790 finished \tANN training loss 0.058262\n",
      ">> Epoch 791 finished \tANN training loss 0.061138\n",
      ">> Epoch 792 finished \tANN training loss 0.062809\n",
      ">> Epoch 793 finished \tANN training loss 0.063792\n",
      ">> Epoch 794 finished \tANN training loss 0.062204\n",
      ">> Epoch 795 finished \tANN training loss 0.061393\n",
      ">> Epoch 796 finished \tANN training loss 0.064340\n",
      ">> Epoch 797 finished \tANN training loss 0.064171\n",
      ">> Epoch 798 finished \tANN training loss 0.057654\n",
      ">> Epoch 799 finished \tANN training loss 0.063617\n",
      ">> Epoch 800 finished \tANN training loss 0.060003\n",
      ">> Epoch 801 finished \tANN training loss 0.064289\n",
      ">> Epoch 802 finished \tANN training loss 0.058685\n",
      ">> Epoch 803 finished \tANN training loss 0.065167\n",
      ">> Epoch 804 finished \tANN training loss 0.067270\n",
      ">> Epoch 805 finished \tANN training loss 0.096563\n",
      ">> Epoch 806 finished \tANN training loss 0.062665\n",
      ">> Epoch 807 finished \tANN training loss 0.057508\n",
      ">> Epoch 808 finished \tANN training loss 0.065867\n",
      ">> Epoch 809 finished \tANN training loss 0.062585\n",
      ">> Epoch 810 finished \tANN training loss 0.066369\n",
      ">> Epoch 811 finished \tANN training loss 0.064904\n",
      ">> Epoch 812 finished \tANN training loss 0.072862\n",
      ">> Epoch 813 finished \tANN training loss 0.064284\n",
      ">> Epoch 814 finished \tANN training loss 0.063369\n",
      ">> Epoch 815 finished \tANN training loss 0.055445\n",
      ">> Epoch 816 finished \tANN training loss 0.055962\n",
      ">> Epoch 817 finished \tANN training loss 0.055414\n",
      ">> Epoch 818 finished \tANN training loss 0.060621\n",
      ">> Epoch 819 finished \tANN training loss 0.067830\n",
      ">> Epoch 820 finished \tANN training loss 0.056516\n",
      ">> Epoch 821 finished \tANN training loss 0.060190\n",
      ">> Epoch 822 finished \tANN training loss 0.058085\n",
      ">> Epoch 823 finished \tANN training loss 0.060950\n",
      ">> Epoch 824 finished \tANN training loss 0.070018\n",
      ">> Epoch 825 finished \tANN training loss 0.056513\n",
      ">> Epoch 826 finished \tANN training loss 0.054119\n",
      ">> Epoch 827 finished \tANN training loss 0.054998\n",
      ">> Epoch 828 finished \tANN training loss 0.055094\n",
      ">> Epoch 829 finished \tANN training loss 0.053330\n",
      ">> Epoch 830 finished \tANN training loss 0.054360\n",
      ">> Epoch 831 finished \tANN training loss 0.053970\n",
      ">> Epoch 832 finished \tANN training loss 0.058213\n",
      ">> Epoch 833 finished \tANN training loss 0.068260\n",
      ">> Epoch 834 finished \tANN training loss 0.054157\n",
      ">> Epoch 835 finished \tANN training loss 0.064817\n",
      ">> Epoch 836 finished \tANN training loss 0.055420\n",
      ">> Epoch 837 finished \tANN training loss 0.055823\n",
      ">> Epoch 838 finished \tANN training loss 0.053667\n",
      ">> Epoch 839 finished \tANN training loss 0.066898\n",
      ">> Epoch 840 finished \tANN training loss 0.062161\n",
      ">> Epoch 841 finished \tANN training loss 0.070430\n",
      ">> Epoch 842 finished \tANN training loss 0.061525\n",
      ">> Epoch 843 finished \tANN training loss 0.062838\n",
      ">> Epoch 844 finished \tANN training loss 0.059061\n",
      ">> Epoch 845 finished \tANN training loss 0.058171\n",
      ">> Epoch 846 finished \tANN training loss 0.065754\n",
      ">> Epoch 847 finished \tANN training loss 0.060840\n",
      ">> Epoch 848 finished \tANN training loss 0.056389\n",
      ">> Epoch 849 finished \tANN training loss 0.058049\n",
      ">> Epoch 850 finished \tANN training loss 0.056422\n",
      ">> Epoch 851 finished \tANN training loss 0.057957\n",
      ">> Epoch 852 finished \tANN training loss 0.060376\n",
      ">> Epoch 853 finished \tANN training loss 0.059754\n",
      ">> Epoch 854 finished \tANN training loss 0.058082\n",
      ">> Epoch 855 finished \tANN training loss 0.059654\n",
      ">> Epoch 856 finished \tANN training loss 0.058665\n",
      ">> Epoch 857 finished \tANN training loss 0.056326\n",
      ">> Epoch 858 finished \tANN training loss 0.061211\n",
      ">> Epoch 859 finished \tANN training loss 0.061323\n",
      ">> Epoch 860 finished \tANN training loss 0.055896\n",
      ">> Epoch 861 finished \tANN training loss 0.054904\n",
      ">> Epoch 862 finished \tANN training loss 0.061287\n",
      ">> Epoch 863 finished \tANN training loss 0.050011\n",
      ">> Epoch 864 finished \tANN training loss 0.055652\n",
      ">> Epoch 865 finished \tANN training loss 0.052775\n",
      ">> Epoch 866 finished \tANN training loss 0.060066\n",
      ">> Epoch 867 finished \tANN training loss 0.060838\n",
      ">> Epoch 868 finished \tANN training loss 0.060930\n",
      ">> Epoch 869 finished \tANN training loss 0.057063\n",
      ">> Epoch 870 finished \tANN training loss 0.062285\n",
      ">> Epoch 871 finished \tANN training loss 0.066091\n",
      ">> Epoch 872 finished \tANN training loss 0.061150\n",
      ">> Epoch 873 finished \tANN training loss 0.057206\n",
      ">> Epoch 874 finished \tANN training loss 0.063785\n",
      ">> Epoch 875 finished \tANN training loss 0.065880\n",
      ">> Epoch 876 finished \tANN training loss 0.057053\n",
      ">> Epoch 877 finished \tANN training loss 0.059925\n",
      ">> Epoch 878 finished \tANN training loss 0.057137\n",
      ">> Epoch 879 finished \tANN training loss 0.059558\n",
      ">> Epoch 880 finished \tANN training loss 0.062936\n",
      ">> Epoch 881 finished \tANN training loss 0.064213\n",
      ">> Epoch 882 finished \tANN training loss 0.050723\n",
      ">> Epoch 883 finished \tANN training loss 0.059691\n",
      ">> Epoch 884 finished \tANN training loss 0.059949\n",
      ">> Epoch 885 finished \tANN training loss 0.052669\n",
      ">> Epoch 886 finished \tANN training loss 0.049282\n",
      ">> Epoch 887 finished \tANN training loss 0.049958\n",
      ">> Epoch 888 finished \tANN training loss 0.051376\n",
      ">> Epoch 889 finished \tANN training loss 0.053348\n",
      ">> Epoch 890 finished \tANN training loss 0.054727\n",
      ">> Epoch 891 finished \tANN training loss 0.059565\n",
      ">> Epoch 892 finished \tANN training loss 0.063904\n",
      ">> Epoch 893 finished \tANN training loss 0.055931\n",
      ">> Epoch 894 finished \tANN training loss 0.057177\n",
      ">> Epoch 895 finished \tANN training loss 0.068814\n",
      ">> Epoch 896 finished \tANN training loss 0.052705\n",
      ">> Epoch 897 finished \tANN training loss 0.057957\n",
      ">> Epoch 898 finished \tANN training loss 0.058886\n",
      ">> Epoch 899 finished \tANN training loss 0.049220\n",
      ">> Epoch 900 finished \tANN training loss 0.055249\n",
      ">> Epoch 901 finished \tANN training loss 0.060554\n",
      ">> Epoch 902 finished \tANN training loss 0.066488\n",
      ">> Epoch 903 finished \tANN training loss 0.054959\n",
      ">> Epoch 904 finished \tANN training loss 0.059655\n",
      ">> Epoch 905 finished \tANN training loss 0.050869\n",
      ">> Epoch 906 finished \tANN training loss 0.054938\n",
      ">> Epoch 907 finished \tANN training loss 0.057190\n",
      ">> Epoch 908 finished \tANN training loss 0.063203\n",
      ">> Epoch 909 finished \tANN training loss 0.054857\n",
      ">> Epoch 910 finished \tANN training loss 0.063571\n",
      ">> Epoch 911 finished \tANN training loss 0.060233\n",
      ">> Epoch 912 finished \tANN training loss 0.063932\n",
      ">> Epoch 913 finished \tANN training loss 0.051529\n",
      ">> Epoch 914 finished \tANN training loss 0.056715\n",
      ">> Epoch 915 finished \tANN training loss 0.061306\n",
      ">> Epoch 916 finished \tANN training loss 0.054462\n",
      ">> Epoch 917 finished \tANN training loss 0.060102\n",
      ">> Epoch 918 finished \tANN training loss 0.053759\n",
      ">> Epoch 919 finished \tANN training loss 0.057441\n",
      ">> Epoch 920 finished \tANN training loss 0.056161\n",
      ">> Epoch 921 finished \tANN training loss 0.058907\n",
      ">> Epoch 922 finished \tANN training loss 0.063581\n",
      ">> Epoch 923 finished \tANN training loss 0.055513\n",
      ">> Epoch 924 finished \tANN training loss 0.057618\n",
      ">> Epoch 925 finished \tANN training loss 0.059759\n",
      ">> Epoch 926 finished \tANN training loss 0.076403\n",
      ">> Epoch 927 finished \tANN training loss 0.065743\n",
      ">> Epoch 928 finished \tANN training loss 0.095774\n",
      ">> Epoch 929 finished \tANN training loss 0.070831\n",
      ">> Epoch 930 finished \tANN training loss 0.064126\n",
      ">> Epoch 931 finished \tANN training loss 0.057399\n",
      ">> Epoch 932 finished \tANN training loss 0.055203\n",
      ">> Epoch 933 finished \tANN training loss 0.080186\n",
      ">> Epoch 934 finished \tANN training loss 0.063879\n",
      ">> Epoch 935 finished \tANN training loss 0.056618\n",
      ">> Epoch 936 finished \tANN training loss 0.052876\n",
      ">> Epoch 937 finished \tANN training loss 0.052053\n",
      ">> Epoch 938 finished \tANN training loss 0.056003\n",
      ">> Epoch 939 finished \tANN training loss 0.058070\n",
      ">> Epoch 940 finished \tANN training loss 0.060748\n",
      ">> Epoch 941 finished \tANN training loss 0.056198\n",
      ">> Epoch 942 finished \tANN training loss 0.053866\n",
      ">> Epoch 943 finished \tANN training loss 0.051199\n",
      ">> Epoch 944 finished \tANN training loss 0.055613\n",
      ">> Epoch 945 finished \tANN training loss 0.056115\n",
      ">> Epoch 946 finished \tANN training loss 0.051967\n",
      ">> Epoch 947 finished \tANN training loss 0.055668\n",
      ">> Epoch 948 finished \tANN training loss 0.059083\n",
      ">> Epoch 949 finished \tANN training loss 0.059173\n",
      ">> Epoch 950 finished \tANN training loss 0.060154\n",
      ">> Epoch 951 finished \tANN training loss 0.054182\n",
      ">> Epoch 952 finished \tANN training loss 0.052033\n",
      ">> Epoch 953 finished \tANN training loss 0.060295\n",
      ">> Epoch 954 finished \tANN training loss 0.057936\n",
      ">> Epoch 955 finished \tANN training loss 0.051385\n",
      ">> Epoch 956 finished \tANN training loss 0.061730\n",
      ">> Epoch 957 finished \tANN training loss 0.052726\n",
      ">> Epoch 958 finished \tANN training loss 0.057939\n",
      ">> Epoch 959 finished \tANN training loss 0.052264\n",
      ">> Epoch 960 finished \tANN training loss 0.047762\n",
      ">> Epoch 961 finished \tANN training loss 0.048903\n",
      ">> Epoch 962 finished \tANN training loss 0.052560\n",
      ">> Epoch 963 finished \tANN training loss 0.053445\n",
      ">> Epoch 964 finished \tANN training loss 0.054946\n",
      ">> Epoch 965 finished \tANN training loss 0.060906\n",
      ">> Epoch 966 finished \tANN training loss 0.069072\n",
      ">> Epoch 967 finished \tANN training loss 0.074790\n",
      ">> Epoch 968 finished \tANN training loss 0.059221\n",
      ">> Epoch 969 finished \tANN training loss 0.061810\n",
      ">> Epoch 970 finished \tANN training loss 0.058189\n",
      ">> Epoch 971 finished \tANN training loss 0.053910\n",
      ">> Epoch 972 finished \tANN training loss 0.051367\n",
      ">> Epoch 973 finished \tANN training loss 0.053458\n",
      ">> Epoch 974 finished \tANN training loss 0.053143\n",
      ">> Epoch 975 finished \tANN training loss 0.052555\n",
      ">> Epoch 976 finished \tANN training loss 0.053486\n",
      ">> Epoch 977 finished \tANN training loss 0.053690\n",
      ">> Epoch 978 finished \tANN training loss 0.051534\n",
      ">> Epoch 979 finished \tANN training loss 0.061024\n",
      ">> Epoch 980 finished \tANN training loss 0.056279\n",
      ">> Epoch 981 finished \tANN training loss 0.064543\n",
      ">> Epoch 982 finished \tANN training loss 0.055182\n",
      ">> Epoch 983 finished \tANN training loss 0.049204\n",
      ">> Epoch 984 finished \tANN training loss 0.053156\n",
      ">> Epoch 985 finished \tANN training loss 0.053196\n",
      ">> Epoch 986 finished \tANN training loss 0.050161\n",
      ">> Epoch 987 finished \tANN training loss 0.055325\n",
      ">> Epoch 988 finished \tANN training loss 0.054225\n",
      ">> Epoch 989 finished \tANN training loss 0.052008\n",
      ">> Epoch 990 finished \tANN training loss 0.047865\n",
      ">> Epoch 991 finished \tANN training loss 0.053936\n",
      ">> Epoch 992 finished \tANN training loss 0.047804\n",
      ">> Epoch 993 finished \tANN training loss 0.053846\n",
      ">> Epoch 994 finished \tANN training loss 0.047867\n",
      ">> Epoch 995 finished \tANN training loss 0.052209\n",
      ">> Epoch 996 finished \tANN training loss 0.056408\n",
      ">> Epoch 997 finished \tANN training loss 0.054922\n",
      ">> Epoch 998 finished \tANN training loss 0.056885\n",
      ">> Epoch 999 finished \tANN training loss 0.058739\n",
      "[END] Fine tuning step\n"
     ]
    }
   ],
   "source": [
    "oof_train2, oof_test2 = get_oof(dbn, x_train, y_train, x_test)\n",
    "train2 = Predict(oof_train2)\n",
    "test2 = Predict(oof_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    }
   ],
   "source": [
    "oof_train3 = np.zeros((x_train.shape[0], _N_CLASS))  # 1000 * _N_CLASS\n",
    "oof_test3 = np.empty((x_test.shape[0], _N_CLASS))  # 500 * _N_CLASS\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kf.split(x_train,y_train)):\n",
    "    kf_X_train = x_train[train_index]  # 800 * 10 交叉验证划分此时的训练集和验证集\n",
    "    kf_X_train = kf_X_train.reshape(kf_X_train.shape[0],1,kf_X_train.shape[1])\n",
    "    kf_y_train = y_train[train_index]  # 1 * 800\n",
    "    kf_y_train = lb.transform(kf_y_train)\n",
    "    kf_X_test = x_train[test_index]  # 200 * 10 验证集\n",
    "    kf_X_test = kf_X_test.reshape(kf_X_test.shape[0],1,kf_X_test.shape[1])\n",
    "\n",
    "    lstm.fit(kf_X_train, kf_y_train,epochs=1000,verbose=0,batch_size=20)  # 当前模型进行训练\n",
    "\n",
    "    oof_train3[test_index] = lstm.predict(kf_X_test)  # 当前验证集进行概率预测， 200 * _N_CLASS\n",
    "    oof_test3 += lstm.predict(x_test_lstm)  # 对测试集概率预测 oof_test_skf[i, :] ，  500 * _N_CLASS\n",
    "\n",
    "oof_test3 /= _N_FOLDS  # 对每一则交叉验证的结果取平均"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_train = []\n",
    "new_train.append(oof_train1)\n",
    "new_train.append(oof_train2)\n",
    "new_train.append(oof_train3)\n",
    "len(new_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_test = []\n",
    "new_test.append(oof_test1)\n",
    "new_test.append(oof_test2)\n",
    "new_test.append(oof_test3)\n",
    "len(new_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train = np.concatenate(new_train, axis=1)\n",
    "new_test = np.concatenate(new_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n"
     ]
    }
   ],
   "source": [
    "new_test = np.array(new_test)\n",
    "count1 = 0\n",
    "count2 = 0\n",
    "for i in range(162):\n",
    "    for j in range(9):\n",
    "        if new_test[i,j] == np.inf:\n",
    "            count1 += 1\n",
    "            print(i,j)\n",
    "        if new_test[i,j] == np.NaN:\n",
    "            count2 += 1\n",
    "            print(i,j)       \n",
    "print(count1,count2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test1 = np.nan_to_num(new_test.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stacking training ones score: 1.0000\n",
      "stacking testing ones score: 0.9255\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators = 27, criterion=\"gini\",random_state=5)\n",
    "clf.fit(new_train, y_train_labels)\n",
    "RFR_pred_train = clf.predict(new_train)\n",
    "RFR_pred_train_label = Predict(RFR_pred_train)\n",
    "stock_train_acc = accuracy_score(y_train, RFR_pred_train_label) \n",
    "new_test1 = np.nan_to_num(new_test.astype(np.float32))\n",
    "RFR1_pred = clf.predict(new_test1)\n",
    "RFR1_pred_label = Predict(RFR1_pred)\n",
    "stock_test_acc = accuracy_score(y_test, RFR1_pred_label) \n",
    "print(\"stacking training ones score: {:.4f}\".format(stock_train_acc))\n",
    "print(\"stacking testing ones score: {:.4f}\".format(stock_test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:0.9255\n",
      "macro-PRE:0.9121\n",
      "macro-SEN:0.9081\n",
      "macroF1-score:0.9098\n"
     ]
    }
   ],
   "source": [
    "print('ACC:{:.4f}'.format( metrics.accuracy_score(y_test, RFR1_pred_label)))\n",
    " \n",
    "print('macro-PRE:{:.4f}'.format( metrics.precision_score(y_test, RFR1_pred_label,average='macro')) )\n",
    " \n",
    "print('macro-SEN:{:.4f}'.format( metrics.recall_score(y_test, RFR1_pred_label,average='macro')))\n",
    " \n",
    "print('macroF1-score:{:.4f}'.format( metrics.f1_score(y_test, RFR1_pred_label,labels=[0,1,2],average='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from scipy import interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes=3\n",
    "y_test_label = label_binarize(y_test, classes=[0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFR2_pred = clf.predict_proba(new_test1)\n",
    "RFR2_pred_1 = np.array(RFR2_pred)\n",
    "RFR3_pred = RFR2_pred_1[:,:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9916\\1125604615.py:19: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "  mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 计算每一类的ROC\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes): # 遍历三个类别\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_label[:, i], RFR3_pred[i, :])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area（方法二）\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test_label.ravel(), RFR3_pred.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# Compute macro-average ROC curve and ROC area（方法一）\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "lw=2\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.4f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='ROC curve of class {0} (area = {1:0.4f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Some extension of Receiver operating characteristic to multi-class')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lw=2\n",
    "# plt.figure()\n",
    "plt.figure(plt.figure(figsize=(8, 6)))\n",
    "\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='Macro-average (AUC = {0:0.4f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='ROC curve of class {0} (area = {1:0.4f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "\n",
    "plt.yticks(fontproperties='Times New Roman', size=16,weight='bold')#设置大小及加粗\n",
    "plt.xticks(fontproperties='Times New Roman', size=16,weight='bold')\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate',fontproperties='Times New Roman', size=20,weight='bold')\n",
    "plt.ylabel('True Positive Rate',fontproperties='Times New Roman', size=20,weight='bold')\n",
    "\n",
    "font1 = {'family' : 'Times New Roman',\n",
    "'weight' : 'bold',\n",
    "'size'   : 15}\n",
    "\n",
    "plt.legend(loc=\"lower right\",prop=font1)\n",
    "\n",
    "plt.savefig(\"EL_RF_ROC.svg\", dpi=300,format=\"svg\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#0099ff  size=10 face=\"黑体\">方法2：DNN </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(438, 9) (438,) (438, 3) (188, 9) (188,) (188, 3)\n"
     ]
    }
   ],
   "source": [
    "print(new_train.shape,y_train.shape,y_train_labels.shape,new_test1.shape,y_test.shape,y_test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_13 (Dense)            (None, 10)                100       \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 3)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 243\n",
      "Trainable params: 243\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def buildDNN(layer1,layer2,n_class):\n",
    "    init = K.initializers.glorot_uniform(seed=1)\n",
    "    simple_adam = tf.keras.optimizers.Adam()\n",
    "    model = K.models.Sequential()\n",
    "    model.add(K.layers.Dense(units=layer1, input_dim=9, kernel_initializer=init, activation='relu'))\n",
    "    model.add(K.layers.Dense(units=layer2, kernel_initializer=init, activation='relu'))\n",
    "    model.add(K.layers.Dense(units=n_class, kernel_initializer=init, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=simple_adam, metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "dnn = buildDNN(layer1=10,layer2=10,n_class=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training \n",
      "Train on 438 samples\n",
      "Epoch 1/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0162 - acc: 0.9909\n",
      "Epoch 2/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0155 - acc: 0.9932\n",
      "Epoch 3/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0153 - acc: 0.9932\n",
      "Epoch 4/500\n",
      "438/438 [==============================] - 0s 23us/sample - loss: 0.0149 - acc: 0.9932\n",
      "Epoch 5/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0154 - acc: 0.9932\n",
      "Epoch 6/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0158 - acc: 0.9932\n",
      "Epoch 7/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0159 - acc: 0.9932\n",
      "Epoch 8/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0151 - acc: 0.9932\n",
      "Epoch 9/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0159 - acc: 0.9932\n",
      "Epoch 10/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0153 - acc: 0.9954\n",
      "Epoch 11/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0151 - acc: 0.9932\n",
      "Epoch 12/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0150 - acc: 0.9954\n",
      "Epoch 13/500\n",
      "438/438 [==============================] - 0s 23us/sample - loss: 0.0153 - acc: 0.9932\n",
      "Epoch 14/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0151 - acc: 0.9932\n",
      "Epoch 15/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0148 - acc: 0.9932\n",
      "Epoch 16/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0150 - acc: 0.9932\n",
      "Epoch 17/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0154 - acc: 0.9909\n",
      "Epoch 18/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0150 - acc: 0.9909\n",
      "Epoch 19/500\n",
      "438/438 [==============================] - 0s 19us/sample - loss: 0.0151 - acc: 0.9932\n",
      "Epoch 20/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0147 - acc: 0.9932\n",
      "Epoch 21/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0151 - acc: 0.9954\n",
      "Epoch 22/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0148 - acc: 0.9932\n",
      "Epoch 23/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0148 - acc: 0.9932\n",
      "Epoch 24/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0147 - acc: 0.9932\n",
      "Epoch 25/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0146 - acc: 0.9932\n",
      "Epoch 26/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0149 - acc: 0.9954\n",
      "Epoch 27/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0151 - acc: 0.9954\n",
      "Epoch 28/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0148 - acc: 0.9954\n",
      "Epoch 29/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0152 - acc: 0.9954\n",
      "Epoch 30/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0144 - acc: 0.9954\n",
      "Epoch 31/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0147 - acc: 0.9954\n",
      "Epoch 32/500\n",
      "438/438 [==============================] - 0s 23us/sample - loss: 0.0146 - acc: 0.9954\n",
      "Epoch 33/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0148 - acc: 0.9954\n",
      "Epoch 34/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0155 - acc: 0.9954\n",
      "Epoch 35/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0150 - acc: 0.9954\n",
      "Epoch 36/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0147 - acc: 0.9954\n",
      "Epoch 37/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0146 - acc: 0.9954\n",
      "Epoch 38/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0150 - acc: 0.9932\n",
      "Epoch 39/500\n",
      "438/438 [==============================] - 0s 23us/sample - loss: 0.0144 - acc: 0.9954\n",
      "Epoch 40/500\n",
      "438/438 [==============================] - 0s 23us/sample - loss: 0.0145 - acc: 0.9932\n",
      "Epoch 41/500\n",
      "438/438 [==============================] - 0s 23us/sample - loss: 0.0145 - acc: 0.9954\n",
      "Epoch 42/500\n",
      "438/438 [==============================] - 0s 23us/sample - loss: 0.0143 - acc: 0.9954\n",
      "Epoch 43/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0145 - acc: 0.9954\n",
      "Epoch 44/500\n",
      "438/438 [==============================] - 0s 23us/sample - loss: 0.0143 - acc: 0.9954\n",
      "Epoch 45/500\n",
      "438/438 [==============================] - 0s 23us/sample - loss: 0.0143 - acc: 0.9954\n",
      "Epoch 46/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0146 - acc: 0.9954\n",
      "Epoch 47/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0143 - acc: 0.9954\n",
      "Epoch 48/500\n",
      "438/438 [==============================] - 0s 23us/sample - loss: 0.0146 - acc: 0.9954\n",
      "Epoch 49/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0141 - acc: 0.9954\n",
      "Epoch 50/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0142 - acc: 0.9954\n",
      "Epoch 51/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0143 - acc: 0.9954\n",
      "Epoch 52/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0142 - acc: 0.9954\n",
      "Epoch 53/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0140 - acc: 0.9954\n",
      "Epoch 54/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0143 - acc: 0.9954\n",
      "Epoch 55/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0141 - acc: 0.9954\n",
      "Epoch 56/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0148 - acc: 0.9932\n",
      "Epoch 57/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0144 - acc: 0.9954\n",
      "Epoch 58/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0141 - acc: 0.9954\n",
      "Epoch 59/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0140 - acc: 0.9954\n",
      "Epoch 60/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0141 - acc: 0.9954\n",
      "Epoch 61/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0143 - acc: 0.9954\n",
      "Epoch 62/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0144 - acc: 0.9954\n",
      "Epoch 63/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0137 - acc: 0.9954\n",
      "Epoch 64/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0144 - acc: 0.9954\n",
      "Epoch 65/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0142 - acc: 0.9954\n",
      "Epoch 66/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0137 - acc: 0.9954\n",
      "Epoch 67/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0142 - acc: 0.9954\n",
      "Epoch 68/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0141 - acc: 0.9954\n",
      "Epoch 69/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0138 - acc: 0.9954\n",
      "Epoch 70/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0145 - acc: 0.9954\n",
      "Epoch 71/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0147 - acc: 0.9954\n",
      "Epoch 72/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0138 - acc: 0.9954\n",
      "Epoch 73/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0137 - acc: 0.9954\n",
      "Epoch 74/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0139 - acc: 0.9954\n",
      "Epoch 75/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0138 - acc: 0.9954\n",
      "Epoch 76/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0141 - acc: 0.9954\n",
      "Epoch 77/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0137 - acc: 0.9954\n",
      "Epoch 78/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0137 - acc: 0.9954\n",
      "Epoch 79/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0136 - acc: 0.9954\n",
      "Epoch 80/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0136 - acc: 0.9954\n",
      "Epoch 81/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0135 - acc: 0.9954\n",
      "Epoch 82/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0135 - acc: 0.9954\n",
      "Epoch 83/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0138 - acc: 0.9954\n",
      "Epoch 84/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0137 - acc: 0.9954\n",
      "Epoch 85/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0136 - acc: 0.9954\n",
      "Epoch 86/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0139 - acc: 0.9954\n",
      "Epoch 87/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0134 - acc: 0.9954\n",
      "Epoch 88/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0134 - acc: 0.9954\n",
      "Epoch 89/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0133 - acc: 0.9954\n",
      "Epoch 90/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0133 - acc: 0.9954\n",
      "Epoch 91/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0136 - acc: 0.9954\n",
      "Epoch 92/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0136 - acc: 0.9954\n",
      "Epoch 93/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0133 - acc: 0.9954\n",
      "Epoch 94/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0133 - acc: 0.9954\n",
      "Epoch 95/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0133 - acc: 0.9954\n",
      "Epoch 96/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0134 - acc: 0.9954\n",
      "Epoch 97/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0133 - acc: 0.9954\n",
      "Epoch 98/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0135 - acc: 0.9954\n",
      "Epoch 99/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0134 - acc: 0.9954\n",
      "Epoch 100/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0132 - acc: 0.9954\n",
      "Epoch 101/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0133 - acc: 0.9954\n",
      "Epoch 102/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0133 - acc: 0.9954\n",
      "Epoch 103/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0137 - acc: 0.9954\n",
      "Epoch 104/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0133 - acc: 0.9954\n",
      "Epoch 105/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0135 - acc: 0.9954\n",
      "Epoch 106/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0133 - acc: 0.9954\n",
      "Epoch 107/500\n",
      "438/438 [==============================] - 0s 52us/sample - loss: 0.0130 - acc: 0.9954\n",
      "Epoch 108/500\n",
      "438/438 [==============================] - 0s 52us/sample - loss: 0.0130 - acc: 0.9954\n",
      "Epoch 109/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0136 - acc: 0.9954\n",
      "Epoch 110/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0134 - acc: 0.9954\n",
      "Epoch 111/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0135 - acc: 0.9954\n",
      "Epoch 112/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0134 - acc: 0.9954\n",
      "Epoch 113/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0131 - acc: 0.9954\n",
      "Epoch 114/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0138 - acc: 0.9954\n",
      "Epoch 115/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0134 - acc: 0.9954\n",
      "Epoch 116/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0132 - acc: 0.9954\n",
      "Epoch 117/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0131 - acc: 0.9954\n",
      "Epoch 118/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0135 - acc: 0.9954\n",
      "Epoch 119/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0128 - acc: 0.9954\n",
      "Epoch 120/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0128 - acc: 0.9954\n",
      "Epoch 121/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0128 - acc: 0.9954\n",
      "Epoch 122/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0127 - acc: 0.9954\n",
      "Epoch 123/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0127 - acc: 0.9954\n",
      "Epoch 124/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0128 - acc: 0.9954\n",
      "Epoch 125/500\n",
      "438/438 [==============================] - 0s 23us/sample - loss: 0.0128 - acc: 0.9954\n",
      "Epoch 126/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0127 - acc: 0.9954\n",
      "Epoch 127/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0127 - acc: 0.9954\n",
      "Epoch 128/500\n",
      "438/438 [==============================] - 0s 23us/sample - loss: 0.0126 - acc: 0.9954\n",
      "Epoch 129/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0127 - acc: 0.9954\n",
      "Epoch 130/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0131 - acc: 0.9954\n",
      "Epoch 131/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0126 - acc: 0.9954\n",
      "Epoch 132/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0126 - acc: 0.9954\n",
      "Epoch 133/500\n",
      "438/438 [==============================] - 0s 23us/sample - loss: 0.0128 - acc: 0.9954\n",
      "Epoch 134/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0127 - acc: 0.9954\n",
      "Epoch 135/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0128 - acc: 0.9954\n",
      "Epoch 136/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0124 - acc: 0.9954\n",
      "Epoch 137/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0127 - acc: 0.9954\n",
      "Epoch 138/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0124 - acc: 0.9954\n",
      "Epoch 139/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0126 - acc: 0.9954\n",
      "Epoch 140/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0129 - acc: 0.9954\n",
      "Epoch 141/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0128 - acc: 0.9954\n",
      "Epoch 142/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0124 - acc: 0.9954\n",
      "Epoch 143/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0134 - acc: 0.9954\n",
      "Epoch 144/500\n",
      "438/438 [==============================] - 0s 23us/sample - loss: 0.0125 - acc: 0.9954\n",
      "Epoch 145/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0126 - acc: 0.9954\n",
      "Epoch 146/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0125 - acc: 0.9954\n",
      "Epoch 147/500\n",
      "438/438 [==============================] - 0s 23us/sample - loss: 0.0122 - acc: 0.9954\n",
      "Epoch 148/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0124 - acc: 0.9954\n",
      "Epoch 149/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0125 - acc: 0.9954\n",
      "Epoch 150/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0122 - acc: 0.9954\n",
      "Epoch 151/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0122 - acc: 0.9954\n",
      "Epoch 152/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0123 - acc: 0.9954\n",
      "Epoch 153/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0125 - acc: 0.9954\n",
      "Epoch 154/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0121 - acc: 0.9954\n",
      "Epoch 155/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0121 - acc: 0.9954\n",
      "Epoch 156/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0124 - acc: 0.9954\n",
      "Epoch 157/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0124 - acc: 0.9954\n",
      "Epoch 158/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0121 - acc: 0.9954\n",
      "Epoch 159/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0119 - acc: 0.9954\n",
      "Epoch 160/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0122 - acc: 0.9954\n",
      "Epoch 161/500\n",
      "438/438 [==============================] - 0s 23us/sample - loss: 0.0120 - acc: 0.9954\n",
      "Epoch 162/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0120 - acc: 0.9954\n",
      "Epoch 163/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0118 - acc: 0.9954\n",
      "Epoch 164/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0119 - acc: 0.9954\n",
      "Epoch 165/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0120 - acc: 0.9954\n",
      "Epoch 166/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0118 - acc: 0.9954\n",
      "Epoch 167/500\n",
      "438/438 [==============================] - 0s 23us/sample - loss: 0.0119 - acc: 0.9954\n",
      "Epoch 168/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0119 - acc: 0.9954\n",
      "Epoch 169/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0118 - acc: 0.9954\n",
      "Epoch 170/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0119 - acc: 0.9954\n",
      "Epoch 171/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0122 - acc: 0.9954\n",
      "Epoch 172/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0126 - acc: 0.9954\n",
      "Epoch 173/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0119 - acc: 0.9954\n",
      "Epoch 174/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0117 - acc: 0.9954\n",
      "Epoch 175/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0119 - acc: 0.9954\n",
      "Epoch 176/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0117 - acc: 0.9954\n",
      "Epoch 177/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0115 - acc: 0.9954\n",
      "Epoch 178/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0119 - acc: 0.9954\n",
      "Epoch 179/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0117 - acc: 0.9954\n",
      "Epoch 180/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0117 - acc: 0.9954\n",
      "Epoch 181/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0128 - acc: 0.9954\n",
      "Epoch 182/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0118 - acc: 0.9954\n",
      "Epoch 183/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0117 - acc: 0.9954\n",
      "Epoch 184/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0119 - acc: 0.9954\n",
      "Epoch 185/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0117 - acc: 0.9954\n",
      "Epoch 186/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0115 - acc: 0.9954\n",
      "Epoch 187/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0114 - acc: 0.9954\n",
      "Epoch 188/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0115 - acc: 0.9954\n",
      "Epoch 189/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0114 - acc: 0.9954\n",
      "Epoch 190/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0116 - acc: 0.9954\n",
      "Epoch 191/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0114 - acc: 0.9954\n",
      "Epoch 192/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0115 - acc: 0.9954\n",
      "Epoch 193/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0113 - acc: 0.9954\n",
      "Epoch 194/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0114 - acc: 0.9954\n",
      "Epoch 195/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0119 - acc: 0.9954\n",
      "Epoch 196/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0113 - acc: 0.9954\n",
      "Epoch 197/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0115 - acc: 0.9954\n",
      "Epoch 198/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0115 - acc: 0.9954\n",
      "Epoch 199/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0111 - acc: 0.9954\n",
      "Epoch 200/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0111 - acc: 0.9954\n",
      "Epoch 201/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0111 - acc: 0.9954\n",
      "Epoch 202/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0112 - acc: 0.9954\n",
      "Epoch 203/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0113 - acc: 0.9954\n",
      "Epoch 204/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0114 - acc: 0.9954\n",
      "Epoch 205/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0110 - acc: 0.9954\n",
      "Epoch 206/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0111 - acc: 0.9954\n",
      "Epoch 207/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0110 - acc: 0.9954\n",
      "Epoch 208/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0111 - acc: 0.9954\n",
      "Epoch 209/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0112 - acc: 0.9954\n",
      "Epoch 210/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0110 - acc: 0.9954\n",
      "Epoch 211/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0110 - acc: 0.9954\n",
      "Epoch 212/500\n",
      "438/438 [==============================] - 0s 23us/sample - loss: 0.0109 - acc: 0.9954\n",
      "Epoch 213/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0108 - acc: 0.9954\n",
      "Epoch 214/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0108 - acc: 0.9954\n",
      "Epoch 215/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0108 - acc: 0.9954\n",
      "Epoch 216/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0111 - acc: 0.9954\n",
      "Epoch 217/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0116 - acc: 0.9954\n",
      "Epoch 218/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0109 - acc: 0.9954\n",
      "Epoch 219/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0108 - acc: 0.9954\n",
      "Epoch 220/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0108 - acc: 0.9954\n",
      "Epoch 221/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0110 - acc: 0.9954\n",
      "Epoch 222/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0107 - acc: 0.9954\n",
      "Epoch 223/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0110 - acc: 0.9954\n",
      "Epoch 224/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0107 - acc: 0.9954\n",
      "Epoch 225/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0113 - acc: 0.9954\n",
      "Epoch 226/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0111 - acc: 0.9954\n",
      "Epoch 227/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0106 - acc: 0.9954\n",
      "Epoch 228/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0116 - acc: 0.9954\n",
      "Epoch 229/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0105 - acc: 0.9954\n",
      "Epoch 230/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0106 - acc: 0.9954\n",
      "Epoch 231/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0108 - acc: 0.9954\n",
      "Epoch 232/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0110 - acc: 0.9954\n",
      "Epoch 233/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0107 - acc: 0.9954\n",
      "Epoch 234/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0105 - acc: 0.9954\n",
      "Epoch 235/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0106 - acc: 0.9954\n",
      "Epoch 236/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0104 - acc: 0.9954\n",
      "Epoch 237/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0106 - acc: 0.9954\n",
      "Epoch 238/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0104 - acc: 0.9954\n",
      "Epoch 239/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0107 - acc: 0.9954\n",
      "Epoch 240/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0104 - acc: 0.9954\n",
      "Epoch 241/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0102 - acc: 0.9954\n",
      "Epoch 242/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0105 - acc: 0.9954\n",
      "Epoch 243/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0108 - acc: 0.9954\n",
      "Epoch 244/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0107 - acc: 0.9954\n",
      "Epoch 245/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0104 - acc: 0.9954\n",
      "Epoch 246/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0102 - acc: 0.9954\n",
      "Epoch 247/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0104 - acc: 0.9954\n",
      "Epoch 248/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0102 - acc: 0.9954\n",
      "Epoch 249/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0105 - acc: 0.9954\n",
      "Epoch 250/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0105 - acc: 0.9954\n",
      "Epoch 251/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0104 - acc: 0.9954\n",
      "Epoch 252/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0101 - acc: 0.9954\n",
      "Epoch 253/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0100 - acc: 0.9954\n",
      "Epoch 254/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0104 - acc: 0.9954\n",
      "Epoch 255/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0100 - acc: 0.9954\n",
      "Epoch 256/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0100 - acc: 0.9954\n",
      "Epoch 257/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0100 - acc: 0.9954\n",
      "Epoch 258/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0101 - acc: 0.9954\n",
      "Epoch 259/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0101 - acc: 0.9954\n",
      "Epoch 260/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0099 - acc: 0.9954\n",
      "Epoch 261/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0102 - acc: 0.9954\n",
      "Epoch 262/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0105 - acc: 0.9954\n",
      "Epoch 263/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0102 - acc: 0.9954\n",
      "Epoch 264/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0099 - acc: 0.9954\n",
      "Epoch 265/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0103 - acc: 0.9954\n",
      "Epoch 266/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0098 - acc: 0.9954\n",
      "Epoch 267/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0103 - acc: 0.9954\n",
      "Epoch 268/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0098 - acc: 0.9954\n",
      "Epoch 269/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0107 - acc: 0.9954\n",
      "Epoch 270/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0104 - acc: 0.9954\n",
      "Epoch 271/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0098 - acc: 0.9954\n",
      "Epoch 272/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0096 - acc: 0.9954\n",
      "Epoch 273/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0095 - acc: 0.9954\n",
      "Epoch 274/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0096 - acc: 0.9954\n",
      "Epoch 275/500\n",
      "438/438 [==============================] - 0s 52us/sample - loss: 0.0096 - acc: 0.9954\n",
      "Epoch 276/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0097 - acc: 0.9954\n",
      "Epoch 277/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0096 - acc: 0.9954\n",
      "Epoch 278/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0095 - acc: 0.9954\n",
      "Epoch 279/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0096 - acc: 0.9954\n",
      "Epoch 280/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0098 - acc: 0.9954\n",
      "Epoch 281/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0097 - acc: 0.9954\n",
      "Epoch 282/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0097 - acc: 0.9954\n",
      "Epoch 283/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0096 - acc: 0.9954\n",
      "Epoch 284/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0096 - acc: 0.9954\n",
      "Epoch 285/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0095 - acc: 0.9954\n",
      "Epoch 286/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0094 - acc: 0.9954\n",
      "Epoch 287/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0094 - acc: 0.9954\n",
      "Epoch 288/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0094 - acc: 0.9954\n",
      "Epoch 289/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0096 - acc: 0.9954\n",
      "Epoch 290/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0097 - acc: 0.9954\n",
      "Epoch 291/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0093 - acc: 0.9954\n",
      "Epoch 292/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0093 - acc: 0.9954\n",
      "Epoch 293/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0092 - acc: 0.9954\n",
      "Epoch 294/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0091 - acc: 0.9954\n",
      "Epoch 295/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0093 - acc: 0.9954\n",
      "Epoch 296/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0096 - acc: 0.9954\n",
      "Epoch 297/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0098 - acc: 0.9954\n",
      "Epoch 298/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0094 - acc: 0.9954\n",
      "Epoch 299/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0092 - acc: 0.9954\n",
      "Epoch 300/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0091 - acc: 0.9954\n",
      "Epoch 301/500\n",
      "438/438 [==============================] - 0s 23us/sample - loss: 0.0096 - acc: 0.9954\n",
      "Epoch 302/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0093 - acc: 0.9954\n",
      "Epoch 303/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0091 - acc: 0.9954\n",
      "Epoch 304/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0090 - acc: 0.9954\n",
      "Epoch 305/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0096 - acc: 0.9954\n",
      "Epoch 306/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0097 - acc: 0.9954\n",
      "Epoch 307/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0093 - acc: 0.9954\n",
      "Epoch 308/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0091 - acc: 0.9954\n",
      "Epoch 309/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0091 - acc: 0.9954\n",
      "Epoch 310/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0090 - acc: 0.9954\n",
      "Epoch 311/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0095 - acc: 0.9977\n",
      "Epoch 312/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0093 - acc: 0.9954\n",
      "Epoch 313/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0090 - acc: 0.9954\n",
      "Epoch 314/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0087 - acc: 0.9954\n",
      "Epoch 315/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0091 - acc: 0.9954\n",
      "Epoch 316/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0089 - acc: 0.9954\n",
      "Epoch 317/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0092 - acc: 0.9954\n",
      "Epoch 318/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0091 - acc: 0.9954\n",
      "Epoch 319/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0087 - acc: 0.9954\n",
      "Epoch 320/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0096 - acc: 0.9977\n",
      "Epoch 321/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0089 - acc: 0.9977\n",
      "Epoch 322/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0090 - acc: 0.9954\n",
      "Epoch 323/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0089 - acc: 0.9954\n",
      "Epoch 324/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0087 - acc: 0.9954\n",
      "Epoch 325/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0085 - acc: 0.9954\n",
      "Epoch 326/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0086 - acc: 0.9954\n",
      "Epoch 327/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0086 - acc: 0.9954\n",
      "Epoch 328/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0088 - acc: 0.9954\n",
      "Epoch 329/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0084 - acc: 0.9954\n",
      "Epoch 330/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0089 - acc: 0.9954\n",
      "Epoch 331/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0087 - acc: 0.9954\n",
      "Epoch 332/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0090 - acc: 0.9977\n",
      "Epoch 333/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0084 - acc: 0.9954\n",
      "Epoch 334/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0084 - acc: 0.9954\n",
      "Epoch 335/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0084 - acc: 0.9954\n",
      "Epoch 336/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0088 - acc: 0.9954\n",
      "Epoch 337/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0085 - acc: 0.9954\n",
      "Epoch 338/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0086 - acc: 0.9977\n",
      "Epoch 339/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0087 - acc: 0.9954\n",
      "Epoch 340/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0084 - acc: 0.9954\n",
      "Epoch 341/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0083 - acc: 0.9954\n",
      "Epoch 342/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0086 - acc: 0.9977\n",
      "Epoch 343/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0089 - acc: 0.9954\n",
      "Epoch 344/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0082 - acc: 0.9954\n",
      "Epoch 345/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0083 - acc: 0.9954\n",
      "Epoch 346/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0085 - acc: 0.9954\n",
      "Epoch 347/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0081 - acc: 0.9954\n",
      "Epoch 348/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0082 - acc: 0.9954\n",
      "Epoch 349/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0083 - acc: 0.9954\n",
      "Epoch 350/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0083 - acc: 0.9954\n",
      "Epoch 351/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0080 - acc: 0.9954\n",
      "Epoch 352/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0084 - acc: 0.9954\n",
      "Epoch 353/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0085 - acc: 0.9977\n",
      "Epoch 354/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0080 - acc: 0.9977\n",
      "Epoch 355/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0083 - acc: 0.9954\n",
      "Epoch 356/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0082 - acc: 0.9954\n",
      "Epoch 357/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0083 - acc: 0.9954\n",
      "Epoch 358/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0081 - acc: 0.9954\n",
      "Epoch 359/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0081 - acc: 0.9954\n",
      "Epoch 360/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0082 - acc: 0.9954\n",
      "Epoch 361/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0079 - acc: 0.9954\n",
      "Epoch 362/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0079 - acc: 0.9954\n",
      "Epoch 363/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0080 - acc: 0.9954\n",
      "Epoch 364/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0082 - acc: 0.9954\n",
      "Epoch 365/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0078 - acc: 0.9954\n",
      "Epoch 366/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0080 - acc: 0.9954\n",
      "Epoch 367/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0083 - acc: 0.9954\n",
      "Epoch 368/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0078 - acc: 0.9977\n",
      "Epoch 369/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0080 - acc: 0.9954\n",
      "Epoch 370/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0083 - acc: 0.9954\n",
      "Epoch 371/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0080 - acc: 0.9954\n",
      "Epoch 372/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0081 - acc: 0.9954\n",
      "Epoch 373/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0078 - acc: 0.9954\n",
      "Epoch 374/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0078 - acc: 0.9954\n",
      "Epoch 375/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0076 - acc: 0.9954\n",
      "Epoch 376/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0079 - acc: 0.9954\n",
      "Epoch 377/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0077 - acc: 0.9954\n",
      "Epoch 378/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0076 - acc: 0.9954\n",
      "Epoch 379/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0076 - acc: 0.9977\n",
      "Epoch 380/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0080 - acc: 0.9977\n",
      "Epoch 381/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0079 - acc: 0.9977\n",
      "Epoch 382/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0076 - acc: 0.9954\n",
      "Epoch 383/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0078 - acc: 0.9954\n",
      "Epoch 384/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0075 - acc: 0.9954\n",
      "Epoch 385/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0075 - acc: 0.9977\n",
      "Epoch 386/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0073 - acc: 0.9977\n",
      "Epoch 387/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0074 - acc: 0.9977\n",
      "Epoch 388/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0075 - acc: 0.9977\n",
      "Epoch 389/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0078 - acc: 0.9954\n",
      "Epoch 390/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0082 - acc: 0.9954\n",
      "Epoch 391/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0076 - acc: 0.9954\n",
      "Epoch 392/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0075 - acc: 0.9954\n",
      "Epoch 393/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0073 - acc: 0.9954\n",
      "Epoch 394/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0076 - acc: 0.9977\n",
      "Epoch 395/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0086 - acc: 0.9954\n",
      "Epoch 396/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0077 - acc: 0.9954\n",
      "Epoch 397/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0074 - acc: 0.9954\n",
      "Epoch 398/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0078 - acc: 0.9977\n",
      "Epoch 399/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0073 - acc: 0.9977\n",
      "Epoch 400/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0072 - acc: 0.9977\n",
      "Epoch 401/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0072 - acc: 0.9954\n",
      "Epoch 402/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0075 - acc: 0.9954\n",
      "Epoch 403/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0069 - acc: 0.9954\n",
      "Epoch 404/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0071 - acc: 0.9977\n",
      "Epoch 405/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0076 - acc: 0.9977\n",
      "Epoch 406/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0071 - acc: 0.9977\n",
      "Epoch 407/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0074 - acc: 0.9977\n",
      "Epoch 408/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0073 - acc: 0.9954\n",
      "Epoch 409/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0072 - acc: 0.9977\n",
      "Epoch 410/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0071 - acc: 0.9954\n",
      "Epoch 411/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0071 - acc: 0.9977\n",
      "Epoch 412/500\n",
      "438/438 [==============================] - 0s 23us/sample - loss: 0.0070 - acc: 0.9977\n",
      "Epoch 413/500\n",
      "438/438 [==============================] - 0s 23us/sample - loss: 0.0073 - acc: 0.9977\n",
      "Epoch 414/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0071 - acc: 0.9977\n",
      "Epoch 415/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0076 - acc: 0.9977\n",
      "Epoch 416/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0070 - acc: 0.9977\n",
      "Epoch 417/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0070 - acc: 0.9954\n",
      "Epoch 418/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0071 - acc: 0.9977\n",
      "Epoch 419/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0071 - acc: 0.9977\n",
      "Epoch 420/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0069 - acc: 0.9977\n",
      "Epoch 421/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0074 - acc: 0.9954\n",
      "Epoch 422/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0070 - acc: 0.9977\n",
      "Epoch 423/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0073 - acc: 0.9977\n",
      "Epoch 424/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0068 - acc: 0.9977\n",
      "Epoch 425/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0068 - acc: 0.9977\n",
      "Epoch 426/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0068 - acc: 0.9977\n",
      "Epoch 427/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0068 - acc: 0.9977\n",
      "Epoch 428/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0067 - acc: 0.9977\n",
      "Epoch 429/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0070 - acc: 0.9977\n",
      "Epoch 430/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0074 - acc: 0.9954\n",
      "Epoch 431/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0066 - acc: 0.9977\n",
      "Epoch 432/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0069 - acc: 0.9977\n",
      "Epoch 433/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0071 - acc: 0.9954\n",
      "Epoch 434/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0065 - acc: 0.9977\n",
      "Epoch 435/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0068 - acc: 0.9977\n",
      "Epoch 436/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0067 - acc: 0.9977\n",
      "Epoch 437/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0066 - acc: 0.9977\n",
      "Epoch 438/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0067 - acc: 0.9977\n",
      "Epoch 439/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0067 - acc: 0.9954\n",
      "Epoch 440/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0066 - acc: 0.9977\n",
      "Epoch 441/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0066 - acc: 0.9977\n",
      "Epoch 442/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0065 - acc: 0.9977\n",
      "Epoch 443/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0066 - acc: 0.9977\n",
      "Epoch 444/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0063 - acc: 0.9977\n",
      "Epoch 445/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0067 - acc: 0.9954\n",
      "Epoch 446/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0064 - acc: 0.9977\n",
      "Epoch 447/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0065 - acc: 0.9977\n",
      "Epoch 448/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0063 - acc: 0.9977\n",
      "Epoch 449/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0064 - acc: 0.9977\n",
      "Epoch 450/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0066 - acc: 0.9977\n",
      "Epoch 451/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0061 - acc: 0.9977\n",
      "Epoch 452/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0068 - acc: 0.9977\n",
      "Epoch 453/500\n",
      "438/438 [==============================] - 0s 23us/sample - loss: 0.0068 - acc: 0.9977\n",
      "Epoch 454/500\n",
      "438/438 [==============================] - 0s 52us/sample - loss: 0.0066 - acc: 0.9977\n",
      "Epoch 455/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0062 - acc: 0.9977\n",
      "Epoch 456/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0061 - acc: 0.9977\n",
      "Epoch 457/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0062 - acc: 0.9977\n",
      "Epoch 458/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0063 - acc: 0.9977\n",
      "Epoch 459/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0061 - acc: 0.9977\n",
      "Epoch 460/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0061 - acc: 0.9977\n",
      "Epoch 461/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0064 - acc: 0.9954\n",
      "Epoch 462/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0063 - acc: 0.9977\n",
      "Epoch 463/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0060 - acc: 0.9977\n",
      "Epoch 464/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0062 - acc: 0.9977\n",
      "Epoch 465/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0060 - acc: 0.9977\n",
      "Epoch 466/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0064 - acc: 0.9977\n",
      "Epoch 467/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0060 - acc: 0.9977\n",
      "Epoch 468/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0060 - acc: 0.9977\n",
      "Epoch 469/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0062 - acc: 0.9977\n",
      "Epoch 470/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0061 - acc: 0.9977\n",
      "Epoch 471/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0063 - acc: 0.9977\n",
      "Epoch 472/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0060 - acc: 0.9977\n",
      "Epoch 473/500\n",
      "438/438 [==============================] - 0s 23us/sample - loss: 0.0059 - acc: 0.9977\n",
      "Epoch 474/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0058 - acc: 0.9977\n",
      "Epoch 475/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0059 - acc: 0.9977\n",
      "Epoch 476/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0061 - acc: 0.9977\n",
      "Epoch 477/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0058 - acc: 0.9977\n",
      "Epoch 478/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0061 - acc: 0.9977\n",
      "Epoch 479/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0057 - acc: 1.0000\n",
      "Epoch 480/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0056 - acc: 0.9977\n",
      "Epoch 481/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0068 - acc: 0.9977\n",
      "Epoch 482/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0066 - acc: 0.9977\n",
      "Epoch 483/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0057 - acc: 0.9977\n",
      "Epoch 484/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0057 - acc: 0.9977\n",
      "Epoch 485/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0057 - acc: 0.9977\n",
      "Epoch 486/500\n",
      "438/438 [==============================] - 0s 23us/sample - loss: 0.0056 - acc: 0.9977\n",
      "Epoch 487/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0056 - acc: 0.9977\n",
      "Epoch 488/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0058 - acc: 0.9977\n",
      "Epoch 489/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0054 - acc: 0.9977\n",
      "Epoch 490/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0057 - acc: 0.9977\n",
      "Epoch 491/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0055 - acc: 0.9977\n",
      "Epoch 492/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0060 - acc: 0.9977\n",
      "Epoch 493/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0057 - acc: 0.9977\n",
      "Epoch 494/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0060 - acc: 0.9977\n",
      "Epoch 495/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0054 - acc: 0.9977\n",
      "Epoch 496/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0054 - acc: 0.9977\n",
      "Epoch 497/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0055 - acc: 0.9977\n",
      "Epoch 498/500\n",
      "438/438 [==============================] - 0s 16us/sample - loss: 0.0057 - acc: 0.9977\n",
      "Epoch 499/500\n",
      "438/438 [==============================] - 0s 18us/sample - loss: 0.0061 - acc: 0.9977\n",
      "Epoch 500/500\n",
      "438/438 [==============================] - 0s 20us/sample - loss: 0.0063 - acc: 0.9977\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18a8c582c40>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_size = 35\n",
    "max_epochs = 500\n",
    "print(\"Starting training \")\n",
    "dnn.fit(new_train, y_train_labels, batch_size=b_size, epochs=max_epochs, shuffle=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_test_proba = dnn.predict(new_test1)\n",
    "dnn_test_pred =Predict(dnn_test_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dnn training with laryer  test score: 0.9148936170212766\n"
     ]
    }
   ],
   "source": [
    "dnn_test_acc = accuracy_score(y_test, dnn_test_pred)\n",
    "print(\"Dnn training with laryer  test score: {}\".format(dnn_test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC: 0.9148936170212766\n",
      "macro-PRE: 0.8913117576779817\n",
      "macro-SEN: 0.8918109809142426\n",
      "macroF1-score: 0.8915356164931717\n"
     ]
    }
   ],
   "source": [
    "print('ACC:', metrics.accuracy_score(y_test,dnn_test_pred))\n",
    " \n",
    "print('macro-PRE:',metrics.precision_score(y_test,dnn_test_pred,average='macro')) \n",
    " \n",
    "print('macro-SEN:',metrics.recall_score(y_test, dnn_test_pred,average='macro'))\n",
    " \n",
    "print('macroF1-score:',metrics.f1_score(y_test, dnn_test_pred,labels=[0,1,2],average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9916\\1718713388.py:19: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "  mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 计算每一类的ROC\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes): # 遍历三个类别\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_label[:, i], dnn_test_proba[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area（方法二）\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test_label.ravel(), dnn_test_proba.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# Compute macro-average ROC curve and ROC area（方法一）\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "lw=2\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.4f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='ROC curve of class {0} (area = {1:0.4f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Some extension of Receiver operating characteristic to multi-class')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#0099ff  size=10 face=\"黑体\">方法3：DBN </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DBN\n",
    "dbn_cant_neuronas_capas_ocultas =  \"10,10\"\n",
    "RBM_cant_epocas_entrenamiento = 35 \n",
    "Backprop_cant_epocas_entrenamiento = 500 \n",
    "\n",
    "# cantidad de neuronas ocultas \n",
    "hidden_layers = []\n",
    "for val in dbn_cant_neuronas_capas_ocultas.split(','):\n",
    "      hidden_layers.append( int(val))\n",
    "\n",
    "dbn = SupervisedDBNClassification(hidden_layers_structure = hidden_layers,\n",
    "                                                learning_rate_rbm=0.05,\n",
    "                                                learning_rate=0.1,\n",
    "                                                n_epochs_rbm=RBM_cant_epocas_entrenamiento,\n",
    "                                                n_iter_backprop=Backprop_cant_epocas_entrenamiento,\n",
    "                                                batch_size=32,\n",
    "                                                activation_function='relu',\n",
    "                                                dropout_p=0.2,\n",
    "                                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 1.843194\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 1.583123\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 1.370053\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 0.974814\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 0.681603\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 0.552442\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 0.490036\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 0.438852\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 0.383054\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 0.338256\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 0.316691\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 0.292734\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 0.277774\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 0.263041\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 0.248388\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 0.236441\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 0.227755\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 0.216726\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 0.213645\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 0.198078\n",
      ">> Epoch 21 finished \tRBM Reconstruction error 0.189086\n",
      ">> Epoch 22 finished \tRBM Reconstruction error 0.176931\n",
      ">> Epoch 23 finished \tRBM Reconstruction error 0.161742\n",
      ">> Epoch 24 finished \tRBM Reconstruction error 0.149698\n",
      ">> Epoch 25 finished \tRBM Reconstruction error 0.138175\n",
      ">> Epoch 26 finished \tRBM Reconstruction error 0.127808\n",
      ">> Epoch 27 finished \tRBM Reconstruction error 0.122318\n",
      ">> Epoch 28 finished \tRBM Reconstruction error 0.118736\n",
      ">> Epoch 29 finished \tRBM Reconstruction error 0.115855\n",
      ">> Epoch 30 finished \tRBM Reconstruction error 0.112718\n",
      ">> Epoch 31 finished \tRBM Reconstruction error 0.109644\n",
      ">> Epoch 32 finished \tRBM Reconstruction error 0.110482\n",
      ">> Epoch 33 finished \tRBM Reconstruction error 0.102233\n",
      ">> Epoch 34 finished \tRBM Reconstruction error 0.104539\n",
      ">> Epoch 35 finished \tRBM Reconstruction error 0.108499\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 1.906158\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 1.446696\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 1.091216\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 0.738089\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 0.564933\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 0.490387\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 0.444548\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 0.398985\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 0.352681\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 0.313939\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 0.283629\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 0.252827\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 0.229629\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 0.207994\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 0.184541\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 0.167353\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 0.152756\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 0.135781\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 0.115869\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 0.099545\n",
      ">> Epoch 21 finished \tRBM Reconstruction error 0.093396\n",
      ">> Epoch 22 finished \tRBM Reconstruction error 0.082629\n",
      ">> Epoch 23 finished \tRBM Reconstruction error 0.072653\n",
      ">> Epoch 24 finished \tRBM Reconstruction error 0.075510\n",
      ">> Epoch 25 finished \tRBM Reconstruction error 0.068560\n",
      ">> Epoch 26 finished \tRBM Reconstruction error 0.065845\n",
      ">> Epoch 27 finished \tRBM Reconstruction error 0.052265\n",
      ">> Epoch 28 finished \tRBM Reconstruction error 0.059907\n",
      ">> Epoch 29 finished \tRBM Reconstruction error 0.057589\n",
      ">> Epoch 30 finished \tRBM Reconstruction error 0.059485\n",
      ">> Epoch 31 finished \tRBM Reconstruction error 0.050658\n",
      ">> Epoch 32 finished \tRBM Reconstruction error 0.048169\n",
      ">> Epoch 33 finished \tRBM Reconstruction error 0.054297\n",
      ">> Epoch 34 finished \tRBM Reconstruction error 0.050936\n",
      ">> Epoch 35 finished \tRBM Reconstruction error 0.055308\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.480655\n",
      ">> Epoch 1 finished \tANN training loss 0.271142\n",
      ">> Epoch 2 finished \tANN training loss 0.183907\n",
      ">> Epoch 3 finished \tANN training loss 0.137262\n",
      ">> Epoch 4 finished \tANN training loss 0.109742\n",
      ">> Epoch 5 finished \tANN training loss 0.092996\n",
      ">> Epoch 6 finished \tANN training loss 0.084818\n",
      ">> Epoch 7 finished \tANN training loss 0.079367\n",
      ">> Epoch 8 finished \tANN training loss 0.076419\n",
      ">> Epoch 9 finished \tANN training loss 0.072141\n",
      ">> Epoch 10 finished \tANN training loss 0.070782\n",
      ">> Epoch 11 finished \tANN training loss 0.069632\n",
      ">> Epoch 12 finished \tANN training loss 0.069119\n",
      ">> Epoch 13 finished \tANN training loss 0.066871\n",
      ">> Epoch 14 finished \tANN training loss 0.067095\n",
      ">> Epoch 15 finished \tANN training loss 0.067284\n",
      ">> Epoch 16 finished \tANN training loss 0.065504\n",
      ">> Epoch 17 finished \tANN training loss 0.065390\n",
      ">> Epoch 18 finished \tANN training loss 0.065157\n",
      ">> Epoch 19 finished \tANN training loss 0.065663\n",
      ">> Epoch 20 finished \tANN training loss 0.065671\n",
      ">> Epoch 21 finished \tANN training loss 0.065754\n",
      ">> Epoch 22 finished \tANN training loss 0.066906\n",
      ">> Epoch 23 finished \tANN training loss 0.066304\n",
      ">> Epoch 24 finished \tANN training loss 0.065600\n",
      ">> Epoch 25 finished \tANN training loss 0.064324\n",
      ">> Epoch 26 finished \tANN training loss 0.064275\n",
      ">> Epoch 27 finished \tANN training loss 0.064219\n",
      ">> Epoch 28 finished \tANN training loss 0.063984\n",
      ">> Epoch 29 finished \tANN training loss 0.064072\n",
      ">> Epoch 30 finished \tANN training loss 0.063351\n",
      ">> Epoch 31 finished \tANN training loss 0.063441\n",
      ">> Epoch 32 finished \tANN training loss 0.062646\n",
      ">> Epoch 33 finished \tANN training loss 0.062410\n",
      ">> Epoch 34 finished \tANN training loss 0.062541\n",
      ">> Epoch 35 finished \tANN training loss 0.062169\n",
      ">> Epoch 36 finished \tANN training loss 0.063183\n",
      ">> Epoch 37 finished \tANN training loss 0.062206\n",
      ">> Epoch 38 finished \tANN training loss 0.062844\n",
      ">> Epoch 39 finished \tANN training loss 0.063527\n",
      ">> Epoch 40 finished \tANN training loss 0.063587\n",
      ">> Epoch 41 finished \tANN training loss 0.062926\n",
      ">> Epoch 42 finished \tANN training loss 0.062592\n",
      ">> Epoch 43 finished \tANN training loss 0.061702\n",
      ">> Epoch 44 finished \tANN training loss 0.062532\n",
      ">> Epoch 45 finished \tANN training loss 0.064203\n",
      ">> Epoch 46 finished \tANN training loss 0.062897\n",
      ">> Epoch 47 finished \tANN training loss 0.061615\n",
      ">> Epoch 48 finished \tANN training loss 0.061289\n",
      ">> Epoch 49 finished \tANN training loss 0.060651\n",
      ">> Epoch 50 finished \tANN training loss 0.061044\n",
      ">> Epoch 51 finished \tANN training loss 0.061224\n",
      ">> Epoch 52 finished \tANN training loss 0.062273\n",
      ">> Epoch 53 finished \tANN training loss 0.060772\n",
      ">> Epoch 54 finished \tANN training loss 0.060594\n",
      ">> Epoch 55 finished \tANN training loss 0.060184\n",
      ">> Epoch 56 finished \tANN training loss 0.060871\n",
      ">> Epoch 57 finished \tANN training loss 0.060586\n",
      ">> Epoch 58 finished \tANN training loss 0.061761\n",
      ">> Epoch 59 finished \tANN training loss 0.061928\n",
      ">> Epoch 60 finished \tANN training loss 0.062139\n",
      ">> Epoch 61 finished \tANN training loss 0.060646\n",
      ">> Epoch 62 finished \tANN training loss 0.060343\n",
      ">> Epoch 63 finished \tANN training loss 0.059763\n",
      ">> Epoch 64 finished \tANN training loss 0.060087\n",
      ">> Epoch 65 finished \tANN training loss 0.060795\n",
      ">> Epoch 66 finished \tANN training loss 0.062146\n",
      ">> Epoch 67 finished \tANN training loss 0.062127\n",
      ">> Epoch 68 finished \tANN training loss 0.061674\n",
      ">> Epoch 69 finished \tANN training loss 0.060950\n",
      ">> Epoch 70 finished \tANN training loss 0.060753\n",
      ">> Epoch 71 finished \tANN training loss 0.062218\n",
      ">> Epoch 72 finished \tANN training loss 0.061860\n",
      ">> Epoch 73 finished \tANN training loss 0.061481\n",
      ">> Epoch 74 finished \tANN training loss 0.061823\n",
      ">> Epoch 75 finished \tANN training loss 0.062036\n",
      ">> Epoch 76 finished \tANN training loss 0.061927\n",
      ">> Epoch 77 finished \tANN training loss 0.062162\n",
      ">> Epoch 78 finished \tANN training loss 0.061251\n",
      ">> Epoch 79 finished \tANN training loss 0.061907\n",
      ">> Epoch 80 finished \tANN training loss 0.061037\n",
      ">> Epoch 81 finished \tANN training loss 0.061558\n",
      ">> Epoch 82 finished \tANN training loss 0.061000\n",
      ">> Epoch 83 finished \tANN training loss 0.061267\n",
      ">> Epoch 84 finished \tANN training loss 0.060852\n",
      ">> Epoch 85 finished \tANN training loss 0.061526\n",
      ">> Epoch 86 finished \tANN training loss 0.061134\n",
      ">> Epoch 87 finished \tANN training loss 0.061409\n",
      ">> Epoch 88 finished \tANN training loss 0.060524\n",
      ">> Epoch 89 finished \tANN training loss 0.059971\n",
      ">> Epoch 90 finished \tANN training loss 0.059509\n",
      ">> Epoch 91 finished \tANN training loss 0.060170\n",
      ">> Epoch 92 finished \tANN training loss 0.061318\n",
      ">> Epoch 93 finished \tANN training loss 0.059876\n",
      ">> Epoch 94 finished \tANN training loss 0.061314\n",
      ">> Epoch 95 finished \tANN training loss 0.060538\n",
      ">> Epoch 96 finished \tANN training loss 0.060196\n",
      ">> Epoch 97 finished \tANN training loss 0.059954\n",
      ">> Epoch 98 finished \tANN training loss 0.059850\n",
      ">> Epoch 99 finished \tANN training loss 0.060402\n",
      ">> Epoch 100 finished \tANN training loss 0.060212\n",
      ">> Epoch 101 finished \tANN training loss 0.060147\n",
      ">> Epoch 102 finished \tANN training loss 0.059393\n",
      ">> Epoch 103 finished \tANN training loss 0.059069\n",
      ">> Epoch 104 finished \tANN training loss 0.059604\n",
      ">> Epoch 105 finished \tANN training loss 0.059869\n",
      ">> Epoch 106 finished \tANN training loss 0.059755\n",
      ">> Epoch 107 finished \tANN training loss 0.060162\n",
      ">> Epoch 108 finished \tANN training loss 0.060184\n",
      ">> Epoch 109 finished \tANN training loss 0.059860\n",
      ">> Epoch 110 finished \tANN training loss 0.059198\n",
      ">> Epoch 111 finished \tANN training loss 0.058936\n",
      ">> Epoch 112 finished \tANN training loss 0.058765\n",
      ">> Epoch 113 finished \tANN training loss 0.059147\n",
      ">> Epoch 114 finished \tANN training loss 0.059748\n",
      ">> Epoch 115 finished \tANN training loss 0.059823\n",
      ">> Epoch 116 finished \tANN training loss 0.060307\n",
      ">> Epoch 117 finished \tANN training loss 0.060477\n",
      ">> Epoch 118 finished \tANN training loss 0.060888\n",
      ">> Epoch 119 finished \tANN training loss 0.060105\n",
      ">> Epoch 120 finished \tANN training loss 0.058571\n",
      ">> Epoch 121 finished \tANN training loss 0.059059\n",
      ">> Epoch 122 finished \tANN training loss 0.059605\n",
      ">> Epoch 123 finished \tANN training loss 0.059121\n",
      ">> Epoch 124 finished \tANN training loss 0.059435\n",
      ">> Epoch 125 finished \tANN training loss 0.058959\n",
      ">> Epoch 126 finished \tANN training loss 0.058945\n",
      ">> Epoch 127 finished \tANN training loss 0.058781\n",
      ">> Epoch 128 finished \tANN training loss 0.058362\n",
      ">> Epoch 129 finished \tANN training loss 0.058646\n",
      ">> Epoch 130 finished \tANN training loss 0.059569\n",
      ">> Epoch 131 finished \tANN training loss 0.059268\n",
      ">> Epoch 132 finished \tANN training loss 0.057969\n",
      ">> Epoch 133 finished \tANN training loss 0.058569\n",
      ">> Epoch 134 finished \tANN training loss 0.059906\n",
      ">> Epoch 135 finished \tANN training loss 0.059823\n",
      ">> Epoch 136 finished \tANN training loss 0.059420\n",
      ">> Epoch 137 finished \tANN training loss 0.059595\n",
      ">> Epoch 138 finished \tANN training loss 0.059871\n",
      ">> Epoch 139 finished \tANN training loss 0.059077\n",
      ">> Epoch 140 finished \tANN training loss 0.057832\n",
      ">> Epoch 141 finished \tANN training loss 0.057825\n",
      ">> Epoch 142 finished \tANN training loss 0.058316\n",
      ">> Epoch 143 finished \tANN training loss 0.059295\n",
      ">> Epoch 144 finished \tANN training loss 0.059610\n",
      ">> Epoch 145 finished \tANN training loss 0.058754\n",
      ">> Epoch 146 finished \tANN training loss 0.058328\n",
      ">> Epoch 147 finished \tANN training loss 0.058729\n",
      ">> Epoch 148 finished \tANN training loss 0.060199\n",
      ">> Epoch 149 finished \tANN training loss 0.058486\n",
      ">> Epoch 150 finished \tANN training loss 0.058605\n",
      ">> Epoch 151 finished \tANN training loss 0.058874\n",
      ">> Epoch 152 finished \tANN training loss 0.059655\n",
      ">> Epoch 153 finished \tANN training loss 0.060063\n",
      ">> Epoch 154 finished \tANN training loss 0.059407\n",
      ">> Epoch 155 finished \tANN training loss 0.058144\n",
      ">> Epoch 156 finished \tANN training loss 0.058506\n",
      ">> Epoch 157 finished \tANN training loss 0.058208\n",
      ">> Epoch 158 finished \tANN training loss 0.057577\n",
      ">> Epoch 159 finished \tANN training loss 0.057736\n",
      ">> Epoch 160 finished \tANN training loss 0.057987\n",
      ">> Epoch 161 finished \tANN training loss 0.058444\n",
      ">> Epoch 162 finished \tANN training loss 0.056879\n",
      ">> Epoch 163 finished \tANN training loss 0.056670\n",
      ">> Epoch 164 finished \tANN training loss 0.057360\n",
      ">> Epoch 165 finished \tANN training loss 0.056480\n",
      ">> Epoch 166 finished \tANN training loss 0.056395\n",
      ">> Epoch 167 finished \tANN training loss 0.057644\n",
      ">> Epoch 168 finished \tANN training loss 0.056779\n",
      ">> Epoch 169 finished \tANN training loss 0.057694\n",
      ">> Epoch 170 finished \tANN training loss 0.057194\n",
      ">> Epoch 171 finished \tANN training loss 0.057270\n",
      ">> Epoch 172 finished \tANN training loss 0.055398\n",
      ">> Epoch 173 finished \tANN training loss 0.055985\n",
      ">> Epoch 174 finished \tANN training loss 0.055917\n",
      ">> Epoch 175 finished \tANN training loss 0.056759\n",
      ">> Epoch 176 finished \tANN training loss 0.055714\n",
      ">> Epoch 177 finished \tANN training loss 0.056361\n",
      ">> Epoch 178 finished \tANN training loss 0.056267\n",
      ">> Epoch 179 finished \tANN training loss 0.056557\n",
      ">> Epoch 180 finished \tANN training loss 0.055940\n",
      ">> Epoch 181 finished \tANN training loss 0.055908\n",
      ">> Epoch 182 finished \tANN training loss 0.056345\n",
      ">> Epoch 183 finished \tANN training loss 0.056137\n",
      ">> Epoch 184 finished \tANN training loss 0.055656\n",
      ">> Epoch 185 finished \tANN training loss 0.056132\n",
      ">> Epoch 186 finished \tANN training loss 0.055970\n",
      ">> Epoch 187 finished \tANN training loss 0.055613\n",
      ">> Epoch 188 finished \tANN training loss 0.055453\n",
      ">> Epoch 189 finished \tANN training loss 0.055258\n",
      ">> Epoch 190 finished \tANN training loss 0.056338\n",
      ">> Epoch 191 finished \tANN training loss 0.056373\n",
      ">> Epoch 192 finished \tANN training loss 0.057059\n",
      ">> Epoch 193 finished \tANN training loss 0.057597\n",
      ">> Epoch 194 finished \tANN training loss 0.057106\n",
      ">> Epoch 195 finished \tANN training loss 0.057139\n",
      ">> Epoch 196 finished \tANN training loss 0.057903\n",
      ">> Epoch 197 finished \tANN training loss 0.057887\n",
      ">> Epoch 198 finished \tANN training loss 0.058123\n",
      ">> Epoch 199 finished \tANN training loss 0.057482\n",
      ">> Epoch 200 finished \tANN training loss 0.058273\n",
      ">> Epoch 201 finished \tANN training loss 0.057864\n",
      ">> Epoch 202 finished \tANN training loss 0.057201\n",
      ">> Epoch 203 finished \tANN training loss 0.057428\n",
      ">> Epoch 204 finished \tANN training loss 0.057732\n",
      ">> Epoch 205 finished \tANN training loss 0.057780\n",
      ">> Epoch 206 finished \tANN training loss 0.057722\n",
      ">> Epoch 207 finished \tANN training loss 0.056711\n",
      ">> Epoch 208 finished \tANN training loss 0.057065\n",
      ">> Epoch 209 finished \tANN training loss 0.056544\n",
      ">> Epoch 210 finished \tANN training loss 0.056465\n",
      ">> Epoch 211 finished \tANN training loss 0.057066\n",
      ">> Epoch 212 finished \tANN training loss 0.057659\n",
      ">> Epoch 213 finished \tANN training loss 0.056878\n",
      ">> Epoch 214 finished \tANN training loss 0.056745\n",
      ">> Epoch 215 finished \tANN training loss 0.056781\n",
      ">> Epoch 216 finished \tANN training loss 0.057980\n",
      ">> Epoch 217 finished \tANN training loss 0.057402\n",
      ">> Epoch 218 finished \tANN training loss 0.058026\n",
      ">> Epoch 219 finished \tANN training loss 0.058326\n",
      ">> Epoch 220 finished \tANN training loss 0.056856\n",
      ">> Epoch 221 finished \tANN training loss 0.057422\n",
      ">> Epoch 222 finished \tANN training loss 0.057133\n",
      ">> Epoch 223 finished \tANN training loss 0.057583\n",
      ">> Epoch 224 finished \tANN training loss 0.056949\n",
      ">> Epoch 225 finished \tANN training loss 0.057005\n",
      ">> Epoch 226 finished \tANN training loss 0.056449\n",
      ">> Epoch 227 finished \tANN training loss 0.056367\n",
      ">> Epoch 228 finished \tANN training loss 0.056573\n",
      ">> Epoch 229 finished \tANN training loss 0.057055\n",
      ">> Epoch 230 finished \tANN training loss 0.055671\n",
      ">> Epoch 231 finished \tANN training loss 0.055322\n",
      ">> Epoch 232 finished \tANN training loss 0.055161\n",
      ">> Epoch 233 finished \tANN training loss 0.055036\n",
      ">> Epoch 234 finished \tANN training loss 0.055364\n",
      ">> Epoch 235 finished \tANN training loss 0.056392\n",
      ">> Epoch 236 finished \tANN training loss 0.054823\n",
      ">> Epoch 237 finished \tANN training loss 0.055169\n",
      ">> Epoch 238 finished \tANN training loss 0.055379\n",
      ">> Epoch 239 finished \tANN training loss 0.055743\n",
      ">> Epoch 240 finished \tANN training loss 0.054938\n",
      ">> Epoch 241 finished \tANN training loss 0.054636\n",
      ">> Epoch 242 finished \tANN training loss 0.055791\n",
      ">> Epoch 243 finished \tANN training loss 0.056348\n",
      ">> Epoch 244 finished \tANN training loss 0.055723\n",
      ">> Epoch 245 finished \tANN training loss 0.056318\n",
      ">> Epoch 246 finished \tANN training loss 0.055397\n",
      ">> Epoch 247 finished \tANN training loss 0.055325\n",
      ">> Epoch 248 finished \tANN training loss 0.055151\n",
      ">> Epoch 249 finished \tANN training loss 0.055046\n",
      ">> Epoch 250 finished \tANN training loss 0.055901\n",
      ">> Epoch 251 finished \tANN training loss 0.055516\n",
      ">> Epoch 252 finished \tANN training loss 0.055326\n",
      ">> Epoch 253 finished \tANN training loss 0.055257\n",
      ">> Epoch 254 finished \tANN training loss 0.055649\n",
      ">> Epoch 255 finished \tANN training loss 0.056169\n",
      ">> Epoch 256 finished \tANN training loss 0.055633\n",
      ">> Epoch 257 finished \tANN training loss 0.054384\n",
      ">> Epoch 258 finished \tANN training loss 0.054352\n",
      ">> Epoch 259 finished \tANN training loss 0.054494\n",
      ">> Epoch 260 finished \tANN training loss 0.053897\n",
      ">> Epoch 261 finished \tANN training loss 0.054477\n",
      ">> Epoch 262 finished \tANN training loss 0.053725\n",
      ">> Epoch 263 finished \tANN training loss 0.053535\n",
      ">> Epoch 264 finished \tANN training loss 0.053320\n",
      ">> Epoch 265 finished \tANN training loss 0.053459\n",
      ">> Epoch 266 finished \tANN training loss 0.053428\n",
      ">> Epoch 267 finished \tANN training loss 0.054045\n",
      ">> Epoch 268 finished \tANN training loss 0.053517\n",
      ">> Epoch 269 finished \tANN training loss 0.053967\n",
      ">> Epoch 270 finished \tANN training loss 0.054159\n",
      ">> Epoch 271 finished \tANN training loss 0.055447\n",
      ">> Epoch 272 finished \tANN training loss 0.056218\n",
      ">> Epoch 273 finished \tANN training loss 0.055918\n",
      ">> Epoch 274 finished \tANN training loss 0.055135\n",
      ">> Epoch 275 finished \tANN training loss 0.055057\n",
      ">> Epoch 276 finished \tANN training loss 0.053978\n",
      ">> Epoch 277 finished \tANN training loss 0.054157\n",
      ">> Epoch 278 finished \tANN training loss 0.054790\n",
      ">> Epoch 279 finished \tANN training loss 0.054459\n",
      ">> Epoch 280 finished \tANN training loss 0.055038\n",
      ">> Epoch 281 finished \tANN training loss 0.054152\n",
      ">> Epoch 282 finished \tANN training loss 0.054048\n",
      ">> Epoch 283 finished \tANN training loss 0.055111\n",
      ">> Epoch 284 finished \tANN training loss 0.054894\n",
      ">> Epoch 285 finished \tANN training loss 0.054617\n",
      ">> Epoch 286 finished \tANN training loss 0.053245\n",
      ">> Epoch 287 finished \tANN training loss 0.052820\n",
      ">> Epoch 288 finished \tANN training loss 0.053376\n",
      ">> Epoch 289 finished \tANN training loss 0.054821\n",
      ">> Epoch 290 finished \tANN training loss 0.054041\n",
      ">> Epoch 291 finished \tANN training loss 0.055558\n",
      ">> Epoch 292 finished \tANN training loss 0.055045\n",
      ">> Epoch 293 finished \tANN training loss 0.054040\n",
      ">> Epoch 294 finished \tANN training loss 0.054822\n",
      ">> Epoch 295 finished \tANN training loss 0.056124\n",
      ">> Epoch 296 finished \tANN training loss 0.053647\n",
      ">> Epoch 297 finished \tANN training loss 0.053586\n",
      ">> Epoch 298 finished \tANN training loss 0.053550\n",
      ">> Epoch 299 finished \tANN training loss 0.053431\n",
      ">> Epoch 300 finished \tANN training loss 0.053504\n",
      ">> Epoch 301 finished \tANN training loss 0.053374\n",
      ">> Epoch 302 finished \tANN training loss 0.053366\n",
      ">> Epoch 303 finished \tANN training loss 0.053326\n",
      ">> Epoch 304 finished \tANN training loss 0.053235\n",
      ">> Epoch 305 finished \tANN training loss 0.053798\n",
      ">> Epoch 306 finished \tANN training loss 0.052996\n",
      ">> Epoch 307 finished \tANN training loss 0.053667\n",
      ">> Epoch 308 finished \tANN training loss 0.053668\n",
      ">> Epoch 309 finished \tANN training loss 0.053416\n",
      ">> Epoch 310 finished \tANN training loss 0.053066\n",
      ">> Epoch 311 finished \tANN training loss 0.052649\n",
      ">> Epoch 312 finished \tANN training loss 0.053054\n",
      ">> Epoch 313 finished \tANN training loss 0.053455\n",
      ">> Epoch 314 finished \tANN training loss 0.054220\n",
      ">> Epoch 315 finished \tANN training loss 0.053516\n",
      ">> Epoch 316 finished \tANN training loss 0.053197\n",
      ">> Epoch 317 finished \tANN training loss 0.053416\n",
      ">> Epoch 318 finished \tANN training loss 0.052525\n",
      ">> Epoch 319 finished \tANN training loss 0.052632\n",
      ">> Epoch 320 finished \tANN training loss 0.051855\n",
      ">> Epoch 321 finished \tANN training loss 0.051363\n",
      ">> Epoch 322 finished \tANN training loss 0.051767\n",
      ">> Epoch 323 finished \tANN training loss 0.052678\n",
      ">> Epoch 324 finished \tANN training loss 0.051877\n",
      ">> Epoch 325 finished \tANN training loss 0.051219\n",
      ">> Epoch 326 finished \tANN training loss 0.051604\n",
      ">> Epoch 327 finished \tANN training loss 0.052334\n",
      ">> Epoch 328 finished \tANN training loss 0.052685\n",
      ">> Epoch 329 finished \tANN training loss 0.052347\n",
      ">> Epoch 330 finished \tANN training loss 0.053082\n",
      ">> Epoch 331 finished \tANN training loss 0.053957\n",
      ">> Epoch 332 finished \tANN training loss 0.053406\n",
      ">> Epoch 333 finished \tANN training loss 0.053899\n",
      ">> Epoch 334 finished \tANN training loss 0.053997\n",
      ">> Epoch 335 finished \tANN training loss 0.052440\n",
      ">> Epoch 336 finished \tANN training loss 0.053239\n",
      ">> Epoch 337 finished \tANN training loss 0.052754\n",
      ">> Epoch 338 finished \tANN training loss 0.053076\n",
      ">> Epoch 339 finished \tANN training loss 0.053761\n",
      ">> Epoch 340 finished \tANN training loss 0.055205\n",
      ">> Epoch 341 finished \tANN training loss 0.054000\n",
      ">> Epoch 342 finished \tANN training loss 0.052977\n",
      ">> Epoch 343 finished \tANN training loss 0.053533\n",
      ">> Epoch 344 finished \tANN training loss 0.053161\n",
      ">> Epoch 345 finished \tANN training loss 0.052426\n",
      ">> Epoch 346 finished \tANN training loss 0.051691\n",
      ">> Epoch 347 finished \tANN training loss 0.051885\n",
      ">> Epoch 348 finished \tANN training loss 0.051725\n",
      ">> Epoch 349 finished \tANN training loss 0.051980\n",
      ">> Epoch 350 finished \tANN training loss 0.051933\n",
      ">> Epoch 351 finished \tANN training loss 0.052305\n",
      ">> Epoch 352 finished \tANN training loss 0.052494\n",
      ">> Epoch 353 finished \tANN training loss 0.052440\n",
      ">> Epoch 354 finished \tANN training loss 0.052767\n",
      ">> Epoch 355 finished \tANN training loss 0.053122\n",
      ">> Epoch 356 finished \tANN training loss 0.052129\n",
      ">> Epoch 357 finished \tANN training loss 0.051849\n",
      ">> Epoch 358 finished \tANN training loss 0.051645\n",
      ">> Epoch 359 finished \tANN training loss 0.051005\n",
      ">> Epoch 360 finished \tANN training loss 0.051334\n",
      ">> Epoch 361 finished \tANN training loss 0.050415\n",
      ">> Epoch 362 finished \tANN training loss 0.050504\n",
      ">> Epoch 363 finished \tANN training loss 0.051183\n",
      ">> Epoch 364 finished \tANN training loss 0.050881\n",
      ">> Epoch 365 finished \tANN training loss 0.051099\n",
      ">> Epoch 366 finished \tANN training loss 0.051733\n",
      ">> Epoch 367 finished \tANN training loss 0.051435\n",
      ">> Epoch 368 finished \tANN training loss 0.051425\n",
      ">> Epoch 369 finished \tANN training loss 0.051762\n",
      ">> Epoch 370 finished \tANN training loss 0.051239\n",
      ">> Epoch 371 finished \tANN training loss 0.052065\n",
      ">> Epoch 372 finished \tANN training loss 0.052438\n",
      ">> Epoch 373 finished \tANN training loss 0.051398\n",
      ">> Epoch 374 finished \tANN training loss 0.052251\n",
      ">> Epoch 375 finished \tANN training loss 0.051991\n",
      ">> Epoch 376 finished \tANN training loss 0.052001\n",
      ">> Epoch 377 finished \tANN training loss 0.051667\n",
      ">> Epoch 378 finished \tANN training loss 0.051977\n",
      ">> Epoch 379 finished \tANN training loss 0.052016\n",
      ">> Epoch 380 finished \tANN training loss 0.052308\n",
      ">> Epoch 381 finished \tANN training loss 0.052396\n",
      ">> Epoch 382 finished \tANN training loss 0.051538\n",
      ">> Epoch 383 finished \tANN training loss 0.051027\n",
      ">> Epoch 384 finished \tANN training loss 0.051428\n",
      ">> Epoch 385 finished \tANN training loss 0.050721\n",
      ">> Epoch 386 finished \tANN training loss 0.051349\n",
      ">> Epoch 387 finished \tANN training loss 0.051578\n",
      ">> Epoch 388 finished \tANN training loss 0.052422\n",
      ">> Epoch 389 finished \tANN training loss 0.051101\n",
      ">> Epoch 390 finished \tANN training loss 0.050744\n",
      ">> Epoch 391 finished \tANN training loss 0.050411\n",
      ">> Epoch 392 finished \tANN training loss 0.051318\n",
      ">> Epoch 393 finished \tANN training loss 0.051658\n",
      ">> Epoch 394 finished \tANN training loss 0.050356\n",
      ">> Epoch 395 finished \tANN training loss 0.049678\n",
      ">> Epoch 396 finished \tANN training loss 0.049169\n",
      ">> Epoch 397 finished \tANN training loss 0.049155\n",
      ">> Epoch 398 finished \tANN training loss 0.049902\n",
      ">> Epoch 399 finished \tANN training loss 0.050834\n",
      ">> Epoch 400 finished \tANN training loss 0.052730\n",
      ">> Epoch 401 finished \tANN training loss 0.050397\n",
      ">> Epoch 402 finished \tANN training loss 0.050576\n",
      ">> Epoch 403 finished \tANN training loss 0.049104\n",
      ">> Epoch 404 finished \tANN training loss 0.049758\n",
      ">> Epoch 405 finished \tANN training loss 0.049676\n",
      ">> Epoch 406 finished \tANN training loss 0.051236\n",
      ">> Epoch 407 finished \tANN training loss 0.050912\n",
      ">> Epoch 408 finished \tANN training loss 0.050111\n",
      ">> Epoch 409 finished \tANN training loss 0.048369\n",
      ">> Epoch 410 finished \tANN training loss 0.048902\n",
      ">> Epoch 411 finished \tANN training loss 0.048767\n",
      ">> Epoch 412 finished \tANN training loss 0.048143\n",
      ">> Epoch 413 finished \tANN training loss 0.048904\n",
      ">> Epoch 414 finished \tANN training loss 0.048963\n",
      ">> Epoch 415 finished \tANN training loss 0.048540\n",
      ">> Epoch 416 finished \tANN training loss 0.048567\n",
      ">> Epoch 417 finished \tANN training loss 0.048728\n",
      ">> Epoch 418 finished \tANN training loss 0.048963\n",
      ">> Epoch 419 finished \tANN training loss 0.049289\n",
      ">> Epoch 420 finished \tANN training loss 0.049583\n",
      ">> Epoch 421 finished \tANN training loss 0.049395\n",
      ">> Epoch 422 finished \tANN training loss 0.049260\n",
      ">> Epoch 423 finished \tANN training loss 0.050071\n",
      ">> Epoch 424 finished \tANN training loss 0.048183\n",
      ">> Epoch 425 finished \tANN training loss 0.048910\n",
      ">> Epoch 426 finished \tANN training loss 0.049366\n",
      ">> Epoch 427 finished \tANN training loss 0.048731\n",
      ">> Epoch 428 finished \tANN training loss 0.048887\n",
      ">> Epoch 429 finished \tANN training loss 0.048849\n",
      ">> Epoch 430 finished \tANN training loss 0.048915\n",
      ">> Epoch 431 finished \tANN training loss 0.050041\n",
      ">> Epoch 432 finished \tANN training loss 0.048443\n",
      ">> Epoch 433 finished \tANN training loss 0.048236\n",
      ">> Epoch 434 finished \tANN training loss 0.049004\n",
      ">> Epoch 435 finished \tANN training loss 0.049350\n",
      ">> Epoch 436 finished \tANN training loss 0.048394\n",
      ">> Epoch 437 finished \tANN training loss 0.048150\n",
      ">> Epoch 438 finished \tANN training loss 0.048850\n",
      ">> Epoch 439 finished \tANN training loss 0.048794\n",
      ">> Epoch 440 finished \tANN training loss 0.048345\n",
      ">> Epoch 441 finished \tANN training loss 0.047885\n",
      ">> Epoch 442 finished \tANN training loss 0.047568\n",
      ">> Epoch 443 finished \tANN training loss 0.048789\n",
      ">> Epoch 444 finished \tANN training loss 0.048000\n",
      ">> Epoch 445 finished \tANN training loss 0.049062\n",
      ">> Epoch 446 finished \tANN training loss 0.048357\n",
      ">> Epoch 447 finished \tANN training loss 0.048056\n",
      ">> Epoch 448 finished \tANN training loss 0.047839\n",
      ">> Epoch 449 finished \tANN training loss 0.047802\n",
      ">> Epoch 450 finished \tANN training loss 0.047603\n",
      ">> Epoch 451 finished \tANN training loss 0.047976\n",
      ">> Epoch 452 finished \tANN training loss 0.048289\n",
      ">> Epoch 453 finished \tANN training loss 0.048422\n",
      ">> Epoch 454 finished \tANN training loss 0.048594\n",
      ">> Epoch 455 finished \tANN training loss 0.048111\n",
      ">> Epoch 456 finished \tANN training loss 0.048392\n",
      ">> Epoch 457 finished \tANN training loss 0.048837\n",
      ">> Epoch 458 finished \tANN training loss 0.048189\n",
      ">> Epoch 459 finished \tANN training loss 0.047598\n",
      ">> Epoch 460 finished \tANN training loss 0.048346\n",
      ">> Epoch 461 finished \tANN training loss 0.046562\n",
      ">> Epoch 462 finished \tANN training loss 0.046500\n",
      ">> Epoch 463 finished \tANN training loss 0.047011\n",
      ">> Epoch 464 finished \tANN training loss 0.047970\n",
      ">> Epoch 465 finished \tANN training loss 0.047613\n",
      ">> Epoch 466 finished \tANN training loss 0.047820\n",
      ">> Epoch 467 finished \tANN training loss 0.048271\n",
      ">> Epoch 468 finished \tANN training loss 0.047323\n",
      ">> Epoch 469 finished \tANN training loss 0.047915\n",
      ">> Epoch 470 finished \tANN training loss 0.046471\n",
      ">> Epoch 471 finished \tANN training loss 0.047106\n",
      ">> Epoch 472 finished \tANN training loss 0.046443\n",
      ">> Epoch 473 finished \tANN training loss 0.045644\n",
      ">> Epoch 474 finished \tANN training loss 0.046418\n",
      ">> Epoch 475 finished \tANN training loss 0.048115\n",
      ">> Epoch 476 finished \tANN training loss 0.048009\n",
      ">> Epoch 477 finished \tANN training loss 0.046314\n",
      ">> Epoch 478 finished \tANN training loss 0.046472\n",
      ">> Epoch 479 finished \tANN training loss 0.046517\n",
      ">> Epoch 480 finished \tANN training loss 0.047016\n",
      ">> Epoch 481 finished \tANN training loss 0.046646\n",
      ">> Epoch 482 finished \tANN training loss 0.046608\n",
      ">> Epoch 483 finished \tANN training loss 0.047411\n",
      ">> Epoch 484 finished \tANN training loss 0.046298\n",
      ">> Epoch 485 finished \tANN training loss 0.045938\n",
      ">> Epoch 486 finished \tANN training loss 0.045952\n",
      ">> Epoch 487 finished \tANN training loss 0.045420\n",
      ">> Epoch 488 finished \tANN training loss 0.045239\n",
      ">> Epoch 489 finished \tANN training loss 0.045624\n",
      ">> Epoch 490 finished \tANN training loss 0.045815\n",
      ">> Epoch 491 finished \tANN training loss 0.045053\n",
      ">> Epoch 492 finished \tANN training loss 0.045261\n",
      ">> Epoch 493 finished \tANN training loss 0.045738\n",
      ">> Epoch 494 finished \tANN training loss 0.045429\n",
      ">> Epoch 495 finished \tANN training loss 0.045003\n",
      ">> Epoch 496 finished \tANN training loss 0.045388\n",
      ">> Epoch 497 finished \tANN training loss 0.044297\n",
      ">> Epoch 498 finished \tANN training loss 0.044395\n",
      ">> Epoch 499 finished \tANN training loss 0.044082\n",
      "[END] Fine tuning step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SupervisedDBNClassification(batch_size=32, dropout_p=0.2,\n",
       "                            idx_to_label_map={0: 0, 1: 1, 2: 2},\n",
       "                            l2_regularization=1.0,\n",
       "                            label_to_idx_map={0: 0, 1: 1, 2: 2},\n",
       "                            learning_rate=0.1, n_iter_backprop=500,\n",
       "                            verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SupervisedDBNClassification</label><div class=\"sk-toggleable__content\"><pre>SupervisedDBNClassification(batch_size=32, dropout_p=0.2,\n",
       "                            idx_to_label_map={0: 0, 1: 1, 2: 2},\n",
       "                            l2_regularization=1.0,\n",
       "                            label_to_idx_map={0: 0, 1: 1, 2: 2},\n",
       "                            learning_rate=0.1, n_iter_backprop=500,\n",
       "                            verbose=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SupervisedDBNClassification(batch_size=32, dropout_p=0.2,\n",
       "                            idx_to_label_map={0: 0, 1: 1, 2: 2},\n",
       "                            l2_regularization=1.0,\n",
       "                            label_to_idx_map={0: 0, 1: 1, 2: 2},\n",
       "                            learning_rate=0.1, n_iter_backprop=500,\n",
       "                            verbose=True)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbn.fit(new_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbn_test_pred =dbn.predict(new_test1)\n",
    "dbn_test_proba =dbn.predict_proba(new_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(np.isnan(dbn_test_pred ).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbn training test score: 0.898936170212766\n"
     ]
    }
   ],
   "source": [
    "dbn_test_acc = accuracy_score(y_test, dbn_test_pred)\n",
    "print(\"dbn training test score: {}\".format(dbn_test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC: 0.898936170212766\n",
      "macro-PRE: 0.872142449484074\n",
      "macro-SEN: 0.8733063547577035\n",
      "macroF1-score: 0.8724969448850685\n"
     ]
    }
   ],
   "source": [
    "print('ACC:', metrics.accuracy_score(y_test,dbn_test_pred)) \n",
    " \n",
    "print('macro-PRE:',metrics.precision_score(y_test,dbn_test_pred,average='macro')) \n",
    " \n",
    "print('macro-SEN:',metrics.recall_score(y_test, dbn_test_pred,average='macro'))\n",
    " \n",
    "print('macroF1-score:',metrics.f1_score(y_test, dbn_test_pred,labels=[0,1,2],average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9916\\1687146651.py:19: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "  mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 计算每一类的ROC\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes): # 遍历三个类别\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_label[:, i], dbn_test_proba[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area（方法二）\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test_label.ravel(), dbn_test_proba.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# Compute macro-average ROC curve and ROC area（方法一）\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "lw=2\n",
    "# plt.figure()\n",
    "# plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "#          label='micro-average ROC curve (area = {0:0.4f})'\n",
    "#                ''.format(roc_auc[\"micro\"]),\n",
    "#          color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.4f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='ROC curve of class {0} (area = {1:0.4f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Some extension of Receiver operating characteristic to multi-class')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#0099ff  size=10 face=\"黑体\">方法3：RNN </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 1, 9)]            0         \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 10)                800       \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 3)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 943\n",
      "Trainable params: 943\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def buildLSTM(timeStep,inputColNum,outStep,learnRate=1e-4):\n",
    "    '''\n",
    "    搭建LSTM网络，激活函数为tanh\n",
    "    timeStep：输入时间步\n",
    "    inputColNum：输入列数\n",
    "    outStep：输出时间步\n",
    "    learnRate：学习率    \n",
    "    '''\n",
    "    #输入层\n",
    "    inputLayer = Input(shape=(timeStep,inputColNum))\n",
    "\n",
    "    #中间层\n",
    "    middle = LSTM(10,activation='tanh')(inputLayer)\n",
    "    middle = Dense(10,activation='tanh')(middle)\n",
    "\n",
    "    #输出层 全连接\n",
    "    outputLayer = Dense(outStep)(middle)\n",
    "    \n",
    "    #建模\n",
    "    model = Model(inputs=inputLayer,outputs=outputLayer)\n",
    "    optimizer = Adam(learning_rate=learnRate)\n",
    "    model.compile(optimizer=optimizer,loss='mse') \n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "#搭建LSTM\n",
    "lstm = buildLSTM(timeStep=1,inputColNum=9,outStep=3,learnRate=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_lstm = new_train.reshape(new_train.shape[0],1,new_train.shape[1])\n",
    "new_test_lstm = new_test1.reshape(new_test1.shape[0],1,new_test1.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 438 samples\n",
      "Epoch 1/500\n",
      "438/438 [==============================] - 1s 2ms/sample - loss: 0.3411\n",
      "Epoch 2/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.3353\n",
      "Epoch 3/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.3296\n",
      "Epoch 4/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.3240\n",
      "Epoch 5/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.3186\n",
      "Epoch 6/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.3133\n",
      "Epoch 7/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.3081\n",
      "Epoch 8/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.3030\n",
      "Epoch 9/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.2980\n",
      "Epoch 10/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.2931\n",
      "Epoch 11/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.2883\n",
      "Epoch 12/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.2835\n",
      "Epoch 13/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.2788\n",
      "Epoch 14/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.2742\n",
      "Epoch 15/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.2696\n",
      "Epoch 16/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.2650\n",
      "Epoch 17/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.2605\n",
      "Epoch 18/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.2561\n",
      "Epoch 19/500\n",
      "438/438 [==============================] - 0s 41us/sample - loss: 0.2517\n",
      "Epoch 20/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.2473\n",
      "Epoch 21/500\n",
      "438/438 [==============================] - 0s 40us/sample - loss: 0.2430\n",
      "Epoch 22/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.2387\n",
      "Epoch 23/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.2344\n",
      "Epoch 24/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.2302\n",
      "Epoch 25/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.2260\n",
      "Epoch 26/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.2218\n",
      "Epoch 27/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.2176\n",
      "Epoch 28/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.2135\n",
      "Epoch 29/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.2094\n",
      "Epoch 30/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.2054\n",
      "Epoch 31/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.2013\n",
      "Epoch 32/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.1973\n",
      "Epoch 33/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.1933\n",
      "Epoch 34/500\n",
      "438/438 [==============================] - 0s 39us/sample - loss: 0.1893\n",
      "Epoch 35/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.1854\n",
      "Epoch 36/500\n",
      "438/438 [==============================] - 0s 39us/sample - loss: 0.1815\n",
      "Epoch 37/500\n",
      "438/438 [==============================] - 0s 39us/sample - loss: 0.1776\n",
      "Epoch 38/500\n",
      "438/438 [==============================] - 0s 39us/sample - loss: 0.1738\n",
      "Epoch 39/500\n",
      "438/438 [==============================] - 0s 39us/sample - loss: 0.1699\n",
      "Epoch 40/500\n",
      "438/438 [==============================] - 0s 39us/sample - loss: 0.1661\n",
      "Epoch 41/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.1624\n",
      "Epoch 42/500\n",
      "438/438 [==============================] - 0s 39us/sample - loss: 0.1587\n",
      "Epoch 43/500\n",
      "438/438 [==============================] - 0s 39us/sample - loss: 0.1550\n",
      "Epoch 44/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.1513\n",
      "Epoch 45/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.1477\n",
      "Epoch 46/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.1441\n",
      "Epoch 47/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.1406\n",
      "Epoch 48/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.1370\n",
      "Epoch 49/500\n",
      "438/438 [==============================] - 0s 39us/sample - loss: 0.1336\n",
      "Epoch 50/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.1302\n",
      "Epoch 51/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.1268\n",
      "Epoch 52/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.1235\n",
      "Epoch 53/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.1202\n",
      "Epoch 54/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.1169\n",
      "Epoch 55/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.1138\n",
      "Epoch 56/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.1106\n",
      "Epoch 57/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.1075\n",
      "Epoch 58/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.1045\n",
      "Epoch 59/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.1015\n",
      "Epoch 60/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0986\n",
      "Epoch 61/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0957\n",
      "Epoch 62/500\n",
      "438/438 [==============================] - 0s 39us/sample - loss: 0.0929\n",
      "Epoch 63/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0901\n",
      "Epoch 64/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0874\n",
      "Epoch 65/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0847\n",
      "Epoch 66/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0821\n",
      "Epoch 67/500\n",
      "438/438 [==============================] - 0s 39us/sample - loss: 0.0795\n",
      "Epoch 68/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0770\n",
      "Epoch 69/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0746\n",
      "Epoch 70/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0722\n",
      "Epoch 71/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0699\n",
      "Epoch 72/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0676\n",
      "Epoch 73/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0654\n",
      "Epoch 74/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0633\n",
      "Epoch 75/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0612\n",
      "Epoch 76/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0592\n",
      "Epoch 77/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0572\n",
      "Epoch 78/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0553\n",
      "Epoch 79/500\n",
      "438/438 [==============================] - 0s 39us/sample - loss: 0.0534\n",
      "Epoch 80/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0516\n",
      "Epoch 81/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0499\n",
      "Epoch 82/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0482\n",
      "Epoch 83/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0466\n",
      "Epoch 84/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0450\n",
      "Epoch 85/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0436\n",
      "Epoch 86/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0421\n",
      "Epoch 87/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0407\n",
      "Epoch 88/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0394\n",
      "Epoch 89/500\n",
      "438/438 [==============================] - 0s 59us/sample - loss: 0.0381\n",
      "Epoch 90/500\n",
      "438/438 [==============================] - 0s 61us/sample - loss: 0.0369\n",
      "Epoch 91/500\n",
      "438/438 [==============================] - 0s 50us/sample - loss: 0.0357\n",
      "Epoch 92/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0346\n",
      "Epoch 93/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0335\n",
      "Epoch 94/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0325\n",
      "Epoch 95/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0315\n",
      "Epoch 96/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0306\n",
      "Epoch 97/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0297\n",
      "Epoch 98/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0289\n",
      "Epoch 99/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0281\n",
      "Epoch 100/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0273\n",
      "Epoch 101/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0266\n",
      "Epoch 102/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0259\n",
      "Epoch 103/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0253\n",
      "Epoch 104/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0247\n",
      "Epoch 105/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0241\n",
      "Epoch 106/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0236\n",
      "Epoch 107/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0230\n",
      "Epoch 108/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0226\n",
      "Epoch 109/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0221\n",
      "Epoch 110/500\n",
      "438/438 [==============================] - 0s 39us/sample - loss: 0.0217\n",
      "Epoch 111/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0213\n",
      "Epoch 112/500\n",
      "438/438 [==============================] - 0s 39us/sample - loss: 0.0209\n",
      "Epoch 113/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0205\n",
      "Epoch 114/500\n",
      "438/438 [==============================] - 0s 39us/sample - loss: 0.0202\n",
      "Epoch 115/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0199\n",
      "Epoch 116/500\n",
      "438/438 [==============================] - 0s 41us/sample - loss: 0.0196\n",
      "Epoch 117/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0193\n",
      "Epoch 118/500\n",
      "438/438 [==============================] - 0s 39us/sample - loss: 0.0190\n",
      "Epoch 119/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0187\n",
      "Epoch 120/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0185\n",
      "Epoch 121/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0183\n",
      "Epoch 122/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0181\n",
      "Epoch 123/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0179\n",
      "Epoch 124/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0177\n",
      "Epoch 125/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0175\n",
      "Epoch 126/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0173\n",
      "Epoch 127/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0171\n",
      "Epoch 128/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0170\n",
      "Epoch 129/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0168\n",
      "Epoch 130/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0167\n",
      "Epoch 131/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0165\n",
      "Epoch 132/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0164\n",
      "Epoch 133/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0163\n",
      "Epoch 134/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0162\n",
      "Epoch 135/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0160\n",
      "Epoch 136/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0159\n",
      "Epoch 137/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0158\n",
      "Epoch 138/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0157\n",
      "Epoch 139/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0156\n",
      "Epoch 140/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0155\n",
      "Epoch 141/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0154\n",
      "Epoch 142/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0153\n",
      "Epoch 143/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0153\n",
      "Epoch 144/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0152\n",
      "Epoch 145/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0151\n",
      "Epoch 146/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0150\n",
      "Epoch 147/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0149\n",
      "Epoch 148/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0149\n",
      "Epoch 149/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0148\n",
      "Epoch 150/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0147\n",
      "Epoch 151/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0147\n",
      "Epoch 152/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0146\n",
      "Epoch 153/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0145\n",
      "Epoch 154/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0145\n",
      "Epoch 155/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0144\n",
      "Epoch 156/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0144\n",
      "Epoch 157/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0143\n",
      "Epoch 158/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0143\n",
      "Epoch 159/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0142\n",
      "Epoch 160/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0142\n",
      "Epoch 161/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0141\n",
      "Epoch 162/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0141\n",
      "Epoch 163/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0140\n",
      "Epoch 164/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0140\n",
      "Epoch 165/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0139\n",
      "Epoch 166/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0139\n",
      "Epoch 167/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0138\n",
      "Epoch 168/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0138\n",
      "Epoch 169/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0137\n",
      "Epoch 170/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0137\n",
      "Epoch 171/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0137\n",
      "Epoch 172/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0136\n",
      "Epoch 173/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0136\n",
      "Epoch 174/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0135\n",
      "Epoch 175/500\n",
      "438/438 [==============================] - 0s 41us/sample - loss: 0.0135\n",
      "Epoch 176/500\n",
      "438/438 [==============================] - 0s 39us/sample - loss: 0.0135\n",
      "Epoch 177/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0134\n",
      "Epoch 178/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0134\n",
      "Epoch 179/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0134\n",
      "Epoch 180/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0133\n",
      "Epoch 181/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0133\n",
      "Epoch 182/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0133\n",
      "Epoch 183/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0132\n",
      "Epoch 184/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0132\n",
      "Epoch 185/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0132\n",
      "Epoch 186/500\n",
      "438/438 [==============================] - 0s 41us/sample - loss: 0.0131\n",
      "Epoch 187/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0131\n",
      "Epoch 188/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0131\n",
      "Epoch 189/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0130\n",
      "Epoch 190/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0130\n",
      "Epoch 191/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0130\n",
      "Epoch 192/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0130\n",
      "Epoch 193/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0129\n",
      "Epoch 194/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0129\n",
      "Epoch 195/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0129\n",
      "Epoch 196/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0129\n",
      "Epoch 197/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0128\n",
      "Epoch 198/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0128\n",
      "Epoch 199/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0128\n",
      "Epoch 200/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0128\n",
      "Epoch 201/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0127\n",
      "Epoch 202/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0127\n",
      "Epoch 203/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0127\n",
      "Epoch 204/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0127\n",
      "Epoch 205/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0126\n",
      "Epoch 206/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0126\n",
      "Epoch 207/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0126\n",
      "Epoch 208/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0126\n",
      "Epoch 209/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0126\n",
      "Epoch 210/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0125\n",
      "Epoch 211/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0125\n",
      "Epoch 212/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0125\n",
      "Epoch 213/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0125\n",
      "Epoch 214/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0125\n",
      "Epoch 215/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0124\n",
      "Epoch 216/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0124\n",
      "Epoch 217/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0124\n",
      "Epoch 218/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0124\n",
      "Epoch 219/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0124\n",
      "Epoch 220/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0124\n",
      "Epoch 221/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0123\n",
      "Epoch 222/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0123\n",
      "Epoch 223/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0123\n",
      "Epoch 224/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0123\n",
      "Epoch 225/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0123\n",
      "Epoch 226/500\n",
      "438/438 [==============================] - 0s 39us/sample - loss: 0.0123\n",
      "Epoch 227/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0122\n",
      "Epoch 228/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0122\n",
      "Epoch 229/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0122\n",
      "Epoch 230/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0122\n",
      "Epoch 231/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0122\n",
      "Epoch 232/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0122\n",
      "Epoch 233/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0122\n",
      "Epoch 234/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0121\n",
      "Epoch 235/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0121\n",
      "Epoch 236/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0121\n",
      "Epoch 237/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0121\n",
      "Epoch 238/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0121\n",
      "Epoch 239/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0121\n",
      "Epoch 240/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0121\n",
      "Epoch 241/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0120\n",
      "Epoch 242/500\n",
      "438/438 [==============================] - 0s 39us/sample - loss: 0.0120\n",
      "Epoch 243/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0120\n",
      "Epoch 244/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0120\n",
      "Epoch 245/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0120\n",
      "Epoch 246/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0120\n",
      "Epoch 247/500\n",
      "438/438 [==============================] - 0s 100us/sample - loss: 0.0120\n",
      "Epoch 248/500\n",
      "438/438 [==============================] - 0s 46us/sample - loss: 0.0120\n",
      "Epoch 249/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0119\n",
      "Epoch 250/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0119\n",
      "Epoch 251/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0119\n",
      "Epoch 252/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0119\n",
      "Epoch 253/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0119\n",
      "Epoch 254/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0119\n",
      "Epoch 255/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0119\n",
      "Epoch 256/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0119\n",
      "Epoch 257/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0119\n",
      "Epoch 258/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0118\n",
      "Epoch 259/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0118\n",
      "Epoch 260/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0118\n",
      "Epoch 261/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0118\n",
      "Epoch 262/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0118\n",
      "Epoch 263/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0118\n",
      "Epoch 264/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0118\n",
      "Epoch 265/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0118\n",
      "Epoch 266/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0118\n",
      "Epoch 267/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0118\n",
      "Epoch 268/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0118\n",
      "Epoch 269/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0117\n",
      "Epoch 270/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0117\n",
      "Epoch 271/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0117\n",
      "Epoch 272/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0117\n",
      "Epoch 273/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0117\n",
      "Epoch 274/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0117\n",
      "Epoch 275/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0117\n",
      "Epoch 276/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0117\n",
      "Epoch 277/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0117\n",
      "Epoch 278/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0117\n",
      "Epoch 279/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0117\n",
      "Epoch 280/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0117\n",
      "Epoch 281/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0116\n",
      "Epoch 282/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0116\n",
      "Epoch 283/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0116\n",
      "Epoch 284/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0116\n",
      "Epoch 285/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0116\n",
      "Epoch 286/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0116\n",
      "Epoch 287/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0116\n",
      "Epoch 288/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0116\n",
      "Epoch 289/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0116\n",
      "Epoch 290/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0116\n",
      "Epoch 291/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0116\n",
      "Epoch 292/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0116\n",
      "Epoch 293/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0116\n",
      "Epoch 294/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0116\n",
      "Epoch 295/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0116\n",
      "Epoch 296/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0115\n",
      "Epoch 297/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0115\n",
      "Epoch 298/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0115\n",
      "Epoch 299/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0115\n",
      "Epoch 300/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0115\n",
      "Epoch 301/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0115\n",
      "Epoch 302/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0115\n",
      "Epoch 303/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0115\n",
      "Epoch 304/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0115\n",
      "Epoch 305/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0115\n",
      "Epoch 306/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0115\n",
      "Epoch 307/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0115\n",
      "Epoch 308/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0115\n",
      "Epoch 309/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0115\n",
      "Epoch 310/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0115\n",
      "Epoch 311/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0115\n",
      "Epoch 312/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0114\n",
      "Epoch 313/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0114\n",
      "Epoch 314/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0114\n",
      "Epoch 315/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0114\n",
      "Epoch 316/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0114\n",
      "Epoch 317/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0114\n",
      "Epoch 318/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0114\n",
      "Epoch 319/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0114\n",
      "Epoch 320/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0114\n",
      "Epoch 321/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0114\n",
      "Epoch 322/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0114\n",
      "Epoch 323/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0114\n",
      "Epoch 324/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0114\n",
      "Epoch 325/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0114\n",
      "Epoch 326/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0114\n",
      "Epoch 327/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0114\n",
      "Epoch 328/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0114\n",
      "Epoch 329/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0114\n",
      "Epoch 330/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0114\n",
      "Epoch 331/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0114\n",
      "Epoch 332/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0114\n",
      "Epoch 333/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0113\n",
      "Epoch 334/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0113\n",
      "Epoch 335/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0113\n",
      "Epoch 336/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0113\n",
      "Epoch 337/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0113\n",
      "Epoch 338/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0113\n",
      "Epoch 339/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0113\n",
      "Epoch 340/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0113\n",
      "Epoch 341/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0113\n",
      "Epoch 342/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0113\n",
      "Epoch 343/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0113\n",
      "Epoch 344/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0113\n",
      "Epoch 345/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0113\n",
      "Epoch 346/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0113\n",
      "Epoch 347/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0113\n",
      "Epoch 348/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0113\n",
      "Epoch 349/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0113\n",
      "Epoch 350/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0113\n",
      "Epoch 351/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0113\n",
      "Epoch 352/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0113\n",
      "Epoch 353/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0113\n",
      "Epoch 354/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0113\n",
      "Epoch 355/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0113\n",
      "Epoch 356/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0113\n",
      "Epoch 357/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0112\n",
      "Epoch 358/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0112\n",
      "Epoch 359/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0112\n",
      "Epoch 360/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0112\n",
      "Epoch 361/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0112\n",
      "Epoch 362/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0112\n",
      "Epoch 363/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0112\n",
      "Epoch 364/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0112\n",
      "Epoch 365/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0112\n",
      "Epoch 366/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0112\n",
      "Epoch 367/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0112\n",
      "Epoch 368/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0112\n",
      "Epoch 369/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0112\n",
      "Epoch 370/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0112\n",
      "Epoch 371/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0112\n",
      "Epoch 372/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0112\n",
      "Epoch 373/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0112\n",
      "Epoch 374/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0112\n",
      "Epoch 375/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0112\n",
      "Epoch 376/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0112\n",
      "Epoch 377/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0112\n",
      "Epoch 378/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0112\n",
      "Epoch 379/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0112\n",
      "Epoch 380/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0112\n",
      "Epoch 381/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0112\n",
      "Epoch 382/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0112\n",
      "Epoch 383/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0112\n",
      "Epoch 384/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0111\n",
      "Epoch 385/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0111\n",
      "Epoch 386/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0111\n",
      "Epoch 387/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0111\n",
      "Epoch 388/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0111\n",
      "Epoch 389/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0111\n",
      "Epoch 390/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0111\n",
      "Epoch 391/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0111\n",
      "Epoch 392/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0111\n",
      "Epoch 393/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0111\n",
      "Epoch 394/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0111\n",
      "Epoch 395/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0111\n",
      "Epoch 396/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0111\n",
      "Epoch 397/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0111\n",
      "Epoch 398/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0111\n",
      "Epoch 399/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0111\n",
      "Epoch 400/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0111\n",
      "Epoch 401/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0111\n",
      "Epoch 402/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0111\n",
      "Epoch 403/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0111\n",
      "Epoch 404/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0111\n",
      "Epoch 405/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0111\n",
      "Epoch 406/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0111\n",
      "Epoch 407/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0111\n",
      "Epoch 408/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0111\n",
      "Epoch 409/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0111\n",
      "Epoch 410/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0111\n",
      "Epoch 411/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0111\n",
      "Epoch 412/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0111\n",
      "Epoch 413/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0111\n",
      "Epoch 414/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0111\n",
      "Epoch 415/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0111\n",
      "Epoch 416/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0111\n",
      "Epoch 417/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0110\n",
      "Epoch 418/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0110\n",
      "Epoch 419/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0110\n",
      "Epoch 420/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0110\n",
      "Epoch 421/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0110\n",
      "Epoch 422/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0110\n",
      "Epoch 423/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0110\n",
      "Epoch 424/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0110\n",
      "Epoch 425/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0110\n",
      "Epoch 426/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0110\n",
      "Epoch 427/500\n",
      "438/438 [==============================] - 0s 91us/sample - loss: 0.0110\n",
      "Epoch 428/500\n",
      "438/438 [==============================] - 0s 41us/sample - loss: 0.0110\n",
      "Epoch 429/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0110\n",
      "Epoch 430/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0110\n",
      "Epoch 431/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0110\n",
      "Epoch 432/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0110\n",
      "Epoch 433/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0110\n",
      "Epoch 434/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0110\n",
      "Epoch 435/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0110\n",
      "Epoch 436/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0110\n",
      "Epoch 437/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0110\n",
      "Epoch 438/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0110\n",
      "Epoch 439/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0110\n",
      "Epoch 440/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0110\n",
      "Epoch 441/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0110\n",
      "Epoch 442/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0110\n",
      "Epoch 443/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0110\n",
      "Epoch 444/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0110\n",
      "Epoch 445/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0110\n",
      "Epoch 446/500\n",
      "438/438 [==============================] - 0s 41us/sample - loss: 0.0110\n",
      "Epoch 447/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0110\n",
      "Epoch 448/500\n",
      "438/438 [==============================] - 0s 34us/sample - loss: 0.0110\n",
      "Epoch 449/500\n",
      "438/438 [==============================] - 0s 36us/sample - loss: 0.0110\n",
      "Epoch 450/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0110\n",
      "Epoch 451/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0110\n",
      "Epoch 452/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0110\n",
      "Epoch 453/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0109\n",
      "Epoch 454/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0109\n",
      "Epoch 455/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0109\n",
      "Epoch 456/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0109\n",
      "Epoch 457/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0109\n",
      "Epoch 458/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0109\n",
      "Epoch 459/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0109\n",
      "Epoch 460/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0109\n",
      "Epoch 461/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0109\n",
      "Epoch 462/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0109\n",
      "Epoch 463/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0109\n",
      "Epoch 464/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0109\n",
      "Epoch 465/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0109\n",
      "Epoch 466/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0109\n",
      "Epoch 467/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0109\n",
      "Epoch 468/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0109\n",
      "Epoch 469/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0109\n",
      "Epoch 470/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0109\n",
      "Epoch 471/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0109\n",
      "Epoch 472/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0109\n",
      "Epoch 473/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0109\n",
      "Epoch 474/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0109\n",
      "Epoch 475/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0109\n",
      "Epoch 476/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0109\n",
      "Epoch 477/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0109\n",
      "Epoch 478/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0109\n",
      "Epoch 479/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0109\n",
      "Epoch 480/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0109\n",
      "Epoch 481/500\n",
      "438/438 [==============================] - 0s 32us/sample - loss: 0.0109\n",
      "Epoch 482/500\n",
      "438/438 [==============================] - 0s 30us/sample - loss: 0.0109\n",
      "Epoch 483/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0109\n",
      "Epoch 484/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0109\n",
      "Epoch 485/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0109\n",
      "Epoch 486/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0109\n",
      "Epoch 487/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0109\n",
      "Epoch 488/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0109\n",
      "Epoch 489/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0109\n",
      "Epoch 490/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0109\n",
      "Epoch 491/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0109\n",
      "Epoch 492/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0108\n",
      "Epoch 493/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0109\n",
      "Epoch 494/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0108\n",
      "Epoch 495/500\n",
      "438/438 [==============================] - 0s 27us/sample - loss: 0.0108\n",
      "Epoch 496/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0109\n",
      "Epoch 497/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0108\n",
      "Epoch 498/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0108\n",
      "Epoch 499/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0108\n",
      "Epoch 500/500\n",
      "438/438 [==============================] - 0s 25us/sample - loss: 0.0108\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18a8e8416d0>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 500 \n",
    "batchSize = 35\n",
    "lstm.fit(new_train_lstm,y_train_labels,epochs=epochs,verbose=1,batch_size=batchSize) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    }
   ],
   "source": [
    "LSTM_train_proba = lstm.predict(new_train_lstm)\n",
    "LSTM_train_pred =Predict(LSTM_train_proba)\n",
    "LSTM_test_proba = lstm.predict(new_test_lstm)\n",
    "LSTM_test_pred =Predict(LSTM_test_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN training with laryer  train score: 0.9840182648401826\n",
      "RNN training with laryer  test score: 0.898936170212766\n"
     ]
    }
   ],
   "source": [
    "LSTM_train_acc =  accuracy_score(y_train, LSTM_train_pred)\n",
    "LSTM_test_acc = accuracy_score(y_test, LSTM_test_pred)\n",
    "print(\"RNN training with laryer  train score: {}\".format(LSTM_train_acc))\n",
    "print(\"RNN training with laryer  test score: {}\".format(LSTM_test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC: 0.898936170212766\n",
      "macro-PRE: 0.8774838330393885\n",
      "macro-SEN: 0.8741252214462065\n",
      "macroF1-score: 0.8749616407415562\n"
     ]
    }
   ],
   "source": [
    "print('ACC:', metrics.accuracy_score(y_test,LSTM_test_pred)) \n",
    " \n",
    "print('macro-PRE:',metrics.precision_score(y_test,LSTM_test_pred,average='macro')) \n",
    " \n",
    "print('macro-SEN:',metrics.recall_score(y_test, LSTM_test_pred,average='macro'))\n",
    " \n",
    "print('macroF1-score:',metrics.f1_score(y_test, LSTM_test_pred,labels=[0,1,2],average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9916\\3893804580.py:19: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "  mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 计算每一类的ROC\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes): # 遍历三个类别\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_label[:, i], LSTM_test_proba[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area（方法二）\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test_label.ravel(), LSTM_test_proba.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# Compute macro-average ROC curve and ROC area（方法一）\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "lw=2\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.4f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='ROC curve of class {0} (area = {1:0.4f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Some extension of Receiver operating characteristic to multi-class')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
